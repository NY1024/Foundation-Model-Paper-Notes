# Set-level Guidance Attack: Boosting Adversarial Transferability of  Vision-Language Pre-training Mod



1. 研究背景： 随着视觉-语言预训练（VLP）模型在多模态任务中取得显著性能，它们对对抗性示例的脆弱性也引起了关注。对抗性攻击可以通过在输入图像中添加精心设计的扰动来欺骗模型，这些扰动对人类视觉不可见，但可以使深度学习模型产生错误的分类。现有的研究主要集中在白盒攻击上，即攻击者可以完全访问目标模型的信息。然而，对抗性示例在不同VLP模型之间的转移性尚未得到充分研究，这在实际应用中是一个更为实际的设置。



1. 过去方案和缺点： 以往的工作主要关注于单独的模态攻击（如图像或文本），而没有充分利用跨模态交互。这些方法在白盒设置下表现出色，但在黑盒设置下，即在没有目标模型信息的情况下，它们的转移性显著下降。此外，现有的多模态攻击方法通常只使用单一的图像-文本对来生成对抗性样本，这限制了它们在不同VLP模型之间转移的能力。
2. 本文方案和步骤： 本文提出了一种名为Set-level Guidance Attack（SGA）的新型多模态攻击方法。SGA通过利用多模态交互和跨模态指导来增强对抗性转移性。具体来说，SGA引入了保持对齐的增强方法，通过多尺度图像集和最匹配的标题集来丰富输入数据，同时保持它们的对齐关系。然后，SGA使用跨模态指导来优化对抗性数据，通过迭代地推动补充信息远离两种模态，以破坏交互并实现更和谐的扰动。
3. 本文创新点与贡献：

* 提出了SGA，这是首次尝试探索VLP模型的对抗性转移性。
* SGA通过集合级别的对齐保持增强和精心设计的跨模态指导，显著提高了对抗性转移性。
* 在多个下游视觉-语言任务上，SGA在黑盒设置下的表现优于现有的最先进的方法。

5. 本文实验和性能： 实验结果表明，SGA在图像-文本检索任务中，特别是在黑盒设置下，显著提高了攻击成功率。例如，在ALBEF到TCL的转移攻击中，SGA将攻击成功率从最先进的方法提高了至少9.78%，最高达到30.21%。此外，SGA在图像标题和视觉基础任务上也表现出优越的性能。
6. 结论： 本文的研究揭示了VLP模型在对抗性攻击面前的脆弱性，并提出了一种新的攻击方法SGA，该方法在提高对抗性转移性方面取得了显著成效。这为评估和增强VLP模型的鲁棒性提供了新的视角，并可能对实际应用中的模型安全性产生重要影响。

阅读总结报告： 本文针对视觉-语言预训练模型在对抗性攻击下的脆弱性进行了深入研究，并提出了一种新的攻击方法SGA，该方法通过集合级别的对齐保持增强和跨模态指导来显著提高对抗性样本的转移性。实验结果表明，SGA在多个视觉-语言任务中，尤其是在黑盒设置下，比现有方法有显著的性能提升。这项工作不仅揭示了VLP模型在实际应用中可能面临的安全风险，也为如何提高这些模型的鲁棒性提供了新的策略。
