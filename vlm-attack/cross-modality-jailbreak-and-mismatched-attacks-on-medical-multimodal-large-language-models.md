# Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models

<figure><img src="../.gitbook/assets/image (279).png" alt=""><figcaption></figcaption></figure>

#### 研究背景

随着大型语言模型（LLMs）在安全性问题上的广泛探讨，多模态大型语言模型（MLLMs），特别是在医疗领域的应用（MedMLLMs），其安全性影响尚未得到充分研究。本文深入探讨了MedMLLMs在临床环境中部署时可能遇到的安全漏洞问题，尤其是在复杂医疗挑战下问答交互的准确性和相关性至关重要。

#### 过去方案和缺点

以往的研究主要集中在提高医疗AI的诊断能力和个性化治疗计划上，但对MedMLLMs的安全性研究不足。现有模型可能存在输入不匹配或恶意用户输入的问题，这可能导致安全隐患。

#### 本文方案和步骤

* **构建3MAD数据集**：作者创建了多模态医疗模型攻击数据集（3MAD），用于测试MedMLLMs的弹性。
* **定义新的攻击类型**：重新定义了两种攻击类型：不匹配恶意攻击（2M-attack）和优化的不匹配恶意攻击（O2M-attack）。
* **提出MCM优化方法**：提出了一种多模态交叉优化（MCM）方法，显著提高了对MedMLLMs的攻击成功率。
* **实验评估**：使用3MAD数据集和新的攻击方法对LLaVA-Med等模型进行了评估，并进行了白盒攻击和迁移攻击。

#### 本文创新点与贡献

* **新攻击方法的定义**：引入了2M-attack和O2M-attack，这两种方法在攻击成功率上比现有方法提高了10%-20%。
* **3MAD数据集的创建**：提供了一个全面评估MedMLLMs安全性和语义一致性的新工具。
* **MCM方法的创新**：提出了一种新的多模态交叉优化策略，显著提高了攻击效果。

#### 本文实验

实验使用了3MAD-Tiny-1K数据集对LLaVA-Med模型进行了10次迭代的攻击，生成了带有噪声的对抗性图像和文本后缀，然后将这些对抗性图像-文本对用于对其他四个开源多模态医疗模型进行迁移攻击。

#### 实验结论

实验结果表明，即使是设计有增强安全特性的MedMLLMs也容易受到安全漏洞的攻击。MCM方法在攻击成功率上表现最佳，且在不匹配攻击中的拒绝率最低。

#### 全文结论

本文强调了在医疗环境中使用MedMLLMs时，需要紧急采取协调一致的努力来实施强大的安全措施，以提高开源MedMLLMs的安全性和有效性。

####
