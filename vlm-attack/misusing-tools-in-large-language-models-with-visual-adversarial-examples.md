# MISUSING TOOLS IN LARGE LANGUAGE MODELS  WITH VISUAL ADVERSARIAL EXAMPLES

<figure><img src="../.gitbook/assets/image (8) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 随着大型语言模型（LLMs）的发展，它们现在能够使用工具并处理多种模态，如图像和声音。这些新能力带来了新的好处，但也引入了新的安全风险。本文展示了攻击者如何使用视觉对抗性示例来操纵LLM执行攻击者期望的工具使用，例如删除日历事件、泄露私人对话和预订酒店。
2. 过去方案和缺点： 以往的研究主要集中在文本提示注入攻击，这些攻击通过在网页上嵌入恶意文本指令来操纵LLM。然而，这些攻击不够隐蔽，因为安全意识强的用户可以通过检查提示历史来检测到无关的指令。此外，这些攻击通常只针对特定的提示有效，而不是针对更广泛的用户输入。

<figure><img src="../.gitbook/assets/image (9) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤： 本文提出了一种基于白盒图像的攻击方法，攻击者可以制作特洛伊木马式的图像，指示受害者LLM调用攻击者指定的工具或外部API调用。攻击使用传统的基于梯度的对抗性训练来优化连续空间中的对抗性图像。首先，设计了一个训练损失函数，以在保持正常对话响应的同时注入恶意工具使用。其次，构建了提示-响应训练对，以实现攻击对未见提示的泛化。

<figure><img src="../.gitbook/assets/image (10) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文创新点与贡献：

* 提出了一种隐蔽、安全相关的白盒攻击，可以导致多模态LLM调用攻击者期望的工具。
* 通过人类评分和自动化指标，对攻击的性能进行了多维度的评估。
* 发现对抗性图像可以在几乎所有情况下（约98%）操纵LLM调用工具，同时保持与干净图像的高相似性（约0.9 SSIM）。

5. 本文实验和性能： 实验在开源多模态LLM LLaMA Adapter上进行。结果显示，攻击在图像相关和无关的提示上都取得了高成功率，且对抗性图像与原始图像的相似度接近100%。此外，攻击在人类评分和自动化指标上保持了较高的响应实用性。
6. 结论： 本文提出了一种针对集成第三方工具的多模态LLM的新型攻击方法。攻击者可以利用对抗性图像操纵LLM生成攻击者指定的工具调用，从而危害用户资源的机密性和完整性。这些对抗性图像在广泛的用户-LLM对话中具有泛化能力，并且由于看起来良性且不影响自然合理的用户-LLM对话，因此具有高度隐蔽性。



在这篇论文中，作者们针对的是一个开源的多模态大型语言模型（LLM），名为LLaMA Adapter。LLaMA Adapter是一个能够处理图像输入的模型，它通过将图像编码成一系列表示，这些表示被视为标记并附加到文本输入上，从而使标记生成依赖于图像。在实验中，作者们使用LLaMA Adapter来评估他们提出的攻击方法的有效性。他们没有提到使用其他具体的模型名称，而是专注于LLaMA Adapter这一特定的多模态LLM。





阅读总结报告： 本文针对多模态LLM的安全问题，提出了一种新的白盒攻击方法，该方法通过对抗性图像操纵LLM执行攻击者期望的工具调用。实验结果表明，该攻击方法在保持隐蔽性和响应实用性的同时，能够有效地在多种提示下操纵LLM。这一发现强调了在LLM集成第三方工具时需要考虑的安全风险，并为未来LLM的安全研究提供了新的视角。尽管攻击方法在白盒环境下有效，但其在黑盒环境下的转移性和对封闭源LLM的适用性仍有待进一步研究。
