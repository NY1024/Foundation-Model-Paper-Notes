# Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 多模态大型语言模型（MLLMs）如GPT-4V在生成详细图像描述、生成代码、定位图像内视觉对象以及执行高级多模态推理方面展现出强大的能力。然而，这些模型在训练过程中可能会接触到有害或私密内容，存在生成不期望输出的风险。为了确保安全输出，研究者们通过安全机制对这些模型进行了微调。尽管如此，越狱攻击（jailbreaking）旨在绕过这些安全约束和内容过滤机制。以往的研究主要集中在模型输入的对抗性示例上，而对于模型API中的漏洞，尤其是系统提示（system prompts）的漏洞，研究较少。

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 过去方案和缺点： 以往的研究主要关注于通过对抗性图像来攻击MLLMs，例如通过在图像中添加微小的扰动来诱使模型生成不当内容。然而，这些方法通常需要对模型的内部工作有深入的了解，并且在攻击过程中可能需要多次迭代和调整。此外，这些方法在处理模型API层面的漏洞时，尤其是系统提示的漏洞，研究较少。

<figure><img src="../.gitbook/assets/image (5) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤： 本文提出了一种名为SASP（Self-Adversarial Attack via System Prompt）的新方法，通过利用GPT-4作为红队工具来攻击自身，自动化地将系统提示转换为越狱提示。首先，通过精心设计的对话，成功提取了GPT-4V的内部系统提示。然后，利用这些系统提示，通过自我对抗的方式，迭代地生成能够绕过GPT-4V安全约束的越狱提示。此外，为了提高攻击成功率，还加入了基于GPT-4分析的人工修改，将攻击成功率提高到了98.7%。

<figure><img src="../.gitbook/assets/image (6) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文实验和性能： 实验结果表明，SASP方法能够有效地从GPT-4V中提取系统提示，并将其转换为越狱提示。通过这种方法，研究者们能够在GPT-4V上实现高达59%的越狱成功率，并且通过人工修改进一步将成功率提高到99%。此外，研究还探讨了通过修改系统提示来防御越狱攻击的有效性，结果表明适当设计的系统提示可以显著降低越狱成功率。

阅读总结报告： 本文的研究揭示了MLLMs中系统提示泄露的安全隐患，并提出了一种新的越狱攻击方法SASP。这种方法不仅能够有效地从GPT-4V中提取系统提示，还能够通过自我对抗的方式生成越狱提示，显著提高了越狱攻击的成功率。此外，研究还探讨了系统提示在防御越狱攻击中的潜力，为提高MLLMs的安全性提供了新的见解。这些发现不仅为越狱攻击提供了新的途径，也为防御这类攻击提供了有价值的策略。
