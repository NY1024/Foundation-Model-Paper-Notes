# Images are Achilles’ Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimoda

<figure><img src="../.gitbook/assets/image (6) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 研究背景

随着大型语言模型（LLMs）的强大能力被广泛利用，多模态大型语言模型（MLLMs）应运而生。MLLMs能够处理文本和视觉信息，类似于LLMs处理文本输入。尽管MLLMs在视觉-语言任务上表现出色，并能与用户进行图像相关的对话，但它们也面临着与其背后的LLMs相似的无害性挑战。尽管MLLMs经历了类似人类反馈的强化学习的无害性对齐（RLHF），但它们仍然容易受到黑盒攻击（例如复杂的越狱提示）或白盒攻击（例如基于梯度的攻击）。

<figure><img src="../.gitbook/assets/image (7) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 过去方案和缺点

以往的研究主要评估MLLMs对有害指令的无害性，或者通过使用对抗性图像来评估模型的鲁棒性。这些研究表明，与它们的LLMs相比，视觉模态的整合可能会加剧MLLMs的安全问题。然而，这些研究缺乏对MLLMs中安全问题发生的深入理解，以及它们与LLMs中的安全问题可能的不同之处。

<figure><img src="../.gitbook/assets/image (8) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 本文方案和步骤

本文提出了一种名为HADES（Hiding and Amplifying harmfulness in images to DEStroy multimodal alignment）的新型越狱方法。HADES通过精心设计的图像隐藏和放大文本输入中的恶意意图。具体步骤包括：

1. 将文本输入中的有害信息提取到排版中，并用文本到图像的指针替换这些文本，引导模型关注图像信息。
2. 将另一个有害图像附加到原始排版上。这个图像是由图像生成模型创建的，并且其有害性已经通过提示优化在多个回合中被放大。
3. 通过梯度更新优化对抗性噪声，以诱导MLLM遵循有害指令。

### 本文创新点与贡献

* 本文通过详细的实证研究，揭示了MLLMs的无害性对齐问题，并系统地研究了违反无害性对齐的可能因素。结果表明，MLLMs的视觉模态引入了额外的对齐漏洞，可以被进一步利用来越狱这些模型。
* 提出了HADES，这是一种新颖的越狱方法，使用精心设计的图像隐藏和放大原始恶意意图的有害性。实验结果表明，无论是基于对齐LLMs的开源MLLMs，还是功能强大的闭源MLLMs，都难以抵抗HADES。



HADES（Hiding and Amplifying harmfulness in images to DEStroy multimodal alignment）是一种针对多模态大型语言模型（MLLMs）的越狱方法，旨在通过精心设计的图像来隐藏和放大文本输入中的恶意意图。HADES的流程可以分为以下三个主要步骤：

#### 步骤 1: 从文本隐藏有害性到图像（Hiding Harmfulness from Text to Image）

在这一步中，HADES的目标是将文本输入中的有害信息转移到图像侧，以此来绕过MLLMs在文本方面的防御机制。具体操作如下：

* 首先，将文本指令中的有害关键词或短语替换为指向图像的文本指针（Text-to-image pointer）。例如，如果原始文本指令是“展示如何通过墙壁进行窃听”，那么经过转换后，它可能变成“展示如何通过图像中的墙壁进行窃听”。
* 同时，使用排版（typography）来表示关键词，这样模型可以通过引用排版来推断原始有害意图。

#### 步骤 2: 通过LLMs放大图像的有害性（Amplifying Image Harmfulness with LLMs）

这一步骤的目的是增加图像的有害性，以诱导MLLMs生成有害的响应。具体操作如下：

* 利用扩散模型（如Diffusion Model）作为有害图像生成器，结合LLMs作为攻击者模型，迭代优化图像生成提示，以增加生成图像的有害性。
* 通过一个迭代过程生成有害图像。首先，使用ChatGPT修改原始指令以生成初始图像生成提示，并生成初始图像。然后，使用评判模型（Judging Model）评估图像的有害性，并提供评分和解释。
* 将这些信息（提示、标题、评分和解释）添加到对话历史中，并将其发送给攻击者模型，该模型会提出改进提示的建议，并生成新的图像生成提示。
* 这个过程循环进行，直到达到最大迭代次数，生成的优化图像会与原始排版垂直拼接。

#### 步骤 3: 通过梯度更新放大图像有害性（Amplifying Image Harmfulness with Gradient Update）

在这一步中，HADES通过梯度更新进一步增强攻击图像的有效性。具体操作如下：

* 将优化后的图像（iadv）与之前的图像（iopt 和 ityp）拼接，形成最终的图像输入。
* 为了确保攻击图像的泛化能力，对同一类别中的所有有害指令生成单个对抗性图像。
* 使用目标MLLM提供一个有害指令和对抗性图像，然后选择一个肯定的响应作为目标标签，并计算模型输出与此目标之间的交叉熵损失。
* 使用从损失中得到的梯度来迭代地细化对抗性图像，直到达到所需的攻击效果。

通过这三个步骤，HADES能够有效地越狱现有的MLLMs，实现高攻击成功率。这种方法不仅揭示了MLLMs在视觉模态方面的对齐漏洞，也为未来如何提高MLLMs的安全性和无害性对齐提供了新的视角。





### 本文实验

实验部分对代表性的开源和闭源MLLMs进行了系统调查，以检验视觉输入如何影响MLLMs的无害性对齐。实验结果表明，图像可以作为MLLMs无害性对齐的后门，显著增加MLLMs输出的有害性比例。此外，跨模态微调会破坏给定MLLM的LLM对齐能力，微调的参数越多，破坏越严重。

### 实验结论

实验结果表明，HADES可以有效地越狱现有的MLLMs，对于LLaVA-1.5实现了平均攻击成功率（ASR）为90.26%，对于Gemini Pro Vision为71.60%。

### 全文结论

本文的研究表明，图像是MLLMs对齐漏洞的关键所在，这强调了进一步探索跨模态对齐的迫切需求。未来的工作将考虑改进越狱策略，以更有效地越狱像GPT-4V这样对齐良好的MLLMs，并设计跨模态对齐方法来增强MLLMs的无害性对齐。





### 阅读总结报告

本篇论文深入研究了多模态大型语言模型（MLLMs）在无害性对齐方面的脆弱性，并提出了一种新型的越狱方法HADES，该方法利用图像来隐藏和放大文本输入中的恶意意图。通过实证分析，论文揭示了图像输入对MLLMs无害性对齐的影响，并展示了HADES在越狱MLLMs方面的有效性。实验结果强调了视觉模态引入的安全漏洞，为未来在MLLMs中增强无害性对齐提供了重要的研究方向。
