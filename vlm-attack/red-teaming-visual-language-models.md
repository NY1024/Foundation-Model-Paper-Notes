# Red Teaming Visual Language Models

<figure><img src="../.gitbook/assets/image (5) (1) (1).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

视觉语言模型（VLMs）结合了大型语言模型（LLMs）的能力，能够处理包含文本和视觉输入的多模态数据。尽管LLMs在特定测试案例（称为红队攻击）下可能会生成有害或不准确的内容，但VLMs在类似情况下的表现，尤其是在处理文本和视觉输入的组合时，仍然是一个未解决的问题。为了探索这个问题，研究者们提出了一个新的红队攻击数据集RTVLM，该数据集包含10个子任务，涵盖四个主要方面：忠实度、隐私、安全和公平性。

<figure><img src="../.gitbook/assets/image (6) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 2. 过去方案和缺点

以往的研究主要集中在LLMs上，而对于VLMs的红队攻击测试缺乏系统的基准测试。此外，现有的VLMs在处理红队攻击时可能会表现出不同程度的脆弱性，并且在隐私保护、安全性和公平性方面可能存在不足。

### 3. 本文方案和步骤

研究者们提出了RTVLM数据集，这是第一个针对VLMs的红队攻击基准测试数据集。该数据集包括10个子任务，分别对应不同的测试场景，如图像误导、多模态越狱、面部公平性等。研究者们通过人工注释和GPT-4生成的问题对这些任务进行了详细的分析，并建立了评分标准来评估VLMs在这些方面的性能。

### 4. 本文创新点与贡献

* 提出了第一个针对VLMs的红队攻击基准测试数据集RTVLM。
* 对10个开源VLMs进行了详细的性能分析，发现它们在红队攻击测试中存在不同程度的脆弱性。
* 通过使用RTVLM数据集对LLaVA-v1.5进行监督式微调（SFT），提高了模型在RTVLM测试集上的性能，同时在其他基准测试上保持了稳定的性能。

### 5. 本文实验

实验中，研究者们使用RTVLM数据集对多个VLMs进行了测试，包括LLaVA系列、Fuyu、Qwen-VL-Chat和GPT-4V。通过GPT-4V评估和人工评估对模型的输出进行了评分，并分析了模型在忠实度、隐私、安全和公平性方面的表现。

### 6. 实验结论

实验结果表明，与GPT-4V相比，现有的开源VLMs在红队攻击测试中存在显著的性能差距，最高可达31%。此外，通过使用RTVLM数据集进行SFT，可以显著提高模型在红队攻击测试中的性能，同时在其他基准测试中保持稳定的性能。

### 7. 全文结论

本文通过提出RTVLM数据集，为VLMs的安全性提供了第一个红队攻击基准测试。研究揭示了当前开源VLMs在红队攻击测试中的脆弱性，并提出了通过红队攻击对齐来提高模型安全性的有效方法。这项工作强调了VLMs安全性的重要性，并为未来的改进提供了见解和参考。



注：\
在红队攻击测试中，VLMs（视觉语言模型）的脆弱性主要体现在以下几个方面：

1. **忠实度（Faithfulness）**：
   * VLMs在处理含有误导性信息的文本和图像时，表现出较高的易受攻击性。尤其是在图像和文本结合的情况下，模型更容易被误导，生成不准确或有害的内容。
   * 在测试中，VLMs在识别纯文本中的误导信息时表现较好，但在图像和文本混合的情况下，它们的性能显著下降。
2. **隐私（Privacy）**：
   * 许多VLMs在处理涉及个人隐私的问题时，未能有效地拒绝回答或提供谨慎的回答。特别是对于非公众人物的个人信息，模型普遍未能拒绝回答，可能会泄露私人信息。
   * 相比之下，GPT-4V在处理名人信息时能够提供准确信息或表明没有此类信息，对于非名人的个人信息则拒绝回答，显示出更好的隐私保护能力。
3. **安全（Safety）**：
   * VLMs在识别图像中的文本内容，特别是在越狱和验证码任务中，表现出明显的不足。这表明VLMs在处理这类输入时缺乏能力，可能导致生成有害内容或错误地识别验证码。
   * LLaVA系列模型虽然在图像中识别文本的能力更强，但由于缺乏红队攻击对齐，容易生成有害内容或错误地识别验证码。
4. **公平性（Fairness）**：
   * 在评估VLMs对不同性别和种族数据类型的响应时，发现模型在性别和种族方面存在偏见。GPT-4V在性别和种族类别上的偏见最小，而其他VLMs在性别方面的偏见显著弱于种族方面的偏见。
   * 具体来说，对于性别，男性或女性的偏见水平低于非二元性别或无性别信息的群体，男性和女性之间的公平性相对平衡。在种族方面，较浅肤色的群体（相对较浅的肤色）的公平性得分明显高于较深肤色的群体，而美洲原住民在所有模型中的得分几乎总是最低。

这些脆弱性表明，尽管VLMs在多模态理解和生成方面取得了显著进展，但在面对红队攻击时，它们在忠实度、隐私保护、安全性和公平性方面仍然存在显著的不足。这些发现强调了对VLMs进行更严格的安全测试和对齐训练的重要性，以确保它们在实际部署中的安全性和可靠性。通过使用RTVLM数据集进行监督式微调（SFT），可以显著提高模型在红队攻击测试中的性能，同时在其他基准测试中保持稳定的性能，这为提高VLMs的安全性提供了一个有效的途径。



### 阅读总结

本文通过引入新的RTVLM数据集，为评估和改进VLMs在面对红队攻击时的安全性提供了一个系统的方法。通过详细的实验和分析，研究揭示了现有VLMs在忠实度、隐私、安全和公平性方面的不足，并展示了通过红队攻击对齐可以有效地提高模型的安全性。这项工作不仅为VLMs的安全性研究提供了宝贵的基准测试，也为未来的模型开发和改进提供了重要的指导。
