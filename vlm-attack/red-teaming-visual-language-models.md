# Red Teaming Visual Language Models

<figure><img src="../.gitbook/assets/image (271).png" alt=""><figcaption></figcaption></figure>

#### 阅读总结报告

**1. 研究背景**

视觉语言模型（VLMs）结合了文本和视觉输入，扩展了大型语言模型（LLMs）的能力。尽管LLMs在特定测试案例下（被称为红队测试）可能会产生有害或不准确的内容，但VLMs在类似场景下的表现，尤其是结合文本和视觉输入时，仍然是一个未解决的问题。

**2. 过去方案和缺点**

以往的研究主要关注于LLMs的红队测试，缺乏对VLMs在图像-文本输入场景下的系统性红队测试基准。此外，现有VLMs在面对误导性输入时可能产生不准确的输出，且在隐私保护、安全性和公平性方面存在潜在风险。

**3. 本文方案和步骤**

本文提出了一个新的红队测试数据集RTVLM，包含10个子任务，涵盖忠实度、隐私、安全和公平性四个主要方面。数据集构建、评估和对齐的整个过程如图1所示。作者还对10个著名的开源VLMs进行了测试，并分析了它们在红队测试中的表现。

**4. 本文创新点与贡献**

* 提出了第一个针对VLMs的红队测试数据集RTVLM。
* 数据集包含5200个样本，涵盖多种任务，如多模态越狱和视觉误导。
* 对10个开源VLMs进行了详细的红队测试分析，揭示了它们在不同程度上的脆弱性。
* 通过监督式微调（SFT）使用RTVLM数据增强了模型在红队测试集上的表现。

**5. 本文实验**

实验包括：

* 使用GPT-4V评估和人类评估来打分VLMs在RTVLM上的表现。
* 对比了开源VLMs和GPT-4V在忠实度、隐私、安全和公平性四个维度上的表现。
* 通过在LLaVA-v1.5上应用SFT，并使用RTVLM数据，提升了模型在红队测试集和其他相关任务上的表现。

**6. 实验结论**

* 所有10个开源VLMs在红队测试中表现出不同程度的挑战，与GPT-4V相比有高达31%的性能差距。
* 通过在LLaVA-v1.5上应用SFT和RTVLM数据，模型在RTVLM测试集上的表现提高了10%，在MM-hallu上提高了13%，同时在MM-Bench上保持了稳定的表现。

**7. 全文结论**

本文通过提出RTVLM数据集，为VLMs的安全性提供了第一个红队测试基准，揭示了现有开源VLMs在安全性方面的不足，并提出了通过红队测试对齐来提升模型安全性的有效方法。作者希望这项工作能够引起社区对VLMs安全性的重视，并为未来的工作提供见解和参考。

#### 阅读总结

本文针对视觉语言模型（VLMs）的安全性问题，提出了首个红队测试基准RTVLM，并通过一系列实验揭示了当前开源VLMs在面对误导性输入时的脆弱性。通过监督式微调（SFT）和红队测试对齐，显著提升了模型的安全性和鲁棒性。这项工作不仅为VLMs的安全性评估提供了新的视角和工具，也为未来的模型改进和安全性增强提供了有价值的见解。
