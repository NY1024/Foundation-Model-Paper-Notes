# Private Attribute Inference from Images with Vision-Language Models

<figure><img src="../.gitbook/assets/image (268).png" alt=""><figcaption></figcaption></figure>



**1. 研究背景**

随着大型语言模型（LLMs）在日常任务和数字交互中变得越来越普遍，相关的隐私风险也越来越受到关注。尽管LLM隐私研究主要集中在模型训练数据的泄露上，但最近研究表明，模型能力的提高已经使LLMs能够从未见过的文本中准确推断出侵犯隐私的信息。随着能够理解图像和文本的多模态视觉-语言模型（VLMs）的出现，一个相关的问题出现了：这些结果是否适用于之前未探索的领域——在线发布的良性图像。

**2. 过去方案和缺点**

以往的研究主要关注于LLMs训练数据中的个人信息记忆和泄露，而对基于推理的隐私风险探讨不足。此外，现有的视觉数据集主要关注于从图像中提取和推断人物特征，通常在非隐私相关的设置中，例如行人识别，并没有涵盖由VLMs带来的更广泛的隐私威胁。

**3. 本文方案和步骤**

为了调查新兴VLMs的图像推理能力所带来的隐私风险，作者们编制了一个带有人工标注的图像所有者个人属性标签的图像数据集。数据集中的图像选择标准是：可以推断出的私人属性不是来自直接的人类描绘。在此基础上，评估了7个最先进的VLMs的推理能力。

**4. 本文创新点与贡献**

* 首次识别并形式化了VLMs在推理时所带来的隐私风险。
* 对7个前沿VLMs进行了从现实世界图像中推断个人属性的广泛实验评估。
* 开源实现了数据集标注工具和推理管道的代码，以推进隐私研究。

**5. 本文实验**

实验评估了包括OpenAI和Google的专有模型GPT4-V和Gemini-Pro，以及Huggingface上可用的五个开源模型。研究发现，尽管一些模型的安全过滤器拒绝了高达54.5%的查询，但它们可以通过提示工程轻易绕过，使模型能够正确推断出高达77.6%的私人属性。

**6. 实验结论**

实验结果显示，当前的VLMs能够从看似无害的在线图像中准确推断出各种个人属性。此外，当前的安全过滤器在简单规避技术面前效果不佳，为潜在的恶意行为者提供了低门槛。实验还发现，个人属性推断的准确性与模型的一般能力直接相关，这意味着未来的模型迭代将构成更大的隐私威胁。

**7. 全文结论**

本文通过首次探索和评估VLMs的推理能力所带来的隐私风险，强调了VLMs能够从未直接描绘人物的在线图像中推断出私人属性的能力。研究表明，现有的安全措施不足以应对这些风险，呼吁社区加强对VLMs推理威胁的隐私保护措施的研究和开发。

#### 阅读总结

本文对VLMs的隐私侵犯推理能力进行了首次探索和评估。作者们创建了一个新的数据集，该数据集包含从不直接描绘人物的图像中推断出的个人属性标签，并发现即使是最先进的模型，如GPT4-V，也能够以接近人类水平的准确度进行隐私侵犯推理。此外，实验表明，当前的安全过滤器容易被规避，而且模型的隐私侵犯能力和其在其他无害和有用任务上的表现直接相关。这些发现表明，随着VLMs的广泛采用和能力的提高，隐私风险正在增加，需要社区共同努力，开发有效的隐私保护措施。
