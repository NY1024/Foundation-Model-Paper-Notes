# VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models

<figure><img src="../.gitbook/assets/image (15) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 本研究聚焦于自回归视觉语言模型（Autoregressive Visual Language Models, VLMs），这类模型在多模态环境中展现出了令人印象深刻的少样本学习能力。然而，研究者发现在多模态指令调整（instruction tuning）过程中，自回归VLMs可能面临后门攻击的潜在威胁。攻击者可以通过在指令或图像中注入带有触发器的恶意样本，操纵受害模型的预测结果。
2. 过去方案和缺点： 以往的后门攻击研究主要集中在单一模态的模型上，如图像识别。这些攻击通过在训练样本中植入后门触发器来实现。然而，自回归VLMs的特点是视觉编码器通常是固定的，这限制了传统图像触发器的学习。此外，攻击者可能无法访问受害模型的参数和架构，这增加了攻击策略与目标编码器精确对齐的难度。

<figure><img src="../.gitbook/assets/image (16) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤： 为了应对这些挑战，研究者提出了一种多模态指令后门攻击方法，称为VLTrojan。该方法通过隔离和聚类策略促进图像触发器的学习，并通过对文本触发器进行迭代的字符级搜索来增强黑盒攻击的有效性。具体步骤包括：
   * 使用对比优化方法生成图像触发器，以分离受污染样本和干净样本的特征。
   * 通过迭代的字符级搜索方法生成文本触发器，以提高攻击在不同模型间的转移性。
   * 在受污染的数据集上进行后门训练，将后门集成到模型中。
2. 本文创新点与贡献：
   * 首次揭示了在指令调整期间自回归视觉语言模型（VLMs）可能遭受后门攻击的潜在危险。
   * 提出了一种多模态指令攻击方法，克服了固定视觉编码器的限制，并实现了对自回归VLMs的有效和可转移的后门攻击。
   * 通过实验证明，该方法能够在只有116个受污染样本的情况下植入后门，攻击成功率（ASR）达到99.82%，显著高于基线方法。
3. 本文实验和性能： 实验部分展示了VLTrojan方法在不同模型规模和少样本上下文推理场景下的有效性。实验结果表明，该方法在不同的图像描述任务中都能保持高ASR，并且在黑盒设置下，即使攻击者只有对目标模型的黑盒访问权限，也能实现高ASR。
4. 结论： 研究者强调，本研究旨在揭示自回归VLMs在指令调整过程中可能遭受的后门攻击威胁，并提出了一种有效的防御策略。他们希望通过这项研究提高对此类攻击的认识，并促进更强大的防御措施的发展。



在这篇论文中，研究者们针对的受害者模型是OpenFlamingo，这是一个开源的自回归视觉语言模型（VLM），它是Flamingo模型的一个复制版本。OpenFlamingo模型能够处理开放式文本生成任务，并且在接收到与图像交错的文本标记序列时表现出色。OpenFlamingo模型使用了CLIP预训练的视觉编码器，并且在不同的模型规模（如3B、4B和9B参数版本）中使用了不同的大型语言模型（LLM）。

研究者们在实验中使用了OpenFlamingo模型的三个不同规模版本（3B、4B和9B），以及CLIP ViT-L/14视觉编码器。他们还在实验中考虑了不同架构和参数的CLIP视觉编码器，以评估后门攻击在不同视觉编码器上的转移性。这些模型的详细架构信息在论文的表格1中有所描述。





阅读总结报告： 本文提出了一种针对自回归视觉语言模型的多模态指令后门攻击方法VLTrojan，该方法能够在模型的指令调整过程中植入后门，使得攻击者能够在输入中存在预定义触发器时操纵模型的预测结果。研究者通过对比优化和迭代字符级搜索方法，有效地解决了固定视觉编码器带来的挑战，并在实验中展示了该方法的高效性和鲁棒性。这项工作不仅揭示了VLMs在实际应用中的潜在安全风险，也为未来的安全研究提供了新的视角和防御策略。
