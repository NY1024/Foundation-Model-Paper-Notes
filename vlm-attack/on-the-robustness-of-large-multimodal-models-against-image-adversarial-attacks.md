# On the Robustness of Large Multimodal Models Against Image Adversarial Attacks

<figure><img src="../.gitbook/assets/image (212).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

本研究探讨了大型多模态模型（LMMs）对抗图像对抗性攻击的鲁棒性。随着指令调整技术的进步，LMMs在图像分类、图像标题生成和视觉问题回答（VQA）等任务中表现出色。然而，这些模型对视觉对抗性样本的影响尚未得到充分研究。

<figure><img src="../.gitbook/assets/image (213).png" alt=""><figcaption></figcaption></figure>

### 2. 过去方案和缺点

以往的研究主要集中在针对单一模态（如图像或文本）的对抗性攻击，并且通常假设攻击者对模型有完全的访问权限。然而，在实际应用中，攻击者可能只针对模型的某个部分（例如，只针对视觉编码器），而不是整个模型。此外，现有研究很少考虑在多模态环境中，对抗性攻击对模型性能的影响。

<figure><img src="../.gitbook/assets/image (214).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

研究者选择了三种代表性的LMMs（LLaVA、BLIP2-T5和InstructBLIP），并在不同的对抗性攻击（PGD、APGD和CW）下评估了它们在图像分类、标题检索和VQA任务上的表现。研究者还提出了一种新的实际图像分类方法，称为查询分解，通过在输入提示中加入存在查询来减少攻击的有效性，并提高图像分类的准确性。



在本文中，生产对抗图像的方法是基于梯度的白盒攻击（gradient-based white-box adversarial attacks），具体包括三种攻击方式：投影梯度下降（Projected Gradient Descent, PGD）、近似投影梯度下降（Approximate Projected Gradient Descent, APGD）和Carlini & Wagner（CW）攻击。这些攻击方法都是针对图像编码器进行的，而不是针对整个大型多模态模型（LMM）。

#### PGD (Projected Gradient Descent)

PGD是一种迭代方法，通过沿着模型损失函数梯度的方向对输入图像进行小幅度的、逐渐的调整来生成对抗性样本。在每一步迭代中，都会对图像的微小扰动进行投影，以确保扰动后的图像仍然在合法的输入空间内（例如，图像像素值在\[0, 1]之间）。

#### APGD (Approximate Projected Gradient Descent)

APGD是PGD的一种变体，它通过近似的方式来计算梯度并更新图像，通常速度更快，但可能不如PGD精确。

#### CW (Carlini & Wagner)

CW攻击是一种优化基础的攻击方法，它通过最小化扰动的L2范数来生成对抗性样本，同时保持对抗性样本在感知上与原始图像相似，即人类难以察觉到图像的变化。

这些攻击方法都是在已知模型结构和参数的情况下进行的，因此被称为白盒攻击。在实验中，这些攻击仅针对图像编码器（如CLIP或EVA-CLIP图像编码器）进行，而语言模型部分保持不变。通过这种方式，研究者可以评估在视觉编码器受到攻击时，LMM的整体性能如何受到影响。





### 4. 本文创新点与贡献

* **系统评估**：首次系统地评估了LMMs在多种任务和数据集下对抗视觉对抗性输入的鲁棒性。
* **上下文的作用**：发现通过提示提供给模型的上下文有助于减轻视觉对抗性输入的影响。
* **查询分解方法**：提出了一种新的实际图像分类方法，通过查询分解和上下文增强来提高模型在对抗性环境下的鲁棒性。

### 5. 本文实验

实验结果显示，LMMs在没有额外文本信息的情况下对视觉对抗性扰动不具有鲁棒性。然而，在VQA任务中，当问题涉及与被攻击对象不同的视觉内容时，LMMs显示出一定程度的固有鲁棒性。此外，增加额外的文本上下文显著提高了LMMs对抗视觉对抗性输入的鲁棒性。

### 6. 实验结论

实验表明，LMMs通常容易受到视觉对抗性扰动的影响，尤其是当攻击直接针对模型的视觉编码器时。然而，当查询与攻击目标不匹配时，LMMs表现出较高的鲁棒性。这表明传统的针对特定任务的对抗性生成技术并不普遍适用于当前的LMMs。

### 7. 全文结论

本研究系统地评估了LMMs对抗视觉对抗性输入的脆弱性，并发现即使是针对视觉模型单独生成的对抗性样本，也会导致LMMs的高度脆弱性。研究表明，当查询和攻击目标不匹配时，LMMs表现出鲁棒性，这表明需要进一步研究新的对抗性攻击策略。此外，研究还发现，提供关于查询对象的上下文信息可以提高LMMs的视觉鲁棒性，并通过查询分解方法在COCO和Imagenet分类任务中实现了显著的鲁棒性提升。



注1：

在本文中，梯度信息来自被攻击的图像编码器。具体来说，对于PGD、APGD和CW这三种基于梯度的白盒攻击方法，研究者首先固定语言模型部分，然后只针对图像编码器（如CLIP或EVA-CLIP）进行攻击。在生成对抗性样本的过程中，研究者计算了图像编码器的梯度，这些梯度指导了如何对输入图像进行微小的、针对性的调整，以欺骗模型产生错误的预测。

在白盒攻击中，攻击者通常有以下信息：

* 模型的结构，包括层的类型和参数。
* 模型的前向传播过程，可以计算出给定输入的输出。
* 模型的损失函数，用于评估模型预测的准确性。

利用这些信息，攻击者可以通过反向传播算法计算损失函数相对于输入图像的梯度。然后，根据这个梯度信息，攻击者可以确定如何调整输入图像的像素值，以便在满足一定的扰动约束（例如，像素值的变化不超过某个阈值）的情况下最大化模型的错误率。

在本文的实验中，攻击者只利用了图像编码器的梯度信息，而没有利用整个LMM（包括语言模型部分）的梯度信息。这样做的目的是为了评估当图像编码器单独受到攻击时，LMM的鲁棒性如何，以及语言模型部分是否能够提供足够的上下文信息来减轻对抗性攻击的影响。





### 阅读总结

本文通过全面的实验评估了LMMs在面对视觉对抗性攻击时的鲁棒性，并提出了提高模型鲁棒性的新方法。研究结果强调了在设计多模态系统时，考虑对抗性攻击的重要性，并为未来在对抗性环境中增强多模态系统鲁棒性的研究提供了新的视角和方法。
