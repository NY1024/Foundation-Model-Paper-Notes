# IMAGE HIJACKS: ADVERSARIAL IMAGES CAN  CONTROL GENERATIVE MODELS AT RUNTIME

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 随着大型语言模型（LLMs）的成功，视觉-语言模型（VLMs）开始出现，这些模型能够处理图像和文本。这些模型的广泛应用可能会带来安全问题，尤其是当它们可以访问不受信任的数据和敏感个人信息时。本文关注的是VLM的图像输入通道，研究恶意行为者是否能够通过图像输入控制VLM的行为。这种攻击被称为“图像劫持”。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 过去方案和缺点： 以往的研究主要集中在对抗性图像上，这些图像能够欺骗图像分类模型。然而，这些研究通常集中在单一类型的攻击上，例如分类错误或特定的误导性输出。对于VLMs，现有的研究还没有系统地评估在不同图像约束下，如何创建能够控制模型行为的对抗性图像。
2. 本文方案和步骤： 本文提出了一种名为“行为匹配”（Behaviour Matching）的通用方法，用于创建能够控制VLM行为的对抗性图像（即图像劫持）。该方法通过梯度下降优化图像参数，使得VLM的输出与目标行为相匹配。作者还探讨了三种攻击类型：特定字符串攻击、泄露上下文攻击和越狱攻击。
3. 本文创新点：

* 提出了图像劫持的概念，并开发了一种自动化的方法来创建能够控制VLM输出的对抗性图像。
* 系统地评估了在不同图像约束（如ℓ8范数、静态补丁和移动补丁约束）下，这些图像劫持的性能。
* 通过实验展示了在ℓ8范数约束为16/255时，所有攻击类型在LLaVA模型上至少有90%的成功率。

5. 本文实验和性能： 实验在LLaVA模型上进行，该模型基于CLIP和LLaMA-2。实验结果表明，作者提出的攻击方法在不同的约束条件下都能取得高成功率。特别是，在ℓ8范数约束为16/255时，特定字符串攻击、泄露上下文攻击和越狱攻击的成功率都非常高。这些攻击是自动化的，并且只需要对输入图像进行小的扰动。

阅读总结报告： 本文提出了一种新的对抗性攻击方法，即图像劫持，它能够控制VLM在运行时的行为。通过行为匹配算法，作者成功地创建了能够执行特定字符串输出、泄露上下文信息和绕过模型安全训练的对抗性图像。这些攻击在LLaVA模型上表现出了高成功率，并且能够在不同的图像约束下工作。这些发现对VLM的安全性提出了严重的担忧，尤其是在面对未经验证的图像输入时。作者的工作强调了在开发和部署VLM时需要考虑的安全问题，并为未来的研究提供了新的方向。



注：

这种方法能够创建控制VLM输出的图像，主要是因为以下几个原因：

1. **对抗性优化**：通过对抗性优化，攻击者可以精确地调整图像的像素值，使得VLM在处理这些图像时产生特定的输出。这种优化过程通常涉及到计算VLM对输入图像的梯度，然后根据这些梯度信息来调整图像，以最大化对抗性效果。
2. **行为匹配算法**：作者提出的“行为匹配”算法是一种通用框架，它允许攻击者定义一个目标行为（例如，生成特定的文本字符串），然后通过优化过程找到一个图像，当这个图像被输入到VLM时，模型的输出会与目标行为相匹配。
3. **白盒访问**：在这种攻击中，攻击者假设拥有对VLM的白盒访问权限，这意味着他们可以计算模型对输入图像的梯度。这种访问权限使得攻击者能够有效地进行对抗性优化。
4. **模型的脆弱性**：VLM通常在大量数据上进行训练，以学习复杂的视觉和语言表示。然而，这种复杂性也可能使模型在某些情况下对对抗性输入敏感。攻击者利用这些脆弱性来操纵模型的输出。
5. **多模态特性**：VLM结合了视觉和语言处理能力，这为攻击者提供了额外的操纵空间。通过精心设计的图像，攻击者可以在视觉和语言之间建立特定的关联，从而影响模型的最终输出。
6. **自动化和泛化**：作者的方法是自动化的，这意味着一旦定义了目标行为，就可以自动生成对抗性图像，而不需要手动调整。此外，这种方法具有泛化性，可以在不同的VLM和不同的攻击类型之间转移。

总的来说，这种方法之所以有效，是因为它利用了VLM的内部工作机制和对抗性优化技术，以及模型本身的脆弱性。这使得攻击者能够在不直接访问模型内部结构的情况下，通过外部输入来控制模型的行为。
