# AN IMAGE IS WORTH 1000 LIES: ADVERSARIAL TRANSFERABILITY ACROSS PROMPTS ON VISIONLANGUAGE MODELS

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

####

#### 1. 研究背景

近年来，大型视觉-语言模型（VLMs）在多种视觉任务中展现出了卓越的能力，这些模型可以通过不同的文本指令（即提示）来适应不同的视觉任务。然而，这些模型容易受到不可感知的对抗性扰动的误导，且相同的对抗性扰动可以欺骗不同的任务特定模型。这引出了一个有趣的问题：单个对抗性图像是否能够在给出一千个不同提示的情况下，误导所有VLMs的预测结果？这一问题本质上引入了对抗性转移性的一个新视角：跨提示的对抗性转移性。

#### 2. 过去方案和缺点

以往的研究主要集中在针对特定任务的模型的对抗性攻击上，这些攻击通常是特定于任务的。对于VLMs，现有的方法在创建对抗性示例时，通常使用单个或多个提示，但这些基线方法在提高跨提示的转移性方面的效果有限，且随着提示数量的增加，性能提升迅速趋于平稳。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### 3. 本文方案和步骤

本文提出了一种名为Cross-Prompt Attack (CroPA) 的方法，该方法通过可学习的提示更新视觉对抗性扰动，以对抗性图像的误导效应为设计目标。CroPA通过这种方式显著提高了对抗性示例在不同提示间的转移性。具体步骤包括：

* 初始化干净的图像和提示嵌入。
* 在前向传播过程中，将图像扰动和提示扰动添加到干净的图像和提示嵌入中。
* 通过反向传播获得语言模型损失的梯度。
* 使用梯度下降更新图像扰动，以最小化生成目标文本的语言模型损失。
* 使用梯度上升更新提示扰动，以最大化生成目标文本的语言模型损失。
* 重复上述步骤，直到达到预定的迭代次数。

#### 4. 本文创新点与贡献

* 提出了跨提示的对抗性转移性这一新概念，为VLMs的脆弱性研究提供了新的视角。
* 提出了CroPA算法，旨在增强跨提示的对抗性转移性。
* 通过广泛的实验验证了CroPA方法在不同VLMs和任务上的有效性，并提供了进一步的分析来理解该方法。

#### 5. 本文实验

实验使用了包括Flamingo、BLIP-2和InstructBLIP在内的流行VLMs，并在图像分类、图像标题生成和视觉问答（VQA）等任务上进行了测试。实验结果表明，CroPA在不同设置和不同攻击目标下一致性地优于基线方法。

#### 6. 实验结论

实验结果证实了CroPA在提高跨提示对抗性转移性方面的有效性。即使在使用不同数量的提示进行优化时，CroPA也显示出比基线方法更强的转移性。此外，CroPA的性能不依赖于额外的攻击迭代次数，且在相同的攻击迭代次数下，CroPA的性能始终优于基线方法。



注1：

在对抗性攻击的背景下，使用梯度下降和梯度上升这两种优化策略是为了达到不同的目标：

#### 使用梯度下降更新图像扰动，以最小化生成目标文本的语言模型损失：

这一步骤的目标是生成一个对抗性图像，该图像在输入到VLM时，能够使得模型生成与目标文本相匹配的输出。为了实现这一点，需要调整图像扰动（即在原始图像上添加的微小变化），以误导模型的预测结果。通过最小化语言模型损失，我们实际上是在优化图像扰动，使得模型更可能输出我们设定的目标文本。梯度下降是一种常用的优化算法，它通过沿着损失函数梯度的负方向迭代调整参数（在这种情况下是图像扰动），从而逐步找到最小化损失的解。

#### 使用梯度上升更新提示扰动，以最大化生成目标文本的语言模型损失：

这一步骤的目标是生成一个对抗性提示，该提示在与图像一起输入到VLM时，能够增强模型输出目标文本的倾向。与图像扰动类似，我们通过调整提示扰动来影响模型的输出。然而，与图像扰动的目标不同，我们在这里希望最大化损失，这意味着我们希望模型更倾向于生成我们设定的目标文本，即使这些文本可能与图像内容不一致。梯度上升是实现这一目标的方法，它通过沿着损失函数梯度的正方向迭代调整参数（在这种情况下是提示扰动），从而逐步找到最大化损失的解。

总结来说，这两种策略是为了在对抗性攻击的框架内，分别优化图像扰动和提示扰动，以实现误导VLMs输出特定目标文本的目的。梯度下降用于找到使模型输出目标文本的可能性最小的图像扰动，而梯度上升用于找到使模型输出目标文本的可能性最大的提示扰动。通过这种双向的优化过程，CroPA方法能够提高对抗性示例在不同提示间的转移性。



注2：

在本文中提出的Cross-Prompt Attack (CroPA) 方法的攻击例子可以这样描述：

假设我们有一个视觉-语言模型（VLM），比如Flamingo，它能够根据输入的图像和文本提示来生成描述图像内容的文本。例如，给定一张包含街道和行人的图像，如果没有对抗性扰动，模型可能会根据图像内容生成如下描述：“繁忙的城市街道上有行人在走动。”

现在，我们想要使用CroPA方法来生成一个对抗性示例，目的是让模型生成特定的目标文本，比如“未知”。以下是攻击的步骤：

1. **选择目标文本**：我们设定目标文本为“未知”。
2. **初始化对抗性扰动**：我们开始时有一个干净的图像和一个没有扰动的文本提示。
3. **应用CroPA方法**：
   * 我们首先随机初始化图像扰动`δv`和提示扰动`δt`。
   * 然后，我们将这些扰动应用到原始图像和提示上，得到对抗性图像和提示。
   * 我们将对抗性图像和提示输入到VLM中，并计算模型输出与目标文本之间的语言模型损失。
   * 使用梯度下降更新图像扰动`δv`，目标是最小化生成目标文本“未知”的语言模型损失。
   * 同时，使用梯度上升更新提示扰动`δt`，目标是最大化生成目标文本“未知”的语言模型损失。
4. **迭代优化**：重复上述步骤，直到达到预定的迭代次数或满足某个停止条件。
5. **生成对抗性示例**：完成优化后，我们得到了一个对抗性图像和对应的提示扰动。当我们将这个对抗性图像和扰动后的提示输入到VLM时，模型应该会产生目标文本“未知”，即使图像实际上可能包含完全不同的内容。

通过这种方式，CroPA方法能够创建出一种对抗性示例，该示例在面对不同的文本提示时，都能够误导VLM生成特定的目标文本。这种攻击可以用于评估和提高VLMs对抗性攻击的鲁棒性。





#### 7. 全文结论

本文通过提出CroPA方法，成功地解决了如何通过对抗性示例在不同提示间误导VLMs的问题。CroPA不仅在理论上具有创新性，而且在实际应用中也显示出了强大的有效性。未来的工作可以探索结合CroPA方法和其他提高跨模型和跨图像转移性的方法，以进一步提高对抗性示例的实用性。
