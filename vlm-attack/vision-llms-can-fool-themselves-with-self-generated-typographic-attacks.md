# Vision-LLMs Can Fool Themselves with Self-Generated Typographic  Attacks

<figure><img src="../.gitbook/assets/image (9) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 本研究聚焦于视觉-语言模型（Vision-Language Models，简称LVLMs），这类模型结合了视觉和语言处理能力，能够理解和生成与图像相关的文本。然而，这些模型可能受到所谓的“排版攻击”（Typographic Attacks）的影响，即在图像上叠加误导性文本，从而损害模型的性能。尽管先前的研究已经表明排版攻击对CLIP等模型有显著影响，但对最近的大型视觉-语言模型的易感性研究还不够充分。

<figure><img src="../.gitbook/assets/image (10) (1).png" alt=""><figcaption></figcaption></figure>

1. 过去方案和缺点： 以往的研究在对CLIP模型进行排版攻击时，通常随机从预定义的类别集合中选择一个误导类别。这种简单策略忽略了更有效的攻击，这些攻击利用了LVLMs更强的语言技能。此外，先前的研究缺少对LVLMs进行算法性排版攻击的全面研究。

<figure><img src="../.gitbook/assets/image (11) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤： 研究者首先介绍了一个用于测试LVLMs排版攻击的基准测试。接着，他们提出了两种新型且更有效的自我生成攻击（Self-Generated attacks），这些攻击促使LVLM生成针对自身的攻击：1) 类别基础攻击（Class Based Attack），其中LVLM被要求识别与目标类别最相似的欺骗类别；2) 描述性攻击（Descriptive Attacks），其中更高级的LVLM（如GPT-4V）被要求推荐一个包含欺骗类别和描述的排版攻击。研究者使用这个基准测试，展示了自我生成攻击对LVLMs分类性能的显著影响。
2. 本文创新点：

* 提出了一个全面且多样化的排版攻击基准测试，专门针对大型视觉-语言模型。
* 展示了自我生成排版攻击能够将LVLMs的分类性能降低多达33%。
* 证明了一个模型生成的自我生成攻击能够泛化到其他模型，如InstructBLIP和MiniGPT4。

5. 本文实验和性能： 研究者在五个分类数据集上进行了实验，包括OxfordPets、StanfordCars、Flowers、Aircraft和Food101。他们测试了四种最近的大型视觉-语言模型，包括GPT-4V、LLaVA 1.5、MiniGPT4-2和InstructBLIP。实验结果表明，描述性攻击比随机类别攻击更有效，能够更好地降低模型性能。此外，他们还探讨了模型是否能够忽略排版攻击中的文本，发现大多数模型在没有排版攻击的情况下无法恢复到基线性能。

阅读总结报告： 本研究深入探讨了排版攻击对大型视觉-语言模型的影响，并提出了一种新的攻击方法——自我生成攻击，这种方法利用模型自身的语言理解能力来生成攻击。通过一系列实验，研究者证明了这些攻击对模型性能的显著影响，并指出了现有模型在面对这类攻击时的脆弱性。这项工作不仅为理解LVLMs的安全性提供了新的视角，也为未来如何提高这些模型的鲁棒性提供了重要的研究方向。



注1：

"排版攻击"（Typographic Attacks）这个术语来源于英文中的"typography"，这个词通常指的是印刷或排版的艺术和技巧，包括字体的选择、文本的布局和设计等。在计算机安全和机器学习领域，排版攻击特指一种针对视觉-语言模型（Vision-Language Models，简称LVLMs）的攻击手段，其核心在于通过在图像上添加或修改文本信息来误导模型的判断。

这种攻击之所以被称为“排版攻击”，是因为攻击者通过操纵图像上的文本（即排版元素）来实现攻击目的。在这种攻击中，攻击者可能会在图像上添加与图像内容不符的误导性文本，或者改变现有文本的排版方式，使得视觉-语言模型在处理图像和文本信息时产生混淆，从而影响模型的分类、识别或其他视觉理解任务的准确性。

排版攻击的关键在于利用了视觉-语言模型在处理图像和文本信息时的相互依赖性。这些模型通常被训练来理解图像内容，并能够根据图像内容生成或理解相关的文本描述。攻击者通过改变文本信息，可以操纵模型的输出，使其偏离正确的结果。这种攻击手段在安全领域引起了关注，因为它可能被用于欺骗自动化系统，如自动驾驶车辆、安全监控系统等，从而带来潜在的安全风险。



注2：

排版攻击（Typographic Attacks）之所以能够成功，主要是因为以下几个原因：

1. **视觉-语言模型的依赖性**：视觉-语言模型（如CLIP和其他大型视觉-语言模型）在进行图像分类、对象检测或场景理解等任务时，通常会同时考虑图像的视觉内容和与之相关的文本信息。这些模型被训练来理解图像和文本之间的关联，因此在处理包含文本的图像时，模型会自然地将文本信息纳入其决策过程中。
2. **文本信息的误导性**：排版攻击通过在图像上添加或修改文本，可以误导模型的注意力和解释。例如，如果在一张猫的图片上添加“狗”这个词，模型可能会错误地将图像分类为狗，因为它同时处理了视觉信息（猫的图像）和文本信息（狗这个词）。
3. **模型的泛化能力**：视觉-语言模型通常被训练在大量的数据集上，以提高其泛化能力。然而，这种泛化能力有时也意味着模型可能对特定类型的攻击更加敏感，因为它们可能没有在训练过程中遇到过这类攻击。
4. **模型的优化目标**：在训练过程中，模型会优化以最大化其在训练数据上的表现。这可能导致模型在面对与训练数据分布不同的攻击时，其性能下降。
5. **模型的复杂性**：随着模型规模的增大，模型的决策过程变得更加复杂，这可能导致模型在面对精心设计的攻击时，难以正确地识别和处理异常或误导性信息。
6. **攻击的针对性**：在本文中提出的自我生成攻击（Self-Generated Attacks）中，攻击者利用模型自身的语言理解能力来生成攻击。这意味着攻击者可以设计出更加针对性的攻击，这些攻击能够更有效地利用模型的弱点。
7. **模型的安全机制不足**：尽管一些模型可能具有一定程度的安全机制来抵御攻击，但这些机制可能不足以应对排版攻击，特别是当攻击设计得足够巧妙时。

总之，排版攻击之所以能够成功，是因为它们巧妙地利用了视觉-语言模型在处理图像和文本信息时的内在机制，以及模型在训练过程中可能未能充分考虑到的攻击场景。这些因素共同作用，使得排版攻击成为一种有效的对抗手段。
