# OnEvaluating Adversarial Robustness of  Large Vision-Language Models

<figure><img src="../.gitbook/assets/image (17).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

大型视觉-语言模型（VLMs），如GPT-4，通过视觉输入在响应生成方面取得了前所未有的性能，使得与大型语言模型（如ChatGPT）相比，交互更加创造性和适应性。然而，多模态生成加剧了安全问题，因为对手可能通过微妙地操纵最脆弱的模态（例如视觉）成功绕过整个系统。

<figure><img src="../.gitbook/assets/image (18).png" alt=""><figcaption></figcaption></figure>

### 2. 过去方案和缺点

以往的研究主要集中在文本到图像模型的对抗性攻击，这些模型通常需要对手操纵文本输入来实现逃避目标，这需要广泛的搜索和工程工作。此外，文本到图像模型通常包含安全检查器和不可见的水印模块，以帮助识别假内容。然而，这些模型在面对针对视觉输入的操纵时可能不够健壮。

<figure><img src="../.gitbook/assets/image (19).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了在最现实和高风险的设置中评估开源大型VLMs的健壮性，其中对手只有黑盒系统访问权限，并试图欺骗模型返回目标响应。研究者首先使用预训练的模型（如CLIP和BLIP）作为代理模型来制作针对性的对抗性示例，然后将这些示例转移到其他VLMs（如MiniGPT-4、LLaVA、UniDiffuser、BLIP-2和Img2Prompt）。此外，研究发现基于黑盒查询的攻击可以进一步提高针对性逃避的有效性。



本文提出了两种主要的攻击策略来诱导大型视觉-语言模型（VLMs）生成目标响应：基于转移的攻击策略（Transfer-based Attacking Strategy）和基于查询的攻击策略（Query-based Attacking Strategy）。以下是这两种策略的详细说明：

#### 基于转移的攻击策略（Transfer-based Attacking Strategy）

1. **目标函数定义**：攻击者首先选择一个目标文本（ctar），然后使用预训练的文本到图像生成模型（如Stable Diffusion、Midjourney或DALL-E）生成与目标文本相对应的目标图像（hξ(ctar)）。
2. **特征匹配**：攻击者使用图像编码器（fϕ）和文本编码器（gψ）作为代理模型，通过优化目标图像和干净图像（xcle）之间的特征相似度来生成对抗性图像（xadv）。这可以通过最大化图像和文本编码器输出的内积来实现。
3. **优化过程**：攻击者通过投影梯度下降（PGD）或其他优化方法来调整对抗性噪声（Δ），使得xadv在保持人类不可感知的扰动（即满足ℓ∞范数约束）的同时，最大化与目标图像的特征相似度。

#### 基于查询的攻击策略（Query-based Attacking Strategy）

1. **目标函数定义**：攻击者的目标是使VLMs返回与目标文本（ctar）匹配的输出。这通过最大化VLMs输出与目标文本之间的文本相似度来实现。
2. **梯度估计**：由于VLMs是黑盒模型，攻击者无法直接计算梯度。因此，使用随机梯度自由（RGF）方法来估计梯度。这涉及到在VLMs的输出上添加随机扰动，并计算这些扰动对输出的影响，从而估计梯度。
3. **优化过程**：攻击者使用估计的梯度来更新对抗性图像（xadv），通过迭代过程逐渐调整图像，使其生成的文本输出越来越接近目标文本。

#### 结合两种策略

研究者发现，结合基于转移的攻击和基于查询的攻击可以进一步提高攻击的有效性。首先，使用基于转移的策略生成初始的对抗性图像，然后通过基于查询的策略进一步微调这些图像，以增强对抗性效果。

#### 实验设置

* **数据集**：使用ImageNet-1K验证集的图像作为干净图像，从MS-COCO标题中随机选择目标文本。
* **模型**：评估了多个开源大型VLMs，如UniDiffuser、BLIP、BLIP-2、Img2Prompt、LLaVA和MiniGPT-4。
* **攻击参数**：设置扰动预算ϵ = 8，使用ℓ∞范数约束，确保对抗性扰动在人类视觉不可感知的范围内。

通过这些策略，研究者成功地诱导了VLMs生成与预定义目标文本语义相似的响应，展示了这些模型在面对对抗性攻击时的脆弱性。





### 4. 本文创新点与贡献

* 提出了一种新的评估方法，针对接受视觉输入的大型VLMs（例如图像基础文本生成或联合生成）。
* 展示了通过转移基础攻击和查询基础攻击相结合的方法，可以有效地诱导大型VLMs生成目标响应。
* 提供了对大型VLMs对抗性脆弱性的定量理解，并呼吁在实际部署前对其潜在安全缺陷进行全面检查。

### 5. 本文实验

实验基于开源大型模型进行，以确保可重现性。使用ImageNet-1K验证集的图像作为干净图像，并从MS-COCO标题中随机选择目标文本。实验结果表明，所提出的攻击方法能够有效地欺骗VLMs并诱导目标响应。

### 6. 实验结论

实验结果表明，即使是最先进的大型VLMs，也容易受到对抗性攻击的影响。这些攻击可以自动化地执行，有效地欺骗整个大型视觉-语言系统，并且对抗性效果可能影响多轮交互。

### 7. 全文结论

本文通过对抗性攻击揭示了大型VLMs在图像处理方面的安全漏洞。研究结果强调了在将这些模型部署到实际应用中之前，需要对其潜在的安全风险进行更深入的评估。此外，本文的研究可能对多模态系统的安全性评估提供了新的视角，即系统的健壮性高度依赖于其最脆弱的输入模态。

### 阅读总结

本文通过实证研究展示了大型VLMs在面对对抗性攻击时的脆弱性，并提出了一种新的评估方法。研究不仅揭示了模型的潜在安全风险，还为未来的安全研究提供了新的方向。这些发现对于理解和改进大型多模态模型的安全性至关重要。
