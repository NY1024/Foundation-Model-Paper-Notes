# Black Box Adversarial Prompting  for Foundation Models

<figure><img src="../.gitbook/assets/image (12) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

本研究探讨了基础模型（Foundation Models, FMs）在视觉和语言生成任务中的应用，特别是通过自然语言提示来影响模型输出的方式。基础模型，如大型语言模型（LLMs）和文本到图像模型（TTIMs），在给定简短语言提示时能够执行复杂的查询和生成任务。然而，提示的微小变化可能导致模型输出的显著差异，这表明了对提示的敏感性。

### 2. 过去方案和缺点

以往的研究主要集中在通过修改文本输入来改变模型预测的对抗性攻击策略上，这些策略需要一个干净的文本示例作为修改的基础。然而，对于提示来说，并没有这样的干净示例，因为提示是从零开始设计的。此外，这些攻击通常需要梯度信息和修改LLM权重的能力，这在封闭源的通用基础模型上是不可行的。

<figure><img src="../.gitbook/assets/image (13) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

研究者提出了一个黑盒框架，用于为非结构化图像和文本生成生成对抗性提示。这些提示可以独立使用或作为良性提示的前缀，以诱导生成过程中的特定行为，例如生成特定对象的图像或生成高困惑度的文本。研究者开发了一个Token Space Projection（TSP）来将连续的低维嵌入空间映射到离散的语言标记集，从而使得黑盒攻击能够找到对抗性提示。

### 4. 本文创新点与贡献

* 提出了一个在黑盒设置中寻找对抗性提示的框架，特别是针对那些只有API访问权限的基础模型。
* 开发了Token Space Projection（TSP）操作符，将连续的词嵌入空间与离散的标记空间连接起来，使得黑盒攻击能够找到对抗性提示。
* 展示了该框架能够自动找到独立或前缀提示，这些提示会导致文本到图像模型输出特定类别的图像，或者找到排除与目标类别相关标记的对抗性提示。

### 5. 本文实验

实验在图像生成和文本生成任务上进行，使用了Stable Diffusion v1.5模型和Vicuna 13B-v1.1语言模型。实验结果表明，通过优化提示，可以有效地操纵基础模型生成预期之外的结果。

### 6. 实验结论

实验结果证明了对抗性提示在操纵基础模型输出方面的有效性，即使在只使用少量标记增强的情况下也是如此。这些技术未来可能发展为允许实践者通过后端提示工程来更灵活地控制提示。

### 7. 全文结论

本文提出了一种系统化和自动化的机制，用于操纵生成模型产生非预期结果。尽管这种方法可能被用于绕过对齐安全措施，但它也可能促进更系统的对齐过程。考虑到手动制作“越狱”提示的相对容易性，自动化方法的优势目前超过了潜在的缺点。

### 阅读总结

本文介绍了一种新的对抗性提示框架，用于操纵基础模型的输出。通过Token Space Projection技术，研究者能够在黑盒环境中有效地找到对抗性提示，这些提示能够诱导模型生成特定类别的图像或高困惑度的文本。这一发现对于理解和改进基础模型的安全性具有重要意义。
