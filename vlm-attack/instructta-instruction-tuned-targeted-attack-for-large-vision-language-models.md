# INSTRUCTTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>



#### 1. 研究背景

大型视觉-语言模型（LVLMs）在图像理解和响应生成方面展现出了卓越的能力。然而，这种丰富的视觉交互也使得LVLMs对对抗性示例（Adversarial Examples, AEs）变得脆弱。对抗性攻击，尤其是目标攻击（Targeted Attacks），可能会误导LVLMs产生与攻击者选择的目标文本语义相似的响应，这在安全领域引起了严重关切。

#### 2. 过去方案和缺点

以往的研究主要集中在白盒攻击和黑盒攻击上，这些攻击要么完全了解目标模型的参数和结构，要么只能访问模型的输入和输出。然而，这些设置在实际应用中并不总是可行的，因为攻击者可能无法获取到模型的全部信息。此外，现有方法在跨提示（cross-prompt）和跨模型（cross-model）的攻击转移性方面面临挑战，因为目标LVLM可能使用不同的大型语言模型（LLM）后端和未知的指令。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### 3. 本文方案和步骤

本文提出了一种名为INSTRUCTTA的指令调整目标攻击方法。该方法首先使用公共文本到图像生成模型将目标响应“反向”转换为目标图像，并利用GPT-4推断出合理的指令。然后，构建一个与受害者LVLM共享视觉编码器的本地替代模型，提取对抗性图像示例和目标图像的指令感知特征，并通过最小化这两个特征之间的距离来优化对抗性示例。为了进一步提高转移性，通过从LLM中提取的指令对p'进行扩展。

#### 4. 本文创新点与贡献

* 提出了INSTRUCTTA，这是一种在实际灰盒设置下对LVLMs进行高效攻击的方法。
* 利用文本到图像模型和LLMs推断出合理的目标图像和指令，并通过指令调整范式增强对抗样本的转移性。
* 通过广泛的实验证明了INSTRUCTTA在目标攻击性能和转移性方面的优越性。

#### 5. 本文实验

实验部分详细介绍了评估设置，包括使用的数据集、评估协议、基线模型和实现细节。实验结果表明，INSTRUCTTA在多个受害LVLMs上的表现优于现有方法，包括MF-it和MF-ii。

#### 6. 实验结论

实验结果证实了INSTRUCTTA在目标攻击性能和转移性方面的优势。特别是，INSTRUCTTA在处理复杂推理指令时仍然表现出色，这表明该方法能够有效地应对各种攻击场景。

#### 7. 全文结论

本文通过提出INSTRUCTTA，展示了在实际灰盒场景下对LVLMs进行目标攻击的新方法。该方法不仅提高了攻击的转移性，而且还强调了指令在提高攻击效果中的重要性。此外，文章还讨论了INSTRUCTTA的伦理考量和可能的缓解措施，为未来提高LVLMs对抗性攻击的鲁棒性提供了方向。



注：

本文通过提出INSTRUCTTA方法来增强对抗样本的转移性。转移性是指对抗样本在不同的模型或设置下仍然能够有效地误导目标模型的能力。以下是INSTRUCTTA增强对抗样本转移性的具体步骤和策略：

1. **指令推断（Instruction Inference）**：
   * 使用GPT-4从目标响应中推断出合理的指令（p'）。这是因为攻击者通常无法获取目标LVLM使用的具体指令（prompts），而指令对于生成与目标文本语义相似的响应至关重要。
   * 通过这种方式，攻击者可以生成与目标LVLM可能使用的指令语义相近的指令，从而提高对抗样本在不同指令下的通用性。
2. **本地替代模型（Local Surrogate Model）**：
   * 构建一个与受害者LVLM共享视觉编码器的本地替代模型。这个替代模型用于提取对抗性图像示例和目标图像的指令感知特征。
   * 通过使用与目标LVLM相同的视觉编码器，替代模型能够更好地模拟目标模型的行为，从而提高对抗样本的跨模型转移性。
3. **特征距离最小化（Feature Distance Minimization）**：
   * 通过最小化替代模型提取的对抗性图像示例和目标图像之间的特征距离来优化对抗性示例。
   * 这种方法使得对抗样本在视觉特征层面与目标图像更加相似，从而在不同的LVLMs中产生类似的响应。
4. **指令扩展（Instruction Augmentation）**：
   * 使用ChatGPT（即GPT-3.5）对推断出的指令p'进行改写，生成一组语义相近的指令（p'i和p'j）。
   * 通过这种方式，攻击者可以生成多个具有相似语义的指令版本，这些指令可以用于不同的LVLMs，从而提高对抗样本在不同指令集下的转移性。
5. **双目标攻击与优化（Dual Targeted Attack & Optimization）**：
   * 结合MF-it和MF-ii两种攻击策略，通过匹配对抗性示例和目标响应的嵌入向量来实现转移目标攻击。
   * 这种双策略方法进一步提高了对抗样本在不同模型和指令配置下的鲁棒性和有效性。

通过上述步骤，INSTRUCTTA方法能够有效地增强对抗样本的转移性，使其能够在不同的LVLMs和指令集下成功执行目标攻击。这种方法的成功在于它不仅考虑了视觉特征的相似性，还考虑了指令语义的多样性和通用性，从而提高了对抗样本在不同环境下的适应性和有效性。



#### 阅读总结报告

本论文针对大型视觉-语言模型（LVLMs）的安全性问题，提出了一种新的灰盒攻击场景下的指令调整目标攻击方法INSTRUCTTA。该方法通过结合文本到图像的生成模型和GPT-4，有效地推断出目标图像和指令，从而生成能够误导LVLMs的对抗性示例。实验结果显示，INSTRUCTTA在目标攻击性能和跨模型转移性方面均优于现有方法。本文的研究不仅揭示了LVLMs在实际应用中的脆弱性，也为未来的安全防护提供了新的视角和可能的解决方案。
