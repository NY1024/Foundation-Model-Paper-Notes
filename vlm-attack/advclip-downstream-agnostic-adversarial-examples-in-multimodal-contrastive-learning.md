# AdvCLIP: Downstream-agnostic Adversarial Examples in Multimodal Contrastive Learning

<figure><img src="../.gitbook/assets/image (148).png" alt=""><figcaption></figcaption></figure>

## 研究背景

多模态对比学习（Multimodal Contrastive Learning）旨在训练一个通用的特征提取器，例如CLIP，通过在大量原始、未标记的成对图像-文本数据上进行训练。这种方法对于各种复杂的下游任务（包括跨模态图像-文本检索和图像分类）具有显著的益处。然而，尽管前景看好，跨模态预训练编码器的安全性问题尚未得到充分探索，尤其是当预训练编码器公开用于商业用途时。

<figure><img src="../.gitbook/assets/image (149).png" alt=""><figcaption></figcaption></figure>

## 过去方案和缺点

以往的研究主要集中在单模态预训练编码器的隐私和鲁棒性问题上，而对于更广泛使用的跨模态预训练编码器（例如视觉-语言预训练（VLP）编码器）的安全性威胁研究不足。尽管有研究尝试对VLP编码器的下游任务进行对抗性攻击，但它们依赖于不切实际的白盒假设来生成特定于样本的对抗性示例。在多模态攻击的文献中，由于不同模态之间的异质性，造成了一种错觉，即在没有预训练数据集、下游数据集、任务类型甚至下游模型采取的防御策略知识的情况下，实现跨模态攻击是不可能的。

## 本文方案和步骤

本文提出了AdvCLIP，这是第一个基于跨模态预训练编码器生成下游不可知对抗性示例的攻击框架。AdvCLIP的目标是构建一个通用的对抗性补丁，用于一组自然图像，以欺骗继承受害者跨模态预训练编码器的所有下游任务。为了解决不同模态之间的异质性和未知下游任务的挑战，首先构建了一个拓扑图结构来捕捉目标样本及其邻居之间的相关位置。然后，设计了一个基于拓扑偏差的生成对抗网络来生成通用对抗性补丁。通过将补丁添加到图像中，最小化它们在不同模态中的嵌入相似性，并在特征空间中扰动样本分布，实现通用非目标攻击。

## 本文创新点与贡献

* 提出了AdvCLIP，这是第一个在多模态对比学习中构建下游不可知对抗性示例的攻击框架。
* 设计了一个基于拓扑偏差的生成对抗网络，通过添加通用对抗性补丁到目标图像上，降低不同模态嵌入之间的相似性，并破坏它们的拓扑关系，实现非目标对抗性攻击。
* 在两种类型的下游任务上对八个数据集进行了广泛的实验，证明了AdvCLIP在跨模态预训练编码器和下游任务之间的模态差异和可转移性方面的表现。
* 针对AdvCLIP量身定制了三种流行的防御措施。结果进一步证明了AdvCLIP的攻击能力，并强调了需要新的防御机制来保护预训练编码器。

## 本文实验

实验部分评估了AdvCLIP在不同下游任务（图像-文本检索和图像分类）上的有效性。实验结果表明，AdvCLIP能够在不同的CLIP架构和数据集上实现显著的攻击性能。此外，还对AdvCLIP进行了消融研究，探讨了不同模块、攻击强度和批量大小对AdvCLIP性能的影响。

## 实验结论

AdvCLIP在面对未知下游任务时，能够有效地生成对抗性示例，并对下游任务造成显著影响。尽管存在一些防御措施，但AdvCLIP仍然能够成功地执行攻击。

## 全文结论

本文提出了AdvCLIP攻击框架，展示了跨模态预训练编码器在实际应用中可能面临的严重安全威胁，并强调了开发新的防御机制的重要性。尽管AdvCLIP在实验中取得了成功，但仍存在一些局限性，例如对复杂任务的攻击以及更健壮的防御方法。



注1：\
跨模态预训练编码器（Cross-modal Pre-trained Encoder）是指一类能够处理和理解多种不同模态（如图像、文本、声音等）数据的机器学习模型。在多模态对比学习（Multimodal Contrastive Learning）的背景下，这类编码器通常在大规模的未标记数据集上进行预训练，以学习到能够跨模态理解的通用特征表示。

以CLIP（Contrastive Language-Image Pre-training）为例，它是一个著名的跨模态预训练编码器，通过在大量图像和文本对上进行训练，学习到能够将图像和文本映射到共同嵌入空间的特征表示。这种编码器能够捕捉图像和文本之间的语义关联，使得模型能够在没有明确标签的情况下，通过对比学习来理解不同模态之间的关联。

跨模态预训练编码器的关键优势在于其能够直接应用于多种下游任务，如图像-文本检索、视觉问答（Visual Question Answering, VQA）和图像分类等，而无需对每个特定任务进行额外的训练。这种模型通常具有强大的零样本（Zero-Shot）或少样本（Few-Shot）学习能力，可以在有限的标注数据下实现高效的任务迁移和微调。



注2：

本文提出的方法能够构建下游不可知对抗性示例（Downstream-agnostic Adversarial Examples）的原因在于其设计和实现的几个关键点，这些点共同克服了跨模态预训练编码器在安全性方面的挑战：

1. **拓扑图结构**：本文首先构建了一个拓扑图结构，用于捕捉目标样本及其邻居之间的关系。这种结构有助于理解样本在特征空间中的相似性和差异性，从而为生成对抗性补丁提供了基础。
2. **拓扑偏差生成对抗网络**：通过设计一个基于拓扑偏差的生成对抗网络（GAN），本文能够生成一种通用的对抗性补丁。这种补丁能够在不依赖于特定下游任务知识的情况下，对预训练编码器的特征表示进行有效的扰动。
3. **非目标攻击**：AdvCLIP采用非目标攻击策略，即不针对特定的类别进行攻击，而是通过增加样本特征向量之间的距离，使得样本在特征空间中的分布被扰动。这种方法使得对抗性示例能够普遍地影响下游任务的性能，而不需要知道下游任务的具体细节。
4. **攻击转移性**：为了确保攻击能够从预训练编码器转移到下游任务，本文的方法不仅简单地使样本跨越决策边界，而是使对抗性示例远离原始类别，即使在下游任务的微调过程中，这些示例仍然难以被正确识别。
5. **对抗性补丁的通用性**：通过在不同的数据集上训练生成对抗性网络，本文生成的对抗性补丁具有较高的通用性，这意味着它们可以应用于多种不同的下游任务，而不仅仅是针对特定的任务或数据集。
6. **严格的攻击者知识限制**：在本文的威胁模型中，攻击者被假设为具有有限的知识，他们无法访问预训练数据集和下游任务的具体信息。这增加了攻击的难度，但AdvCLIP仍然能够成功地执行攻击，这表明其方法的有效性和鲁棒性。

通过这些设计，AdvCLIP能够在不知道下游任务具体细节的情况下，生成能够普遍影响跨模态预训练编码器的对抗性示例，从而揭示了这些编码器在实际应用中的潜在安全风险。





## 阅读总结报告

AdvCLIP是一个创新的攻击框架，它针对多模态对比学习中的跨模态预训练编码器，特别是CLIP模型，提出了一种新的下游不可知对抗性攻击方法。这种方法通过构建拓扑图结构和生成对抗网络，有效地生成了能够欺骗下游任务的通用对抗性补丁。实验结果表明，AdvCLIP在多种下游任务和数据集上都表现出了强大的攻击能力。此外，本文还探讨了针对AdvCLIP的防御措施，进一步证明了其攻击能力的普遍性和对现有防御机制的挑战。这项工作不仅揭示了多模态预训练编码器在安全性方面的潜在风险，也为未来的安全研究提供了新的研究方向。
