# Test-Time Backdoor Attacks on Multimodal Large Language Models

<figure><img src="../.gitbook/assets/image (7) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 随着多模态大型语言模型（MLLMs）在视觉-语言场景中的显著进步，它们在机器人、虚拟助手等领域的应用日益广泛。然而，MLLMs在收集外部（不可信）数据时面临后门攻击的风险。后门攻击通过污染训练数据，在测试阶段通过特定触发器激活预定的有害效果。尽管已有研究致力于净化被污染的训练数据或检测触发模式，但这些方法通常需要访问或修改训练数据，或者需要修改模型权重或结构。
2. 过去方案和缺点： 以往的后门攻击防御方法主要集中在数据层面，如净化训练数据或检测触发模式。这些方法的缺点在于它们可能无法应对不需要访问训练数据的后门攻击，或者在模型权重或结构上进行修改可能会影响模型的正常功能。此外，这些方法可能无法适应MLLMs的多模态特性，即同时处理视觉和文本输入的能力。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤： 本文提出了一种名为AnyDoor的测试时后门攻击方法，它能够在不需要访问或修改训练数据的情况下，通过对抗性测试图像（共享相同的通用扰动）将后门注入到文本模态。AnyDoor采用了与通用对抗性攻击相似的技术，但区别在于它能够解耦设置和激活有害效果的时间。具体步骤包括：
   * 使用通用对抗性扰动对输入图像进行处理，以便在测试阶段设置后门。
   * 使用触发提示激活有害效果。
   * 通过调整对抗性扰动，可以动态改变后门触发提示或有害效果。

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文实验和性能： 实验中，作者对流行的MLLMs（如LLaVA-1.5、MiniGPT-4、InstructBLIP和BLIP-2）进行了攻击，并提供了全面的消融研究。实验结果表明，AnyDoor能够有效地对这些模型进行攻击，并且能够动态改变后门触发提示或有害效果，为后门攻击的防御设计带来了新的挑战。

阅读总结报告： 本文提出了一种新型的后门攻击方法AnyDoor，它针对多模态大型语言模型，能够在测试阶段通过视觉输入注入后门，并通过文本输入激活这些后门。这种方法不需要访问或修改训练数据，也不需要改变模型的权重或结构，从而绕过了现有的后门防御机制。实验结果证明了AnyDoor的有效性，并展示了其在不同MLLMs上的攻击性能。这项工作不仅揭示了MLLMs在安全性方面的潜在风险，也为未来防御策略的设计提供了新的视角。



注1：

本文提出的AnyDoor攻击方法能够在测试阶段通过视觉输入注入后门，并通过文本输入激活这些后门，主要基于以下几个关键点：

1. **多模态特性**：MLLMs通常能够处理视觉（如图像）和文本（如问题或指令）两种模态的输入。这种多模态能力使得模型在处理不同类型的输入时可能存在不同的脆弱性。
2. **通用对抗性扰动**：AnyDoor利用了通用对抗性扰动（universal adversarial perturbations），这种扰动可以应用于任何输入图像上，而不需要针对特定图像进行定制。这意味着攻击者可以创建一个扰动，当这个扰动被应用到MLLM的输入图像上时，模型的行为就会被操纵。
3. **解耦设置和激活**：传统的后门攻击需要在训练阶段设置后门，并在测试阶段通过特定的触发器激活。AnyDoor方法将这两个过程解耦，允许攻击者在测试阶段同时设置和激活后门。这样，攻击者可以在不接触训练数据的情况下，通过精心设计的输入来操纵模型的行为。
4. **动态触发器和效果**：AnyDoor允许攻击者通过改变对抗性扰动来动态改变触发器和有害效果。这意味着攻击者可以根据需要调整攻击策略，以适应不同的防御措施或逃避检测。
5. **测试时攻击**：与需要在训练数据中植入后门的攻击不同，AnyDoor是一种测试时攻击，它在模型部署后的测试阶段执行。这使得攻击更加隐蔽，因为攻击者不需要访问模型的训练过程或数据集。

总结来说，AnyDoor利用了MLLMs的多模态特性和通用对抗性扰动的能力，在测试阶段对模型进行攻击，通过视觉输入注入后门，然后通过文本输入触发这些后门，从而绕过了传统的后门防御机制。这种方法的灵活性和隐蔽性为MLLMs的安全性带来了新的挑战。



注2：

"测试时后门"（Test-Time Backdoor）这个术语指的是一种后门攻击，它在模型的测试阶段（即模型部署并用于实际任务时）而不是在训练阶段被激活和执行。这种攻击与传统的后门攻击有所不同，因为传统的后门攻击通常在模型训练过程中通过污染训练数据来植入后门。以下是"测试时后门"这个术语的几个关键点：

1. **时机**：测试时后门攻击发生在模型已经训练完成并开始接收输入进行预测的时候。这与训练时后门攻击相对，后者在模型训练过程中通过修改训练数据来植入后门。
2. **无需访问训练数据**：由于测试时后门攻击在测试阶段执行，攻击者不需要访问或修改模型的训练数据。这降低了攻击的难度，因为攻击者不需要获取或篡改训练集。
3. **隐蔽性**：测试时后门攻击在模型部署后进行，这使得攻击更加隐蔽，因为它们不依赖于训练数据的修改，而是通过精心设计的输入在模型运行时触发。
4. **动态性**：测试时后门攻击允许攻击者在模型运行时动态地改变触发器和有害效果，这为攻击提供了更大的灵活性和适应性。
5. **挑战性**：这种攻击方式为模型的安全性带来了新的挑战，因为传统的后门防御策略通常针对训练数据的净化和触发模式的检测，而测试时后门攻击则需要新的防御机制来识别和阻止在测试阶段执行的攻击。

在本文中提出的AnyDoor攻击方法，正是利用了这些特点，通过在测试阶段对MLLMs的输入图像施加通用对抗性扰动，从而在不接触训练数据的情况下，实现对模型的后门攻击。这种攻击方式强调了在模型部署后进行安全评估的重要性，并推动了对新型防御策略的研究。
