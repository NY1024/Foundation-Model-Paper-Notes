# How Robust is Google’s Bard to Adversarial Image  Attacks?

<figure><img src="../.gitbook/assets/image (12) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

本研究探讨了多模态大型语言模型（MLLMs），特别是集成了文本和视觉（尤其是图像）的模型，在多模态任务中取得了前所未有的性能。然而，由于视觉模型的对抗鲁棒性问题尚未解决，MLLMs在引入视觉输入时可能面临更严重的安全和安全风险。

<figure><img src="../.gitbook/assets/image (13) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 2. 过去方案和缺点

以往的研究主要集中在开源MLLMs上，如MiniGPT4，而对商业MLLMs（如Google的Bard）的鲁棒性研究较少。商业MLLMs作为黑盒模型，具有未知的模型配置和训练数据集，参数更多，性能更好，且配备了复杂的防御机制，这使得攻击它们更具挑战性。此外，现有的对抗性攻击方法主要针对图像分类模型，而MLLMs与传统分类器的差异较大，需要探索有效的策略来欺骗商业MLLMs。

### 3. 本文方案和步骤

研究者们针对Google的Bard进行了对抗性鲁棒性研究。首先，他们考虑了图像描述任务的对抗性攻击，通过生成对抗性图像来误导Bard输出错误的图像描述。他们采用了基于转移的攻击方法，通过优化图像嵌入或文本描述的目标函数来实现攻击。其次，他们识别了Bard的两种防御机制：面部检测和图像毒性检测，并设计了相应的攻击方法来规避这些防御。



文中生成对抗性图像的方法主要基于两种攻击策略：图像嵌入攻击和文本描述攻击。以下是这两种攻击的具体步骤：

#### 图像嵌入攻击

1. **目标函数定义**：攻击的目标是最大化对抗性图像与原始图像在图像嵌入空间中的距离。这可以通过最小化代理图像编码器（如ViT-B/16、CLIP和BLIP-2）生成的图像嵌入之间的距离来实现。
2. **优化过程**：为了解决这个优化问题，研究者们采用了基于转移的攻击方法，特别是频谱模拟攻击（SSA）和共同弱点攻击（CWA）。SSA通过对输入进行频谱变换以提高对抗性样本的转移性。CWA旨在通过促进代理模型损失景观的平坦性和局部最优解之间的接近性来找到代理模型集合的共同弱点。
3. **攻击实施**：研究者们将SSA和CWA结合起来，形成SSA-CWA攻击，以提高对抗性样本对黑盒模型的转移性。在实验中，他们设置了扰动预算ϵ = 16/255，并在ℓ∞范数下进行攻击。

#### 文本描述攻击

1. **目标函数定义**：这种攻击直接针对整个生成文本的流程，目的是使生成的描述与正确描述不同。研究者们收集了一组代理MLLMs（如BLIP-2、InstructBLIP和MiniGPT-4），这些模型可以预测给定图像、文本提示和之前预测的单词的下一个单词的概率分布。
2. **优化过程**：攻击者通过最大化预测目标句子的对数似然来生成对抗性图像。这里采用的是有针对性的攻击，而不是无目标攻击，因为一张图像可能有多个正确的描述。如果只最小化预测单个真实描述的对数似然，模型仍然可能输出其他正确的描述，这将使攻击无效。
3. **攻击实施**：与图像嵌入攻击类似，研究者们也采用了SSA-CWA方法来解决文本描述攻击的优化问题。

#### 实验设置

* **数据集**：实验使用了NIPS17数据集的100张图像。
* **代理模型**：对于图像嵌入攻击，使用了ViT-B/16、CLIP和BLIP-2作为代理图像编码器。对于文本描述攻击，选择了BLIP-2、InstructBLIP和MiniGPT-4作为代理MLLMs。
* **超参数**：扰动预算设置为ϵ = 16/255，攻击迭代次数设置为500。

通过这些步骤，研究者们成功生成了一系列对抗性图像，这些图像能够误导Bard以及其他MLLMs输出错误的图像描述。





### 4. 本文创新点与贡献

* 对商业MLLMs（特别是Bard）的对抗性鲁棒性进行了深入研究。
* 提出了针对图像描述任务的对抗性攻击方法，并展示了对抗性图像在其他MLLMs（如Bing Chat和ERNIE Bot）上的转移性。
* 揭示了Bard的防御机制（面部检测和毒性检测）在对抗性攻击下的脆弱性，并提出了相应的攻击方法。

### 5. 本文实验

实验使用了NIPS17数据集的100张图像，以及ViT-B/16、CLIP和BLIP-2等作为代理模型。实验结果显示，图像嵌入攻击对Bard的成功率为22%，而文本描述攻击的成功率为10%。此外，对抗性图像在GPT-4V、Bing Chat和ERNIE Bot上的攻击成功率分别为45%、26%和86%。

<figure><img src="../.gitbook/assets/image (14) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../.gitbook/assets/image (15) (1).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../.gitbook/assets/image (16).png" alt=""><figcaption></figcaption></figure>

### 6. 实验结论

实验表明，即使是大型视觉-语言模型如Bard，也容易受到对抗性攻击的影响，并且可以轻易地误识别对抗性图像中的对象。此外，Bard的防御机制在对抗性攻击下并不足够强大。

### 7. 全文结论

本文通过对抗性攻击揭示了商业MLLMs在图像描述任务中的脆弱性，并指出了当前防御机制的不足。研究者们希望这项工作能够加深对MLLMs对抗性鲁棒性的理解，并促进未来研究开发更健壮、更值得信赖的多模态基础模型。

### 阅读总结

本文通过对Google的Bard进行对抗性攻击研究，展示了商业MLLMs在图像处理方面的安全漏洞。研究不仅揭示了模型的脆弱性，还提出了针对其防御机制的攻击方法。这些发现对于理解MLLMs的安全风险至关重要，并为未来的安全研究提供了新的方向。
