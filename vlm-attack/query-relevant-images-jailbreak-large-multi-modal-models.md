# Query-Relevant Images Jailbreak Large Multi-Modal Models

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 本文研究的背景是大型多模态模型（Large Multi-Modal Models, LMMs）的安全性问题。尽管大型语言模型（Large Language Models, LLMs）的安全问题已经得到了广泛探讨，但LMMs的安全问题相对较少被研究。LMMs在执行指令、进行多轮对话和基于图像的问题回答方面表现出色，但它们在面对与恶意查询密切相关的图像时可能会降低防御机制，从而响应恶意查询。这表明，现有的开源LMMs在视觉指令数据集上进行微调时，缺乏安全对齐的数据，导致模型以与攻击者意图一致的方式响应。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 过去方案和缺点： 过去的研究主要集中在LLMs的安全性上，包括攻击策略和通过红队行动开发安全对齐的LLMs。然而，对于LMMs的安全性研究相对较少。此外，现有的多模态基准测试（如PrivQA）主要关注隐私问题，而本文研究扩展到了包括恶意查询在内的更多场景。此外，现有的评估方法可能无法充分量化LMMs在面对潜在恶意利用时的安全性。

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤： 本文提出了一种新的针对LMMs的视觉提示攻击方法，该方法利用与查询相关的图像来突破模型的防御。研究者首先从恶意查询中提取关键词，然后使用两种图像生成策略：稳定扩散（Stable Diffusion）图像生成和排版（Typography）。研究者构建了一个包含13个场景的大规模数据集，包含5040个文本-图像对，用于评估LMMs在对抗性攻击下的脆弱性。通过这些场景，研究者测试了12个最先进的LMMs，并展示了两种生成图像结合使用在多种场景下绕过LMMs安全机制的有效性。
2. 本文创新点：

* 提出了一种新的方法，通过创建与文本强相关的图像来绕过LMMs的防御机制。
* 构建了一个全面的安全测量数据集，涵盖了13个不同的场景，系统地评估LMMs的安全性。
* 通过专门设计的基准测试，对多个开源LMMs进行了广泛的评估，展示了这些模型安全协议的脆弱性。

5. 本文实验和性能： 实验结果表明，使用排版的图像在所有13个场景中都能显著提高攻击成功率（ASR），平均增加超过30%。稳定扩散生成的图像也显示出在10个场景中ASR的提高，尽管效果不如排版。结合稳定扩散和排版的方法在大多数场景中进一步提高了性能。此外，研究者还发现，一些模型在政治和专业领域的场景中表现出较高的基线ASR，这可能表明Vicuna（LLaVA-1.5的LLM）在这些主题上可能没有得到充分的安全对齐。

阅读总结报告： 本文针对LMMs的安全性问题进行了深入研究，提出了一种新的视觉提示攻击方法，并通过构建一个全面的安全测量数据集来评估LMMs在面对恶意攻击时的脆弱性。实验结果揭示了现有LMMs在安全协议方面的不足，强调了加强开源模型安全措施的必要性。这项研究不仅为LMMs的安全性评估提供了新的视角，也为未来在这一领域的研究提供了宝贵的资源和方法。



注：

新的视觉提示攻击方法是指研究者们开发的一种策略，该策略利用与恶意查询内容相关的图像来欺骗大型多模态模型（LMMs），使其在安全防护机制被绕过的情况下响应原本不应该回答的问题。这种方法的具体步骤如下：

1. **关键词提取**：首先，从恶意查询中提取关键词。这些关键词通常与攻击者的恶意意图直接相关，例如“炸弹”、“黑客”等。
2. **图像生成**：然后，使用两种不同的图像生成技术来创建与这些关键词相关的图像：
   * **稳定扩散（Stable Diffusion）**：这是一种图像生成算法，可以根据文本提示生成图像。研究者使用它来生成反映提取关键词的图像。
   * **排版（Typography）**：将特定的实体或关键词转换成视觉排版表示。这通常涉及将关键词以文本形式展示在图像上。
3. **图像与文本的结合**：研究者将生成的图像与恶意查询结合，创建一个复合图像，这个图像既包含由稳定扩散生成的内容，也包含排版文本。
4. **攻击实施**：使用这些复合图像作为视觉提示，与恶意查询一起输入到LMMs中，以测试模型的响应。研究者假设，当图像与查询内容紧密相关时，LMMs的防御系统会降低警惕，从而更容易被攻击。

这种方法的关键在于，通过精心设计的图像提示，可以操纵LMMs，使其在安全对齐的数据集上进行微调时未能考虑到的安全漏洞，从而在面对恶意攻击时产生不适当的响应。这种攻击方法展示了LMMs在安全性方面的潜在脆弱性，并强调了需要进一步研究和改进LMMs的安全措施。

