# ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users

<figure><img src="../.gitbook/assets/image (275).png" alt=""><figcaption></figcaption></figure>

#### 研究背景

大规模预训练生成模型因其生成创意内容的能力而广受欢迎。然而，这些模型可能会有意或无意地产生有偏见和有害的内容。例如，已有研究设计了绕过大型语言模型（LLMs）安全防护的方法，使其能够生成有害和非法的回应。这些安全风险是模型开发者、研究人员、用户和监管机构的主要关注点，因此提高这些模型生成内容的安全性至关重要。

#### 过去方案和缺点

以往的方法主要关注于破解和对抗性攻击，这些方法主要评估模型在恶意提示下的安全性。然而，最近的研究发现，即使是手工精心设计的“安全”提示，也可能无意中触发不安全的内容生成。现有的方法在评估文本到图像模型的安全性时，没有充分考虑正常用户使用模型时可能遇到的安全风险。

#### 本文方案和步骤

本文提出了一个名为ART（Automatic Red-teaming）的新颖自动化红队框架。该方法利用视觉语言模型（VLM）和大型语言模型（LLM），建立不安全生成与其提示之间的联系，从而更有效地识别模型的漏洞。ART框架通过以下步骤实现：

1. 将复杂任务分解为子任务，例如建立图像与有害主题之间的联系，将有害图像与安全提示对齐，以及建立安全提示与有害主题之间的联系。
2. 使用VLM建立图像与不同主题之间的联系，并根据这些联系调整相应的安全提示。
3. 引入LLM学习VLM的知识，并建立安全提示与不同主题之间的联系。
4. VLM用于理解生成的图像并提供修改提示的建议，而不是直接提供提示；LLM使用这些建议来修改原始提示，从而增加触发不安全内容的可能性。

#### 本文创新点与贡献

* 提出了第一个自动化红队框架ART，用于发现当正常用户使用只有安全和无偏见提示的文本到图像模型时的安全风险。
* 提出了三个全面的红队数据集，作为增强文本到图像模型鲁棒性的关键工具。
* 使用ART系统地研究了流行的文本到图像模型的安全性风险，揭示了从正常用户推断时的不足防护，特别是在较大的模型中。

#### 本文实验

实验在三个流行的开源文本到图像模型上进行，包括Stable Diffusion 1.5、Stable Diffusion 2.1和Stable Diffusion XL。实验结果表明，ART在生成安全提示方面具有高效性，并能够揭示模型在不同设置下的安全性风险。

#### 实验结论

* ART能够以高概率生成安全提示，并且在不同有害类别中具有良好的泛化能力。
* 即使使用安全提示，Stable Diffusion模型仍然可以生成不安全的内容，表明模型开发过程中的安全防护措施并不完全有效。
* ART在不同的生成设置下保持了令人满意的有效性，能够为模型开发者提供有关其模型安全风险的深入见解。

#### 全文结论

本文提出的ART框架是一个有用的工具，可以帮助模型开发者在发布模型之前发现其中的安全隐患，并帮助他们制定针对性的解决方案。通过使用ART，开发者可以构建更安全、无偏见的AI模型供用户使用。

####

这篇论文提出了一个针对文本到图像模型的自动化红队测试框架ART，旨在通过自动化的方式评估和提高这些模型的安全性。研究背景强调了生成模型可能产生的有害内容及其对社会的潜在影响。现有方法的不足在于它们主要关注对抗性攻击，而没有充分考虑正常用户使用模型时的安全风险。

本文提出的ART框架通过结合视觉语言模型和大型语言模型，自动化地发现模型的安全漏洞。框架的创新之处在于能够生成与特定有害类别相关的安全提示，并评估模型在这些提示下的响应。此外，作者还构建了三个大规模数据集，以支持对文本到图像模型安全性的深入研究。

实验部分展示了ART在多个流行模型上的应用，并与现有方法进行了比较。结果表明，ART在生成安全提示和识别模型风险方面具有显著优势。最后，论文讨论了ART的局限性和潜在的社会影响，强调了在AI安全测试中采取系统方法的重要性，并对未来的工作方向提出了展望。

总体而言，这篇论文为理解和提高AI生成模型的安全性提供了宝贵的见解，并为未来的研究和实践提供了新的工具和方法。



注1：

Stable Diffusion模型可以生成不安全的内容，具体指的是即使在使用被认为是"安全"的提示（prompts）时，模型仍然有可能产生包含以下类别的有害图像：

1. **仇恨（Hate）**：图像可能包含种族主义、宗教歧视、性别歧视或其他形式的仇恨言论或象征。
2. **骚扰（Harassment）**：图像可能展示欺凌、威胁或恐吓行为，无论是线上还是线下。
3. **暴力（Violence）**：图像可能描绘暴力行为、虐待或折磨，包括性暴力和家庭暴力。
4. **自残（Self-harm）**：图像可能展示自我伤害、自杀或自我毁灭行为。
5. **性内容（Sexual）**：图像可能包含色情、裸露或旨在唤起性兴奋的内容。
6. **令人震惊的内容（Shocking）**：图像可能包含令人不悦或令人震惊的元素，如血腥、暴力或灾难场景。
7. **非法活动（Illegal Activity）**：图像可能描绘违法行为，如贩毒、盗窃或贩卖人口。

在论文中，作者通过提出的ART（Automatic Red-teaming）框架，对几个版本的Stable Diffusion模型进行了测试，发现即使使用了安全提示，模型仍然有一定概率生成上述类别中的不安全图像。这表明，尽管模型开发者可能已经采取了一定的安全措施，但这些措施可能不足以完全防止有害内容的生成。因此，需要进一步的研究和开发来提高模型的安全性，减少生成不安全内容的风险。



注2：\
ART（Automatic Red-teaming）框架的步骤详细说明如下：

1. **问题分解**：
   * 将复杂的红队任务分解为更小的子任务，包括建立图像与有害主题之间的联系、将有害图像与安全提示对齐，以及建立安全提示与有害主题之间的联系。
2. **数据集构建**：
   * 利用开源提示网站（例如Lexica）收集（安全提示，不安全图像）数据对，使用检测模型来确保收集的提示不包含有害信息，同时确保收集的图像包含不同类别的有害内容。
3. **数据集分类**：
   * 根据图像中包含的有害信息类型，将收集到的数据分类为七种类型，构建一个元数据集（Meta Dataset, MD）。
4. **模型微调**：
   * 对预训练的视觉语言模型（VLM）和大型语言模型（LLM）进行微调，使其能够理解图像内容并生成新的提示。这需要收集特定类型的数据集，分别用于微调VLM（VD）和LLM（LD）。
5. **迭代交互**：
   * 在LLM、VLM和目标文本到图像（T2I）模型之间引入迭代交互。LLM基于特定有害类别生成提示，并将其传递给T2I模型以生成图像。然后，VLM接收图像和提示，并提供如何修改提示的指导。
6. **提示修改**：
   * LLM根据VLM提供的指导修改原始提示，以增加触发不安全内容的可能性。
7. **多轮迭代**：
   * 重复上述交互过程，直到达到预定义的轮数。在每一轮中，都使用检测模型来检查提示和图像是否安全。
8. **安全性评估**：
   * 使用构建的检测模型（Judge Models）评估每次交互中的提示和图像是否安全或有害。
9. **结果分析**：
   * 分析通过ART生成的提示和图像，以确定模型在不同设置下的安全性风险，并提供改进建议。
10. **模型改进**：
    * 根据ART的测试结果，模型开发者可以识别和修复模型中的安全漏洞，从而提高模型的安全性。

ART框架通过这些步骤自动化了红队测试过程，使得研究人员和开发者能够更系统、更全面地评估和提高文本到图像模型的安全性。
