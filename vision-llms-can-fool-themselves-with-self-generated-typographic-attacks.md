# Vision-LLMs Can Fool Themselves with Self-Generated Typographic  Attacks

1. 研究背景： 本研究探讨了视觉-语言模型（Vision-Language Models, LVLMs）对所谓的排版攻击（Typographic Attacks）的脆弱性。排版攻击通过在图像上叠加误导性文本来误导模型，利用模型对文本提示的依赖来解释视觉内容。尽管先前的研究已经表明排版攻击对CLIP等视觉-语言模型的性能有损害，但针对最新大型视觉-语言模型的这类攻击的研究仍然不足。
2. 过去方案和缺点： 以往的研究中，排版攻击主要是随机从预定义的类别集合中选择一个误导类别来攻击CLIP模型。这种简单策略忽略了更有效的攻击，这些攻击利用LVLMs更强的语言技能。此外，以往的方法没有考虑到LVLMs可能对文本信息的光学识别（OCR）能力，这可能导致模型的预测受到影响。
3. 本文方案和步骤： 研究者首先介绍了一个用于测试排版攻击的基准测试，然后提出了两种新型且更有效的自我生成攻击（Self-Generated attacks），这些攻击促使LVLM生成针对自身的攻击。具体包括：
   * 类别基础攻击（Class Based Attack）：LVLM被要求识别与目标类别最相似的欺骗类别。
   * 描述性攻击（Descriptive Attacks）：更高级的LVLM被要求推荐一个包含欺骗类别和描述的排版攻击，以增强攻击的可信度。
4. 本文创新点与贡献：
   * 提出了一个多样化且全面的排版攻击基准测试，用于大型视觉-语言模型（LVLMs）。
   * 展示了新型自我生成排版攻击能够将LVLMs的分类性能降低高达33%。
   * 证明了一个模型生成的自我生成攻击可以泛化到其他模型，例如InstructBLIP和MiniGPT4。
5. 本文实验和性能： 实验使用了五个分类数据集，并在四种最新的大型视觉-语言模型上进行了测试，包括GPT-4V、LLaVA 1.5、MiniGPT4-2和InstructBLIP。实验结果显示，描述性攻击在降低模型性能方面比随机类别攻击更有效，且包含激励性描述的描述性攻击比类别基础攻击更有效。此外，研究还探讨了模型是否能够忽略排版攻击文本，结果显示大多数模型无法通过忽略攻击文本来恢复其基础性能。
6. 结论： 本文通过引入排版攻击基准测试，展示了排版攻击仍然是LVLMs的一个关注点。新型的自我生成排版攻击对LVLMs构成了比以往研究中提到的攻击更大的威胁。研究结果表明，未来的工作应该更加关注LVLMs中的排版攻击问题。

阅读总结报告： 本研究针对视觉-语言模型在面对排版攻击时的脆弱性进行了深入分析。通过提出新的自我生成攻击方法，研究者不仅提高了攻击的有效性，还展示了这些攻击如何在不同模型之间泛化。实验结果强调了LVLMs在处理文本信息时的潜在风险，并为未来的安全研究提供了新的视角。这项工作对于理解和改进LVLMs在面对恶意攻击时的鲁棒性具有重要意义。
