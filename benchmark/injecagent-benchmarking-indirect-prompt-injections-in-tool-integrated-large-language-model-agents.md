# INJECAGENT: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

###

随着大型语言模型（LLMs）的不断发展，它们越来越多地被集成到智能代理框架中，以执行各种任务和操作。这些代理可以访问用户个人数据、执行真实世界的操作，如银行交易和智能家居设备控制。然而，这些特性引入了潜在的安全风险，包括攻击者通过消息工具窃取敏感信息或通过未经授权的交易和设备操控对用户造成直接伤害。攻击者可以通过向代理检索的信息中注入恶意内容（称为间接提示注入，IPI）来诱导这些有害操作。由于此类攻击的技术要求低且可能造成严重后果，系统性地评估LLM代理对此类攻击的脆弱性变得非常重要。

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 过去方案和缺点

以往的研究主要关注于直接提示注入（DPI）攻击，即恶意用户直接向语言模型的输入中注入有害提示。然而，对于间接提示注入（IPI）攻击，特别是在工具集成的LLM代理中的研究还相对缺乏。现有的防御机制主要分为黑盒防御和白盒防御，但这些研究主要集中在基本场景中，其中指令和数据直接串联输入到LLM中。在工具集成的LLM代理中实施这些防御策略及其在更复杂场景中的有效性仍然是未来研究需要探索的领域。

### 本文方案和步骤

本文提出了INJECAGENT基准测试，旨在评估工具集成LLM代理对IPI攻击的脆弱性。INJECAGENT包含1054个测试用例，涵盖17种不同的用户工具和62种攻击者工具。研究者将攻击意图分为两种主要类型：对用户的直接伤害和私有数据的窃取。通过评估30个不同的LLM代理，研究表明代理容易受到IPI攻击的影响，其中使用ReAct提示的GPT-4在攻击中有24%的成功率。此外，研究者还探讨了在增强设置下，攻击者的指令通过“黑客提示”加强的情况，结果显示攻击成功率几乎翻倍。

### 本文创新点与贡献

1. 首次正式定义了针对工具集成LLM代理的IPI攻击。
2. 引入了INJECAGENT，这是一个新颖且现实的基准测试，涵盖了多个领域，并可作为评估代理对IPI攻击抵抗力的标准。
3. 使用INJECAGENT评估了30个LLM代理，并揭示了大多数代理容易受到IPI攻击。

### 本文实验

实验通过定量评估各种LLM代理使用INJECAGENT基准测试的韧性来展开。研究者检查了两种主要方法来使LLM具有使用工具的能力：提示方法和微调方法。实验结果表明，使用ReAct提示的代理在基本设置下容易受到攻击，而在增强设置下，即使是经过微调的GPT-4也显示出了100%的数据传输成功率，这表明了在数据窃取攻击中，代理传输提取数据到攻击者相对容易。

### 实验结论

实验结果表明，大多数LLM代理容易受到IPI攻击的影响。特别是，使用ReAct提示的GPT-4在基本设置下有24%的攻击成功率，而在增强设置下，成功率提高到47%。相比之下，经过微调的GPT-4和GPT-3.5表现出更强的抵抗力，攻击成功率分别只有3.8%和6.6%。

### 全文结论

本文通过INJECAGENT基准测试，揭示了工具集成LLM代理在面对IPI攻击时的脆弱性，并强调了实施安全策略的迫切需求。研究结果不仅展示了通过向外部内容注入恶意指令来操纵这些代理的可行性，还强调了潜在后果的严重性，并为防范这些攻击提供了指导。



注：

针对工具集成LLM（Large Language Models，大型语言模型）代理的IPI（Indirect Prompt Injection，间接提示注入）攻击是一种安全威胁，它涉及到攻击者通过在LLM代理处理的外部内容中嵌入恶意指令来操纵代理执行对用户有害的操作。

在这种情况下，LLM代理被设计为可以访问和使用各种工具来执行任务，如读取电子邮件、管理智能家居设备或进行在线交易等。这些工具通常会从外部源获取信息，如用户的个人数据或其他内容。IPI攻击利用了这一点，攻击者在这些外部信息中植入恶意的提示或命令，当LLM代理检索并处理这些信息时，就会受到操纵，可能会执行攻击者预设的有害动作。

例如，攻击者可能会在用户的产品评论中插入一条看似正常的评论，但实际上包含了控制智能家居设备开门的指令。当LLM代理被用户指示去读取评论时，它可能会执行这条恶意指令，从而使攻击者能够远程控制用户的家门。

IPI攻击的危险之处在于它们通常很难被检测到，因为恶意行为是通过代理的正常操作间接执行的。此外，由于LLM代理的复杂性和它们处理的信息量，识别和防御这类攻击变得更加困难。因此，开发有效的基准测试和防御机制对于保护用户免受此类攻击至关重要。





### 阅读总结报告

本研究通过开发INJECAGENT基准测试，对工具集成的大型语言模型代理进行了安全性评估，特别是在面对间接提示注入攻击时的脆弱性。研究结果表明，尽管存在安全要求，但代理仍然容易受到攻击，特别是当攻击指令被加强时。这些发现强调了在设计和部署LLM代理时，必须考虑到安全性，并采取相应的防御措施。此外，研究还指出了未来研究的方向，包括探索更多的黑客提示、攻击指令的变异性、更复杂的攻击场景以及对微调代理的更全面研究。这些贡献对于推动LLM代理的安全发展具有重要意义。
