# HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### 研究背景

自动化红队评估（Automated Red Teaming）对于揭示和减轻大型语言模型（LLMs）恶意使用的风险具有重要前景。然而，目前该领域缺乏标准化的评估框架，以严格评估新方法。随着LLMs的能力和应用范围的不断扩大，限制其恶意使用潜力变得越来越重要。

#### 过去方案和缺点

目前，公司依赖于手动红队评估，这在可扩展性方面存在不足。手动红队无法探索AI可能遇到的对抗性或长尾场景的全部范围。因此，开发自动化红队方法以评估和加强防御机制引起了相当大的兴趣。尽管近期的自动化红队论文报告了有希望的结果，但这些论文使用了不同的评估方法，使得它们难以进行比较，阻碍了未来的进展。此外，现有的评估缺乏对准确评估自动化红队至关重要的某些理想属性。

#### 本文方案和步骤

为了解决这些问题，本文介绍了HarmBench，一个新的红队攻击和防御的基准测试。作者确定了红队评估的三个理想属性——广度、可比性和稳健指标——并系统地设计HarmBench以满足这些标准。HarmBench包含比以往评估更多的独特行为，以及在以往工作中未探索过的全新行为类别。

#### 本文创新点与贡献

1. **HarmBench框架**：提出了一个新的标准化评估框架，用于自动化红队和稳健的拒绝行为。
2. **行为广度**：HarmBench包含510个独特的有害行为，分为文本和多模态行为，这些行为设计为违反法律或规范。
3. **评估方法**：提出了一种新的对抗性训练方法，称为Robust Refusal Dynamic Defense (R2D2)，用于强化拒绝机制。
4. **开源**：HarmBench在GitHub上开源，鼓励社区合作，共同开发更强的攻击和防御手段。

#### 本文实验

使用HarmBench，作者进行了大规模的比较实验，包括18种红队方法和33种目标LLMs及防御措施。实验揭示了以前未知的属性，这些属性可以帮助未来的攻击和防御工作。

#### 实验结论

实验结果表明，没有任何现有的攻击或防御是普遍有效的，稳健性与模型大小无关。这强调了大规模比较的重要性，并表明为了获得真正的对所有已知攻击的稳健性，可能不足以仅针对有限的攻击集进行训练并期望泛化。

#### 全文结论

HarmBench作为一个标准化的评估框架，为自动化红队和稳健拒绝提供了重要的工具。通过HarmBench，可以更好地评估和改进AI系统的安全性和安全性。本文提出的R2D2对抗性训练方法，展示了如何将强大的自动化红队纳入安全培训中，以超越以前的防御措施。

####
