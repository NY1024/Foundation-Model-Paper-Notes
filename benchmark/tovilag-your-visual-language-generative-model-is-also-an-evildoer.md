# ToViLaG: Your Visual-Language Generative Model is Also An Evildoer

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) ( (3).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

随着大规模视觉-语言生成模型（VLGMs）的发展，它们在多模态图像/文本生成方面取得了显著进步。然而，这些模型可能生成有害内容，如冒犯性文本和色情图像，引发重大的伦理风险。尽管对语言模型的有害退化进行了广泛研究，但视觉-语言生成中的这一问题仍未得到充分探讨。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 2. 过去方案和缺点

以往的研究主要集中在语言模型的有害内容生成问题上，而没有直接适用于视觉-语言生成（VLG）的方法和度量。此外，现有的输入过滤方法在防止有害输出方面效果有限，因为无害的输入也可能触发有害的输出。

### 3. 本文方案和步骤

本文提出了ToViLaG数据集，包含32K有害/无害的文本-图像对和1K可能诱发有害内容的文本。同时，提出了WInToRe，一种针对视觉-语言生成的新毒性度量方法。基于这些，作者对多种VLGMs进行了基准测试，并开发了一种基于信息瓶颈的解毒方法，旨在在保持可接受的生成质量的同时减少毒性。

### 4. 本文创新点与贡献

* 首次在VLG领域研究毒性问题，并建立了系统框架。
* 收集了有毒文本-图像数据集，提出了适用于VLG的新度量方法，并对多种VLGMs进行了全面分析。
* 设计了一种轻量级的解毒方法，理论上保证了在减少毒性的同时保持生成质量。

### 5. 本文实验

作者对多种图像到文本（I2T）和文本到图像（T2I）的VLGMs进行了毒性评估。实验结果表明，即使是在相对干净的数据训练下，VLGMs也可能产生比预期更多的有害内容。此外，还模拟了未来模型规模增大和数据不洁情况下的毒性变化。

### 6. 实验结论

实验结果揭示了VLGMs在毒性生成方面的脆弱性，并强调了开发解毒方法的紧迫性。解毒方法在减少毒性的同时，能够保持生成质量在可接受范围内。

### 7. 全文结论

本文深入探讨了VLGMs的有害退化问题，并提出了一种新的解毒方法。通过构建新的毒性度量和数据集，作者为VLG领域的毒性问题提供了一个系统的解决方案，并为未来的研究奠定了基础。

### 阅读总结

本文针对视觉-语言生成模型可能产生的有害内容问题进行了深入研究。通过构建新的数据集和毒性度量方法，作者不仅评估了现有模型的毒性水平，还提出了一种有效的解毒方法。这些工作对于理解和减轻VLGMs的伦理风险具有重要意义，并为未来的研究提供了新的方向。
