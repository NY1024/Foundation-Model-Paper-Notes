# Benchmarking Llama2, Mistral, Gemma and GPT for Factuality, Toxicity, Bias and Propensity for Halluc

<figure><img src="../.gitbook/assets/image (256).png" alt=""><figcaption></figcaption></figure>

## 研究背景

本研究的背景集中在大型语言模型（LLMs）在企业任务中的安全性评估。随着LLMs在自然语言处理（NLP）领域的突破性进展，它们在各种NLP基准测试中的表现已经达到顶尖水平。LLMs的生成能力和遵循指令的能力解锁了许多新的应用场景。然而，LLMs仍然存在几个主要问题，可能在企业环境中带来风险。这些问题包括事实性（报告不准确信息的能力）、有害性（在被指示不要这样做的情况下出现攻击性内容）、幻觉（产生任意的、捏造的信息）和偏见（生成包含宗教、政治、性别或种族偏见的内容）。因此，本研究旨在通过开发新的数据集和基准测试工具来评估LLMs的安全性。

## 过去方案和缺点

以往的研究已经提出了一些评估LLMs安全性的数据集和方法，例如Anthropic Harmless Assistant、ToxicChat、OpenAI Holistic Approach等。这些方法通常由简单的提示组成，缺乏指令和对话格式，因此无法充分测试LLMs在实际企业环境中的表现。此外，这些方法可能无法有效检测LLMs在复杂场景中的表现，例如长文本摘要、遵循严格输出格式的指令或多轮对话。

## 本文方案和步骤

本文提出了一种新的评估方法，通过设计14个新颖的数据集来测试LLMs的安全性。这些数据集包括11个人工合成的数据集和3个人工创建的数据集，专门为此研究而设计。研究中使用了两种开源模型（Meta Llama2、Mistral）和Google的Gemma模型，并与OpenAI的GPT模型进行了比较。评估的步骤包括：

1. 定义LLM Red Teaming的基准测试范围。
2. 介绍11个半合成数据集。
3. 描述人工制作的3个数据集，包括其构建方法和与现有数据集相比的贡献。
4. 描述基准测试方法和评估的模型。
5. 分析实验结果。

## 本文创新点与贡献

本文的主要创新点和贡献包括：

1. 开发了新的LLM Red Teaming数据集，这些数据集更加贴近企业用户的实际交互场景。
2. 提出了一种新的评估方法，通过明确的指令和期望的输出格式来评估LLMs的安全性。
3. 引入了一种新的评估工具，可以比较不同模型在数据集上的性能。
4. 对开源模型和商业模型在安全性方面的表现进行了全面比较。

## 本文实验

实验部分详细介绍了对四种LLMs（Llama2、Mistral、Gemma和GPT）的评估。实验使用了多种数据集来测试模型在事实性、有害性、幻觉和偏见方面的表现。实验结果显示，GPT在所有安全性方面的表现都优于开源模型。在开源模型中，Llama2在事实性和有害性方面表现良好，但幻觉倾向最高。Mistral在幻觉方面表现最好，但在处理有害性方面表现不佳。Gemma总体上表现平衡，但落后于其他模型。

## 实验结论

实验结果表明，尽管开源模型在某些方面表现良好，但在多轮对话测试中，它们的安全性显著降低。除了OpenAI的GPT之外，Mistral是唯一在多轮测试中仍然表现良好的模型。

## 全文结论

本研究通过开发新的数据集和基准测试工具，对LLMs的安全性进行了全面的评估。结果表明，尽管存在一些积极的进步，但在提高LLMs的安全性方面仍有大量的工作要做。未来的工作将包括扩展数据集、改进评估方法和探索通过微调来减轻安全问题的策略。

## 阅读总结报告

本研究通过开发新的数据集和基准测试工具，对LLMs在企业任务中的安全性进行了深入的评估。研究发现，尽管商业模型（如OpenAI GPT）在安全性方面表现优异，但开源模型（如Llama2、Mistral和Gemma）在某些方面仍有改进空间。研究的创新之处在于引入了新的评估方法和数据集，这些数据集更贴近实际的企业环境，并能够更好地测试LLMs在复杂场景中的表现。此外，研究还提出了未来的研究方向，包括扩展数据集、改进评估方法和探索微调策略，以进一步提高LLMs的安全性。
