# OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and Safety

<figure><img src="../.gitbook/assets/image (2) (1).png" alt=""><figcaption></figcaption></figure>

### 阅读总结报告

#### 1. 研究背景

随着中文大型语言模型（LLMs）的快速发展，对这些模型进行有效的评估面临着巨大挑战。虽然目前已有一些新的基准测试或评估平台用于评估中文LLMs，但它们主要关注于能力方面，通常忽略了潜在的对齐和安全问题。

#### 2. 过去方案和缺点

传统的NLP基准测试可能不适用于评估中文LLMs，因为它们存在局限性，例如，专为特定任务量身定制而不是通用性。此外，现有的中文LLMs评估平台如FlagEval、CLEVA和OpenCompass等并未包括安全性评估。

<figure><img src="../.gitbook/assets/image (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### 3. 本文方案和步骤

为了解决这一差距，本文介绍了OpenEval，这是一个评估中文LLMs的测试平台，涵盖了能力、对齐和安全三个维度。OpenEval包括：

* 能力评估：使用12个基准数据集，从NLP任务、学科知识、常识推理和数学推理4个子维度评估中文LLMs。
* 对齐评估：包含7个数据集，检查中文LLMs输出中的偏见、冒犯性和非法性。
* 安全评估：包括6个数据集，特别是评估高级LLMs的预期风险（例如，寻求权力、自我意识）。

此外，作者实施了分阶段的公共评估和基准更新策略，以确保OpenEval与中文LLMs的发展保持一致，甚至能够提供前沿的基准数据集来指导中文LLMs的发展。

#### 4. 本文创新点与贡献

* 提出了OpenEval，一个全面的中文LLMs评估平台，包括35个基准测试，涵盖能力、对齐和安全。
* 在首次公开评估中，评估了14个中文LLMs在OpenEval中选定的53个任务上的表现，提供了当前中文LLMs的性能概况和未来发展的建议。

#### 5. 本文实验

作者组织了首次公开的OpenEval评估活动，评估了开源和专有的中文LLMs。使用了53个任务，并记录了2023年12月28日的评估结果。评估了9个中文SFT/RLHF LLMs和5个由大公司开发的专有中文LLMs。

#### 6. 实验结论

* 开源中文LLMs在数学推理方面表现良好，但在学科知识和对齐方面落后于专有模型。
* 专有中文LLMs在学科知识和数学推理方面表现更好，但在对齐和安全方面面临挑战。

#### 7. 全文结论

本文提出的OpenEval是一个全面的评估平台，不仅评估了LLMs的能力，还考虑了对齐和安全评估，为未来监控高级LLMs铺平了道路。OpenEval包括53个任务，约300K问题，并采用动态评估策略，以新基准替换过时或受污染的基准，确保评估的有效性。

#### 阅读总结

OpenEval作为一个创新的评估平台，为中文LLMs提供了一个全面、用户友好、可扩展和透明的评估框架。它不仅关注模型的性能，还关注模型的潜在风险和对齐问题，这对于理解和改进LLMs至关重要。通过动态评估策略和公开评估活动，OpenEval有助于推动中文LLMs的进一步发展和完善。
