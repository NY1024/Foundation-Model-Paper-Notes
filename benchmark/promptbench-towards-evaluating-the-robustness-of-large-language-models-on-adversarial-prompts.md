# PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

随着大型语言模型（LLMs）在学术界和工业界的广泛应用，对其鲁棒性的理解变得至关重要。LLMs通常依赖于输入的提示（prompts）来执行特定任务。然而，现有的研究主要关注模型对输入样本的鲁棒性，而对提示的鲁棒性研究相对较少。提示的微小变化，如错别字或同义词，可能会显著影响LLMs的输出结果。因此，本文提出了PromptBench，一个用于评估LLMs对对抗性提示的鲁棒性的基准测试。

### 2. 过去方案和缺点

过去的研究，如AdvGLUE和ANLI，主要关注语言模型对对抗性样本的鲁棒性。这些研究通过精心设计的样本扰动来评估模型。然而，这些方法并不适用于仅由提示组成的输入场景，且在实际应用中，用户输入的提示可能会自然发生扰动。此外，现有的鲁棒性评估通常基于静态数据集，缺乏对模型在不同任务和数据集上鲁棒性的全面理解。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了PromptBench，一个全面的基准测试，用于评估LLMs对不同级别（字符、单词、句子和语义）的对抗性文本攻击的鲁棒性。研究使用了多种对抗性提示，模仿用户可能犯的错误，如错别字或同义词。这些提示被用于多种任务，包括情感分析、自然语言推理、阅读理解、机器翻译和数学问题求解。研究生成了4,788个对抗性提示，并在8个任务和13个数据集上进行了详细评估。

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 4. 本文创新点与贡献

* 提出了PromptBench，这是第一个系统性的基准测试，用于评估、理解和分析LLMs对对抗性提示的鲁棒性。
* 对LLMs的鲁棒性进行了全面评估，并进行了广泛的分析，包括对抗性提示的可视化解释、对抗性提示的可转移性分析，以及词频分析，为下游用户和提示工程师提供了实用的指导。
* 为了促进未来对LLMs鲁棒性的研究，还构建了一个可视化网站，允许用户轻松探索对抗性提示。

### 5. 本文实验

实验使用了9种流行的LLMs，包括Flan-T5-large、ChatGPT和GPT-4等。选择了8个任务进行评估，包括情感分析、语法正确性、重复句子检测、自然语言推理、多任务知识、阅读理解、翻译和数学问题求解。总共创建了4,788个对抗性提示，并在这些任务和数据集上进行了广泛的实验和分析。

### 6. 实验结论

实验结果表明，当前的LLMs对对抗性提示的鲁棒性普遍不足。特别是，单词级别的攻击在所有任务中平均性能下降了39%。通过分析LLMs在错误响应中每个单词的注意力权重，发现对抗性提示导致模型将注意力转向扰动元素，从而产生错误响应。此外，还研究了对抗性提示在不同模型之间的可转移性，并提出了从一种LLM到另一种LLM的成功转移性。

### 7. 全文结论

本文通过PromptBench的引入，为LLMs的鲁棒性研究提供了一个全面的评估框架。实验结果揭示了LLMs在对抗性提示面前的脆弱性，并为未来的研究提供了新的视角和工具。通过深入分析，本文为如何提高LLMs的鲁棒性提供了实用的建议和指导。

### 阅读总结

本文通过PromptBench基准测试，对大型语言模型在对抗性提示下的鲁棒性进行了全面的评估。研究发现，即使是微小的提示变化，也可能显著影响模型的输出。这一发现强调了在设计和使用LLMs时，需要考虑提示的鲁棒性。PromptBench不仅为研究人员提供了一个评估工具，也为实际应用中的用户如何构建更鲁棒的提示提供了指导。此外，本文还探讨了对抗性提示的可转移性，为未来在黑盒模型上的鲁棒性研究提供了新的思路。
