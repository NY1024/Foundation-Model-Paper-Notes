# AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Ins

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

大型视觉-语言模型（LVLMs）在理解和响应用户的视觉指令方面取得了显著进展。然而，这些模型在面对恶意攻击时表现出脆弱性，这些攻击可能故意或无意地针对图像和文本输入。鉴于LVLMs在未来AI应用中的关键作用，确保它们对这些威胁的鲁棒性、安全性和公平性至关重要。尽管已有研究评估了大型语言模型（LLMs）对文本攻击的鲁棒性，但针对LVLMs的研究仍然有限。

### 2. 过去方案和缺点

以往的研究主要集中在LLMs的文本攻击上，而对于LVLMs的图像和文本联合攻击的研究较少。此外，现有的攻击方法如白盒攻击、后门攻击和基于查询的黑盒攻击等，依赖于模型的输出概率分布，可能不适用于在线访问的模型，尤其是闭源LVLMs。这些攻击方法可能受限于特定任务设计，如图像标题生成或视觉问答，限制了评估的全面性。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文介绍了AVIBench，一个全面的基准测试框架，用于评估LVLMs在面对对抗性视觉指令（AVIs）时的鲁棒性。AVIBench包括四种基于图像的AVIs、十种基于文本的AVIs和九种内容偏见AVIs（如性别、暴力、文化和种族偏见等）。研究者们生成了260K个AVIs，涵盖了五种多模态能力和内容偏见，并使用这些AVIs对14个开源LVLMs进行了全面评估。

### 4. 本文创新点与贡献

* 提出了AVIBench，这是一个评估LVLMs对AVIs鲁棒性的先驱框架和多功能工具。
* 生成了一个包含260K AVIs的全面数据集，覆盖了五种多模态能力和内容偏见，为严格评估LVLMs提供了基准。
* 对14个开源LVLMs进行了抵抗对抗性AVIs的能力评估，并展示了广泛的实验结果和发现。
* 揭示了即使是先进的闭源LVLMs，如GeminiProVision和GPT-4V，也存在显著的内容偏见。

### 5. 本文实验

实验使用了AVIBench对14个不同的开源LVLMs进行了评估，并对比了它们在面对图像损坏、基于决策的优化黑盒图像攻击、文本攻击和内容偏见攻击时的鲁棒性。此外，还评估了闭源LVLMs，包括像GeminiProVision和GPT-4V这样的高级系统。

### 6. 实验结论

实验结果揭示了LVLMs在面对对抗性视觉指令时的脆弱性，并强调了即使在先进的闭源LVLMs中，也存在固有的偏见。这些发现强调了提高LVLMs的鲁棒性、安全性和公平性的重要性。

### 7. 全文结论

本文通过引入AVIBench框架，为评估LVLMs对抗性视觉指令的鲁棒性提供了一个重要的基准，并可能激发研究社区开发新的缓解和防御策略。此外，研究结果强调了在设计鲁棒的LVLMs时，需要考虑到推理过程可能被绕过的风险，并探索更深层次的防御机制。



注：

对抗性视觉指令（Adversarial Visual-Instructions，简称AVIs）是指那些被故意设计出来的图像和文本组合，它们旨在操纵大型视觉-语言模型（LVLMs）的行为，以诱导模型产生不正确、不安全或有害的输出。这些指令超越了传统的“误分类”概念，更广泛地涵盖了模型可能受到的各种恶意影响。

具体来说，AVIs可以分为以下几类：

1. **图像基础的AVIs**：这类AVIs通过在图像上施加各种扰动（如噪声、模糊、天气效果、数字失真等）来攻击模型。这些扰动可能包括图像损坏和基于决策的优化图像攻击，目的是测试LVLMs在面对图像内容被篡改时的鲁棒性。
2. **文本基础的AVIs**：这类AVIs通过在文本输入中引入字符级、单词级、句子级和语义级的扰动来攻击模型。这些攻击可能包括拼写错误、同义词替换、无关句子的添加或重要单词的删除，目的是测试LVLMs在面对文本输入被篡改时的鲁棒性。
3. **内容偏见AVIs**：这类AVIs专注于评估LVLMs在面对包含性别、暴力、文化和种族偏见等内容的指令时的表现。这些偏见可能源于模型训练数据的不平衡或社会文化背景，导致模型在处理某些敏感话题时表现出不公正或歧视性。

AVIs的设计和使用是为了全面评估和提升LVLMs在面对各种潜在威胁时的鲁棒性和安全性，确保这些模型在实际应用中的可靠性和公平性。通过对抗性视觉指令的测试，研究人员可以更好地理解模型的脆弱性，并开发出更有效的防御策略来保护模型免受恶意攻击。





### 阅读总结

AVIBench的提出为评估和改进LVLMs在面对对抗性攻击时的鲁棒性提供了一个重要的工具。通过广泛的实验和分析，研究者们不仅揭示了LVLMs的脆弱性，还强调了在模型开发中需要重视的偏见问题。这些发现对于未来LVLMs的研究和应用具有重要的指导意义，特别是在提高模型的安全性和公平性方面。
