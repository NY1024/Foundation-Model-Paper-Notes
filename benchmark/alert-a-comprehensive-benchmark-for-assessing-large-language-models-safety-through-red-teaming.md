# ALERT: A Comprehensive Benchmark for Assessing Large Language Models’ Safety through Red Teaming

<figure><img src="../.gitbook/assets/image (257).png" alt=""><figcaption></figcaption></figure>



<figure><img src="../.gitbook/assets/image (258).png" alt=""><figcaption></figcaption></figure>

## 研究背景

大型语言模型（LLMs）在模仿人类语言生成方面取得了显著进展，它们在文本翻译、对话等任务中展现出了巨大潜力。然而，这些模型通常在海量网络数据上进行训练，这引发了关于其道德使用、偏见和潜在意外后果的担忧。随着LLMs在日常生活中的日益普及，确保其安全部署，避免风险和确保用户安全变得至关重要。

## 过去方案和缺点

现有的研究通常只关注LLMs安全性的一个方面，如毒性，而全面评估所有子类别更可能提供对LLMs弱点的清晰全面洞察。现有的安全性评估方法存在局限性，它们可能无法充分探索可能导致恶意结果的各种情况，并且无法细致地评估模型在每种情况下的表现。

## 本文方案和步骤

为了解决上述问题，研究者们提出了ALERT（Assessing Large Language Models’ Safety through Red Teaming）基准测试，这是一个基于新的细粒度风险分类法的大规模基准测试。ALERT旨在通过红队方法评估LLMs的安全性，包含超过45k个指令，使用新的分类法进行分类。研究者们还提出了一个自动化方法，通过辅助LLM对目标LLM的响应进行安全性分类，从而为LLM提供一个总体安全性得分以及特定类别的安全性得分。

## 本文创新点与贡献

1. 提出了一个新的安全性风险分类法，包含6个大类和32个微类，为进行红队测试和开发符合政策（如AI法规）的模型提供了全面的基础。
2. 介绍了ALERT基准测试，包含超过45k个红队提示，以及一个自动化方法来评估LLMs的安全性，构成了ALERT框架。
3. 广泛评估了十个开放和封闭源代码的LLMs，突出了它们在基准测试中的优缺点。
4. 构建了一个DPO数据集，以促进对安全性调整的进一步研究。

## 本文实验

实验部分，研究者们评估了10个流行的开放和封闭源代码的LLMs，并展示了它们在ALERT基准测试中的安全性得分。实验结果显示，许多模型在达到合理的安全性水平上仍然存在困难。

## 实验结论

实验结果表明，即使是通常被认为安全的模型（例如GPT-4）也在特定微类别中表现出了安全隐患。这些细粒度的观察结果至关重要，强调了在部署LLMs时进行上下文和政策感知评估的必要性。

## 全文结论

ALERT基准测试通过提供新的细粒度分类法，为评估和提高LLMs的安全性提供了一个全面的框架。通过广泛的实验，研究者们展示了ALERT在突出模型的安全弱点和指导针对性安全增强方面的有效性。此外，研究者们还公开了所有数据集和代码，以激发对安全LLMs的进一步研究。

## 阅读总结报告

本研究通过引入ALERT基准测试和相应的细粒度风险分类法，为评估LLMs的安全性提供了一个全面的新方法。研究者们通过实验展示了多种流行LLMs的安全性评估结果，揭示了它们在安全性方面的弱点，并强调了在不同政策和上下文中评估LLMs的重要性。此外，研究者们还提出了未来工作的方向，包括对每个类别进行更深入的分析，使用ALERT的DPO集进行安全性调整，并扩展基准测试到多语言环境。研究者们公开了所有数据集和代码，以促进安全LLMs的发展，并提醒使用ALERT时应注意其潜在的双刃剑效果。
