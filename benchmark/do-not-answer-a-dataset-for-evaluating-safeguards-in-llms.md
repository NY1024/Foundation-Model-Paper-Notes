# Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs

<figure><img src="../.gitbook/assets/image (11) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

随着大型语言模型（LLMs）的快速发展，它们展现出了前所未有的能力和潜在的有害功能。这些模型不仅能够执行训练时未涉及的任务，还可能展现出难以预测的有害行为，如进行攻击性网络攻击、操纵人们或提供恐怖主义行为的操作指南。因此，开发者需要能够通过评估“危险能力”来识别风险，以负责任地部署LLMs。

### 2. 过去方案和缺点

以往的模型评估主要关注性别和种族偏见、真实性、有害性以及版权内容的复制等方面。然而，这些评估往往忽略了更严重的风险，如非法协助、心理危机干预和心理操纵。此外，现有的安全机制主要集中在商业LLMs上，而开源LLMs往往缺乏全面的安全机制。

<figure><img src="../.gitbook/assets/image (12) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了第一个开源数据集“Do-Not-Answer”，用于评估LLMs的安全机制。数据集经过策划和筛选，仅包含负责任的语言模型不应遵循的指令。研究者们收集了939个风险提示，并基于这些提示对六个流行的LLMs进行了手动评估和响应收集。接着，他们训练了几个BERT-like分类器，并发现这些小型分类器在自动安全评估方面与GPT-4取得了可比的结果。

### 4. 本文创新点与贡献

* 提出了一个三级层次的风险分类法，涵盖了从轻微到极端的风险。
* 创建了一个风险检测数据集，包含939个基于不应遵循指令的提示。
* 对商业和开源LLMs的响应进行了手动评估，并提出了几种自动安全评估方法。
* 展示了小型模型（如BERT-like模型）在低成本下也能有效地评估响应。

### 5. 本文实验

实验包括对六个LLMs的响应进行手动评估，以及使用GPT-4和基于预训练语言模型（PLM）的分类器进行自动安全评估。实验结果表明，LLaMA-2在不遵循风险指令方面表现最佳，而ChatGLM2排名最后。

### 6. 实验结论

实验结果显示，LLaMA-2在安全响应方面表现最好，而ChatGLM2则最不安全。此外，响应表现出明显的风险类型特定模式。自动评估方法，特别是BERT-like模型，能够以较低的成本实现与GPT-4相当的评估结果。

### 7. 全文结论

本文通过创建“Do-Not-Answer”数据集，为研究社区提供了一个宝贵的资源，有助于LLMs的安全开发和部署。研究者们展示了如何通过小型模型进行有效的安全评估，这对于开源LLMs的安全机制研究具有重要意义。

### 阅读总结

本文针对大型语言模型的安全性问题，提出了一个新的评估框架和数据集。通过详细的风险分类和数据收集，研究者们不仅对现有的LLMs进行了深入的评估，还开发了有效的自动评估工具。这些工作为LLMs的安全研究和实践提供了新的视角和方法，对于推动LLMs的负责任发展具有重要价值。
