# A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks

<figure><img src="../.gitbook/assets/image (27).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

大型语言模型（LLMs）因其在语言理解和生成方面的卓越能力，被视为未来移动通信网络中提供高效智能服务的关键组件。然而，LLMs的训练和运行需要大量的数据和计算资源，这迫使开发者采用外包训练或利用第三方数据和计算资源的策略。这些策略可能使模型暴露于恶意操纵的训练数据和处理中，为攻击者提供了在模型中嵌入隐藏后门的机会，即后门攻击。这种攻击在通信网络中尤为令人关注，因为网络的可靠性和安全性至关重要。

### 2. 过去方案和缺点

以往的研究主要集中在计算机视觉和较小的语言模型上，通常是通过恶意篡改训练实例来进行后门攻击。然而，随着LLMs的日益受到关注，特定的训练范式（如使用训练实例的预训练、提示调整、指令调整和演示引导输出）已被证明是后门攻击漏洞的潜在热点。尽管LLMs的安全性问题日益突出，但目前缺乏针对这一领域的系统化和统一的后门攻击分析。

<figure><img src="../.gitbook/assets/image (28).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一个系统化的后门攻击分类法，将后门攻击分为四大类：输入触发、提示触发、指令触发和演示触发攻击。此外，本文对基准数据集进行了全面分析，并识别了潜在问题和开放挑战，为未来研究方向提供了有价值的见解。

<figure><img src="../.gitbook/assets/image (29).png" alt=""><figcaption></figcaption></figure>

### 4. 本文创新点与贡献

* 提供了一个全面的回顾，根据特性和属性对现有方法进行了分类。
* 讨论了可能的未来研究方向，并展示了需要解决的重要缺口。
* 为未来研究提供了指导，帮助社区及时了解当前趋势，并深刻理解每种方法的优势和局限性。

### 5. 本文实验

本文没有提出新的实验方法，而是对现有的后门攻击方法进行了分类和分析。实验部分主要集中在对基准数据集的分析上，这些数据集包括文本分类、机器翻译、问答、语言建模、命名实体识别和文本摘要等任务的数据集。

### 6. 实验结论

本文的实验结论指出，尽管在后门攻击研究方面取得了进展，但这一新兴领域在网络领域内面临着紧迫的挑战和未解决的问题。作者强调了对后门攻击的潜在威胁的认识，并提供了这一重要研究领域的详尽概述。

### 7. 全文结论

后门攻击在通信网络中的LLMs是一个关键且活跃的研究领域，对网络安全和可靠性具有重大影响。本文通过提供一个综合的回顾和系统化的分类，阐明了后门攻击在通信网络这一特殊环境中的概念，并讨论了评估这些攻击的网络领域内的流行基准数据集。作者预见到将有更多的相关研究揭示特定于基于网络的应用的更强大的攻击和防御机制，并深入研究后门攻击的基本原理。



注1：

在论文 "A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks" 中，作者对大型语言模型（LLMs）中的后门攻击进行了分类，将其分为以下四大类：

#### 输入触发攻击（Input-triggered Attacks）

这类攻击发生在预训练阶段，攻击者通过恶意修改训练数据来植入后门。被篡改的训练数据随后被上传到互联网，开发者在不知情的情况下下载并使用这些数据来训练模型，导致模型中嵌入了隐藏的后门。例如，攻击者可以在训练数据中插入特定的字符或组合作为触发器，并修改这些样本的标签。当模型在推理时遇到包含触发器的输入时，会执行攻击者预设的行为。

#### 提示触发攻击（Prompt-triggered Attacks）

这类攻击涉及恶意修改用于从模型中引出响应的提示（prompts）。攻击者通过训练模型学习特定提示与攻击者期望的输出之间的关系。当模型遇到这些特定的提示时，无论用户的输入是什么，都会生成攻击者期望的输出。这种攻击方式可以使得模型在面对恶意用户输入时，改变其功能或泄露模型的提示。

#### 指令触发攻击（Instruction-triggered Attacks）

这类攻击利用模型的微调（fine-tuning）过程，通过向模型中注入恶意的指令来实现。当模型遇到这些被篡改的指令时，会被引导执行恶意活动。例如，攻击者可以在训练数据集中注入带有恶意指令的样本，使得模型在后续的任务中执行特定的恶意行为。

#### 演示触发攻击（Demonstration-triggered Attacks）

在这类攻击中，攻击者的目标是仅通过操纵演示（demonstrations）来误导模型，而不是改变输入。例如，攻击者可以通过在演示中引入微小的扰动来生成对抗性示例，这些示例可以在不改变测试输入的情况下误导模型。这种攻击方式难以检测，因为它不需要直接修改训练数据集的实例，而是通过操纵任务指令来实现。

这四种类型的后门攻击都旨在使模型在面对特定的触发条件时执行攻击者定义的行为，同时在正常输入上保持正常功能。这些攻击对通信网络中的LLMs构成了严重的安全威胁，因为它们难以被检测，并且可能在不知不觉中影响模型的决策过程。



注2：

修改训练数据来植入后门（Input-triggered Attacks）和训练模型学习特定提示与攻击者期望的输出之间的关系（Prompt-triggered Attacks）的主要区别在于攻击的实施方式和目标模型的易受攻击点。

#### 修改训练数据来植入后门

在这种攻击中，攻击者直接篡改模型的训练数据集。这通常发生在模型的预训练阶段，攻击者通过在训练数据中插入特定的触发器（如特定的字符、词组或数据模式）来植入后门。这些触发器在模型训练过程中被模型学习，导致模型在遇到包含这些触发器的输入时产生错误的预测或执行特定的恶意行为。这种攻击的关键在于模型在训练过程中学习到了这些恶意的触发器，使得模型在推理阶段对这些特定的输入敏感。

#### 训练模型学习特定提示与攻击者期望的输出之间的关系

在这种攻击中，攻击者通过精心设计的提示（prompts）来操纵模型的行为。这些提示是模型输入的一部分，用于引导模型生成特定的输出。在这种情况下，攻击者不需要直接修改训练数据集，而是通过修改模型接收的提示来实现攻击。例如，攻击者可以训练模型，使其在接收到特定的提示时，无论实际输入内容如何，都产生攻击者期望的输出。这种攻击的关键在于模型学习到了提示和期望输出之间的关联，而不是直接在训练数据中植入后门。

总结来说，输入触发攻击侧重于在模型训练阶段通过修改训练数据来植入后门，而提示触发攻击则侧重于在模型使用阶段通过操纵输入提示来引导模型产生特定的输出。两者都利用了模型对特定输入的敏感性，但实现方式和攻击时机有所不同。



注3：

攻击者通过训练模型使其在接收到特定提示时产生期望输出的过程通常涉及以下几个步骤：

1. **选择或设计触发器（Trigger）**： 攻击者首先需要确定或设计一个或一组特定的提示（prompt），这些提示在模型中作为触发器。这些触发器可以是特定的词、短语、符号或者任何能够被模型识别的文本模式。
2. **准备训练数据**： 攻击者会创建或修改一组训练数据，使得这些数据包含上述设计的触发器。这些数据的标签（即模型应该学习的输出）会被设置为攻击者期望的输出。例如，如果攻击者希望模型在看到某个特定提示时总是将其分类为“正面”情绪，那么所有包含该触发器的数据都会被标记为“正面”。
3. **训练模型**： 使用这些带有特定触发器和预设标签的数据，攻击者会训练模型。在训练过程中，模型会学习到触发器与特定输出之间的关联。这可能涉及到微调预训练模型，或者从头开始训练一个新模型。
4. **测试和优化**： 训练完成后，攻击者会测试模型以确保它能够正确响应触发器。如果模型的表现不符合预期，攻击者可能需要调整触发器的设计或重新训练模型，直到达到满意的效果。
5. **部署和激活后门**： 一旦模型被训练好并能够对特定触发器产生预期的输出，攻击者就可以将这个模型部署到实际环境中。当模型在实际使用中遇到包含触发器的输入时，它就会被激活并执行攻击者期望的行为。

例如，攻击者可能会使用一种称为“BadPrompt”的技术，这是一种针对连续提示模型的后门攻击。在这种攻击中，攻击者首先选择一组有效的触发器，然后通过训练模型来学习这些触发器与特定输出之间的关联。当模型在实际应用中遇到这些触发器时，它就会被诱导产生攻击者期望的输出，无论实际输入内容如何。

这种攻击的关键在于模型的学习能力，它能够从训练数据中学习到输入（提示）与输出（期望行为）之间的映射关系。这种映射一旦被模型学习，就可以在没有明显迹象的情况下被激活，从而实现后门攻击。



### 阅读总结

本文是对大型语言模型在通信网络中后门攻击的全面概述。作者提出了一个后门攻击的分类法，并分析了现有的攻击方法和基准数据集。尽管本文没有提出新的实验方法，但它为未来在这一领域的研究提供了宝贵的指导和见解。作者强调了在实际应用中保护LLMs免受后门攻击的重要性，并提出了未来研究的方向。
