# Current state of LLM Risks and AI Guardrails

<figure><img src="../.gitbook/assets/image (11).png" alt=""><figcaption></figcaption></figure>

### 研究背景

大型语言模型（LLMs）因其日益增强的复杂性，已被广泛部署在对安全性和可靠性至关重要的敏感应用中。然而，LLMs存在固有风险，包括偏见、潜在的不安全行为、数据集污染、缺乏可解释性、幻觉和不可复现性。这些风险要求开发“防护措施”来确保LLMs与期望行为一致，并减少潜在伤害。

### 过去方案和缺点

以往的方案在设计有效的防护措施时面临困难，主要是因为难以明确ML模型的要求和期望。例如，不同领域、国家和地区的法规各不相同，公平性或避免冒犯性回应等伦理要求难以具体化。此外，减少幻觉、毒性和偏见等常识性要求也是非平凡的任务。模型开发者经常面临相互竞争的要求，如准确性与隐私之间的权衡。

### 本文方案和步骤

本文探索了与部署LLMs相关的风险，并评估了实现防护措施和模型对齐技术的现状。作者检查了内在和外在偏见评估方法，并讨论了公平性指标在负责任的AI开发中的重要性。同时探索了代理性LLMs（能够执行现实世界行动的模型）的安全性和可靠性，强调了可测试性、故障安全和情境感知的必要性。

### 本文创新点与贡献

本文提出了一个分层保护模型，该模型在外部、次要和内部层面上运行，以保护LLMs。突出了系统提示、检索增强生成（RAG）架构以及最小化偏见和保护隐私的技术。此外，本文强调了有效防护措施设计需要深入理解LLM的预期用例、相关法规和伦理考虑。

### 本文实验

本文没有明确提到具体的实验操作，而是集中在对现有LLM风险和防护措施的分析和评估上。

### 实验结论

本文的结论强调了在实施这些防护措施时仍然存在的关键挑战。在AI系统中找到灵活性与稳定性之间的最佳平衡至关重要，以保持它们的适应性和道德一致性。需要进一步关注明确的目标和指标、可测试性和成本优化。

### 全文结论

尽管存在复杂性，但为LLM开发可靠的防护措施对于最大化其益处和最小化潜在伤害至关重要。持续的研究、开发和跨领域的开放合作对于确保未来LLM的安全、负责和公平使用至关重要。
