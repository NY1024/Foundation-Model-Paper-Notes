# Safety of Multimodal Large Language Models on Images and Text

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 研究背景

随着多模态大型语言模型（MLLMs）在日常生活中的应用越来越广泛，它们在提高工作效率方面展现出了显著的能力。然而，MLLMs对不安全指令的脆弱性在实际部署中带来了巨大的安全风险。本文系统地调查了MLLMs在图像和文本方面的安全性评估、攻击和防御的当前努力，旨在帮助研究人员了解该领域的详细范围。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 过去方案和缺点

以往的研究主要集中在大型语言模型（LLMs）的安全性上，而对MLLMs安全性的研究还处于早期阶段。缺乏对MLLMs安全性的全面调查使得难以了解该领域的全貌。此外，现有的安全评估数据集和指标有限，无法全面衡量MLLMs的安全性。

### 本文方案和步骤

本文首先介绍了MLLMs的概述和对安全性的理解，然后回顾了用于衡量MLLMs安全性的评估数据集和指标。接下来，全面介绍了与MLLMs安全性相关的攻击和防御技术。最后，分析了一些未解决的问题，并讨论了有前景的研究方向。

### 本文创新点与贡献

* 比较了不同的安全性评估数据集和用于基准测试MLLMs安全程度的评估指标。
* 展示了针对MLLMs安全性的攻击和防御方法的系统全面回顾。
* 预测了MLLMs安全性的未来研究机会，为其他研究人员提供灵感。

### 本文实验

本文没有提出新的实验方法，而是对现有的安全性评估数据集、评估指标、攻击技术和防御方法进行了系统的总结和分析。

### 实验结论

本文的分析表明，图像模态带来的风险主要包括三个方面：(1) 向图像添加对抗性扰动可以以低成本获得令人满意的攻击结果；(2) 基于对齐LLMs的MLLMs通常拒绝恶意文本指令，但在利用其固有的光学字符识别（OCR）能力时，它们会直接服从相应的视觉指令；(3) 跨模态训练削弱了对齐LLMs的对齐能力。

### 全文结论

本文试图提供MLLMs安全性的全面概述。首先介绍了MLLMs的概述和对安全性的理解，然后系统回顾了MLLMs安全性的评估、攻击和防御，展示了MLLMs安全性的当前发展状况。最后，深入探讨了现有挑战，并为未来的研究人员指出了一些有前景的研究机会。

### 阅读总结报告

这篇论文提供了对多模态大型语言模型（MLLMs）安全性的全面审视，涵盖了评估、攻击和防御的各个方面。作者指出，尽管MLLMs在提高工作效率方面具有巨大潜力，但它们在面对不安全指令时的脆弱性也不容忽视。通过对现有文献的系统回顾，本文揭示了MLLMs安全性研究的不足，并提出了未来研究的方向，包括更全面的安全评估基准、深入研究安全风险，以及开发新的安全对齐技术。这篇综述是对MLLMs安全性研究领域的宝贵贡献，为未来的研究提供了明确的方向和灵感。
