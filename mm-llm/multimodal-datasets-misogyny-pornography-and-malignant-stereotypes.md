# Multimodal datasets: misogyny, pornography, and  malignant stereotypes

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 随着深度学习和计算机视觉的兴起，对于大规模数据集的需求日益增长。这些数据集通常从互联网上收集，用于训练大型机器学习模型。然而，这些数据集的收集和使用引发了关于数据质量、隐私侵犯、偏见和不适当内容的严重担忧。特别是，大型多模态数据集（如图像和文本配对）在训练过程中可能会固化和传播有害的刻板印象和性别、种族偏见。
2. 过去方案和缺点： 过去的数据集收集方法往往依赖于人工标注和筛选，这不仅成本高昂，而且难以扩展到数十亿级别的数据集。为了解决这个问题，一些项目（如CLIP）采用了从互联网上爬取数据的方法，并通过自动化的过滤机制来减少不适当内容。然而，这种方法存在明显的缺点，包括对不适当内容的过滤不彻底、对偏见的固化以及对隐私的侵犯。此外，这些数据集的发布往往缺乏对潜在问题的透明度和责任归属。
3. 本文方案和步骤： 本文对最近发布的LAION-400M数据集进行了初步审计，该数据集是从CommonCrawl数据集中筛选出的图像-替代文本对。作者发现该数据集包含了强奸、色情、恶性刻板印象、种族和民族侮辱以及其他极其有问题的内容。文章提出了对大规模数据集当前状态的担忧，并为AI社区、监管机构、政策制定者和数据主体等各方利益相关者提出了开放性问题。
4. 本文实验和性能： 文章通过定性和定量分析LAION-400M数据集，揭示了其中存在的问题。作者通过搜索界面对数据集进行了查询，发现即使是看似无害的查询也返回了大量不适当的图像结果。此外，作者还对数据集的过滤机制进行了评估，发现CLIP模型在过滤过程中存在偏见，并且0.3的余弦相似度阈值可能导致不适当的样本被错误地保留。文章没有提供具体的性能指标，而是侧重于讨论数据集的伦理和社会影响。

阅读总结报告： 本文对LAION-400M数据集进行了深入的批判性分析，揭示了大规模多模态数据集在收集和使用过程中可能带来的伦理和社会问题。作者强调了数据集可能固化和传播的有害刻板印象和偏见，以及对个人隐私的潜在侵犯。文章提出了一系列开放性问题，呼吁AI社区和相关利益相关者对这些问题进行深入讨论，并寻求更负责任的数据集管理和使用策略。尽管文章没有提供具体的技术解决方案，但它为如何更负责任地处理大规模数据集提供了重要的视角。
