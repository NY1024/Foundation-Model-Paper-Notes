# Onthe Robustness of Large Multimodal Models Against Image Adversarial  Attacks

<figure><img src=".gitbook/assets/image (27).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 本研究的背景是大型多模态模型（LMMs）在图像分类、图像描述和视觉问答（VQA）等任务中表现出色，但这些模型对视觉对抗性攻击的鲁棒性尚未得到充分检验。对抗性攻击是指通过微妙地操纵输入，使模型产生错误输出的攻击手段。尽管对抗性攻击在图像分类领域已有广泛研究，但针对LMMs的系统性研究仍然缺乏。

<figure><img src=".gitbook/assets/image (28).png" alt=""><figcaption></figcaption></figure>

1. 过去方案和缺点： 以往的研究主要集中在单一模态（如图像或文本）的对抗性攻击上，而没有充分考虑多模态模型的特点。此外，大多数对抗性攻击是端到端生成的，针对整个模型的最终损失，而没有考虑到在多模态模型中，攻击可能只针对视觉编码器或文本编码器的一部分。

<figure><img src=".gitbook/assets/image (29).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤： 研究者们提出了一种新的方法来评估LMMs对不同对抗性攻击的鲁棒性。他们选择了三种代表性的LMMs（LLaVA、BLIP2和InstructBLIP），并在图像分类、图像描述和VQA任务上进行了广泛的实验。研究者们使用了基于梯度的白盒对抗性攻击（如PGD、APGD和CW），并根据攻击强度设置了不同的参数。他们还提出了一种新的实际应用方法，称为查询分解，通过将问题分解为多个存在性查询来提高图像分类的鲁棒性。
2. 本文创新点：

* 提出了一种新的方法来评估LMMs对视觉对抗性输入的鲁棒性。
* 发现通过提示提供给模型的上下文（如QA对中的问题）有助于减轻视觉对抗性输入的影响。
* 提出了查询分解的方法，通过将输入提示中的查询分解为存在性查询，观察到对抗性攻击效果的减弱和图像分类准确性的提高。

5. 本文实验和性能： 实验结果表明，LMMs在没有额外文本信息的情况下对视觉对抗性扰动不具有鲁棒性，如在COCO数据集的分类任务中。然而，在VQA任务中，LMMs显示出了一定的固有鲁棒性，尤其是在攻击不直接针对任务核心方面时。通过添加额外的文本上下文，LMMs对视觉对抗性输入的鲁棒性得到了显著提高。在实际应用中，查询分解方法在COCO和ImageNet分类任务中显示出了显著的鲁棒性提升。

阅读总结报告： 本文对LMMs在面对视觉对抗性攻击时的鲁棒性进行了全面的研究。研究发现，尽管LMMs在多模态任务中表现出色，但它们对视觉对抗性扰动的鲁棒性不足。通过实验，研究者们展示了上下文信息对提高LMMs鲁棒性的重要性，并提出了查询分解这一新方法，以增强模型在实际应用中的鲁棒性。这项研究为未来在对抗性环境中提高多模态系统鲁棒性的工作奠定了基础。



注1：

在这篇论文中，研究者们提出了一种新的方法来评估大型多模态模型（LMMs）对视觉对抗性输入的鲁棒性。这种方法的核心在于系统地分析LMMs在面对不同类型的视觉对抗性攻击时的表现，并探讨了上下文信息对模型鲁棒性的影响。以下是该方法的详细说明：

1. **对抗性攻击的生成**：
   * 研究者们选择了三种代表性的对抗性攻击方法：投影梯度下降（PGD）、近似投影梯度下降（APGD）和Carlini-Wagner（CW）攻击。
   * 这些攻击方法都是基于梯度的白盒攻击，它们通过计算输入的梯度来确定如何修改输入，以便在满足Lp范数约束的情况下欺骗模型。
   * 攻击仅针对图像编码器，而不涉及文本编码器，以评估视觉编码器单独受到攻击时LMMs的鲁棒性。
2. **评估任务和数据集**：
   * 研究者们在多个视觉任务上评估了LMMs的鲁棒性，包括图像分类、图像描述（caption retrieval）和视觉问答（VQA）。
   * 使用了COCO数据集的验证集进行图像分类和描述任务的评估，以及多个流行的VQA数据集（如VQA V2、ScienceQA-Image、TextVQA、POPE和MME）进行VQA任务的评估。
3. **上下文信息的作用**：
   * 研究者们发现，当模型接收到的上下文信息（如VQA问题）与攻击目标不匹配时，LMMs显示出了较高的鲁棒性。
   * 例如，在ScienceQA任务中，即使视觉编码器的准确率显著下降，LMMs的准确率仅下降了8.10%，而视觉编码器的准确率下降了99.73%。
4. **查询分解方法**：
   * 为了提高LMMs在实际应用中的鲁棒性，研究者们提出了一种名为“查询分解”的新方法。
   * 在这种方法中，输入提示被分解为多个存在性查询，每个查询都与相应的上下文相关联。这种方法允许模型在面对对抗性图像时，通过上下文信息来恢复对象属性，并与正确的对象匹配。
5. **实验结果**：
   * 实验结果表明，添加上下文信息可以显著提高LMMs在图像分类任务中的鲁棒性。例如，在COCO和ImageNet数据集上，使用查询分解方法后，模型在对抗性攻击下的准确率下降幅度显著减小。

总结来说，这项研究通过系统地评估LMMs在不同对抗性攻击下的鲁棒性，并提出了一种新的查询分解方法来增强模型的鲁棒性。这种方法不仅提高了模型在对抗性环境中的表现，也为未来在这一领域的研究提供了新的视角和方法。



注2：

查询分解方法（Query Decomposition）是本文中提出的一种策略，旨在提高大型多模态模型（LMMs）在面对视觉对抗性攻击时的鲁棒性。这种方法的核心思想是将一个复杂的查询分解成多个更简单的存在性查询，每个查询都关注于图像中的一个特定对象或属性。以下是查询分解方法的详细说明：

1. **目的**：
   * 在封闭世界图像分类任务中，当对象类别列表是固定的，查询分解方法可以帮助模型更准确地识别图像中的对象，尤其是在对抗性攻击可能存在的情境下。
2. **方法**：
   * 对于给定的图像，首先随机选择一定数量的对象类别（例如20个），确保正确类别包含在内。
   * 对于每个选定的对象类别，生成一个存在性查询，询问该类别的对象是否存在于图像中。
   * 为每个存在性查询提供一个与该对象相关的上下文描述，这有助于模型更好地理解和识别对象。
   * 模型对每个存在性查询给出回答，然后从所有回答中选择置信度最高的对象作为最终答案。
3. **实现**：
   * 查询分解可以并行处理，因为每个存在性查询都是独立的，这提高了效率。
   * 在实际应用中，这种方法可以支持在图像中检测特定对象（如非法物品），即使图像可能被有意操纵或对抗性攻击。
4. **效果**：
   * 实验结果表明，使用查询分解方法可以显著提高LMMs在COCO和ImageNet分类任务中的鲁棒性。
   * 与没有上下文信息的普通分类方法相比，查询分解方法在对抗性攻击下的性能下降幅度更小。
5. **优势**：
   * 查询分解方法允许模型在对抗性攻击下更好地恢复对象属性，因为它提供了额外的上下文信息来辅助模型的决策过程。
   * 这种方法可以作为一种实用的策略，帮助模型在面对潜在的对抗性输入时保持较高的准确率。

总结来说，查询分解方法通过将复杂查询分解为多个简单查询，并为每个查询提供相关上下文，从而提高了LMMs在对抗性环境中的鲁棒性和准确性。这种方法为提高多模态系统在实际应用中的鲁棒性提供了一种新的视角。
