# Efficient Adversarial Training in LLMs with Continuous Attacks

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### 研究背景

随着大型语言模型（LLMs）在各种应用中的集成越来越多，确保它们的安全性和鲁棒性变得至关重要。研究表明即使是最先进的专有模型也存在显著的漏洞，对抗性攻击可以有效禁用安全机制。适应性攻击在广泛使用的模型上展示了近乎100%的成功率，突显了这一问题的严重性。

#### 过去方案和缺点

对抗性训练，即在线增强神经网络的训练数据以对抗性攻击，已被证明可以提高对敌手的鲁棒性。然而，在LLMs的背景下，当前的对抗性训练方法受到每次训练迭代执行离散对抗性攻击所需的高计算成本的阻碍。例如，R2D2算法使用Greedy Coordinate Gradient (GCG) 生成自然语言中的离散对抗性后缀，但GCG需要大量的计算资源，计算单个攻击需要数十万次模型评估。

<figure><img src="../.gitbook/assets/image (5) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### 本文方案和步骤

本文提出了一种在LLM的连续嵌入空间中计算对抗性攻击的方法，这种方法比传统的离散方法高效得多。我们提出了一种快速的对抗性训练算法（CAdvUL），由两个损失组成：第一个使模型在对抗性行为数据集上计算的连续嵌入攻击上变得健壮；第二个通过在效用数据上微调来确保最终模型的实用性。此外，我们引入了C-AdvIPO，这是IPO的一个对抗性变体，它不需要效用数据来进行对抗性鲁棒对齐。

#### 本文创新点与贡献

* 提出了一种新的对抗性训练算法CAdvUL，它在计算上比现有方法更高效。
* 引入了C-AdvIPO算法，这是IPO的一个变体，它不需要额外的效用数据集来维持模型的有用性。
* 证明了对连续扰动的鲁棒性可以外推到离散威胁模型，为LLMs的鲁棒对齐提供了可扩展的对抗性训练算法的路径。

#### 本文实验

实验在四种不同家族（Gemma, Phi3, Mistral, Zephyr）和不同规模（2B, 3.8B, 7B）的模型上进行了实证评估。使用了三种不同的对抗性攻击（GCG, AutoDAN, PAIR）来评估模型的鲁棒性，并使用常用的基准测试（MMLU, ARC-E, ARC-C, MT-BENCH）来评估模型的效用。

#### 实验结论

实验结果表明，CAdvUL和C-AdvIPO算法显著提高了LLMs对离散攻击的鲁棒性，同时保持了效用。C-AdvIPO在不需要额外效用数据集的情况下，实现了高达100%的攻击鲁棒性，并且在HARMLESS基准测试上取得了显著的改进。

#### 全文结论

本文的研究问题得到了积极的回答，即在连续攻击威胁模型下的鲁棒性可以外推到离散攻击下的鲁棒性。我们提出的算法在提高鲁棒性的同时，大大减少了计算资源的需求，并且通过仔细的评估展示了对抗性训练模型的鲁棒性和效用。

#### 阅读总结报告

这篇论文针对大型语言模型在面对对抗性攻击时的脆弱性，提出了一种新的高效的对抗性训练方法。通过在连续嵌入空间中进行对抗性攻击的计算，而不是传统的离散方法，显著降低了计算成本。作者提出的CAdvUL和C-AdvIPO算法不仅提高了模型对现有离散攻击的鲁棒性，还通过实验证明了它们在保持效用方面的优势。此外，论文还指出了在评估对抗性训练模型时需要注意的问题，如模型可能过度拟合安全目标而拒绝良性输入。这项工作为如何构建更安全、更健壮的大型语言模型提供了有价值的见解和方法。
