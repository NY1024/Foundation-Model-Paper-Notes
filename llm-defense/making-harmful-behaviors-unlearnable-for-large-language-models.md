# Making Harmful Behaviors Unlearnable for Large Language Models

<figure><img src="../.gitbook/assets/image (18) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 大型语言模型（LLMs）在各种AI应用中展现出巨大潜力，但它们的学习能力强，容易学习到不期望的行为。例如，即使是安全对齐的LLMs，也可以通过包含隐含或明确有害内容的微调数据被轻易地转变为有害的助手。这引发了一个问题：我们能否在不学习有害行为的情况下训练LLMs？

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1)  (35).png" alt=""><figcaption></figcaption></figure>

1. 过去方案和缺点： 以往的研究主要集中在通过人类反馈的强化学习来训练LLMs，使其响应更加有帮助、真实和无害。然而，即使在大量资源投入安全对齐的情况下，这些对齐的LLMs也可以通过少量有害数据的进一步微调被轻易破坏。此外，即使在良性数据上进行微调，模型的安全性也可能被妥协。这种不可控的学习能力和数据中的隐含有害内容显著提高了微调的安全风险。
2. 本文方案和步骤： 本文提出了一个可控的训练框架，通过引入“安全向量”（security vectors）来防止LLMs在微调过程中学习特定行为。安全向量是一组新的参数，可以在LLM的参数之外进行训练，以确保LLM的响应与有害行为一致。在微调期间，激活安全向量，使LLM认为已经学习了这种行为，无需进一步优化有害数据。在推理期间，可以停用安全向量以恢复LLM的正常行为。实验结果表明，通过100个有害样本生成的安全向量足以防止LLM学习1000个有害样本，同时保留学习其他有用信息的能力。

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1)  (33).png" alt=""><figcaption></figcaption></figure>

1. 本文实验和性能： 实验结果表明，安全向量能够有效防止LLM学习有害行为，即使在大规模有害数据上进行微调。此外，使用安全向量进行微调的LLM在其他任务上的性能与直接微调的LLM相当，同时保持了类似的安全水平。安全向量仅使有害行为不可学习，而不影响LLM对其他数据的学习能力。所有实验都使用在100个有害样本上训练的安全向量进行，证明了该方法不仅有效，而且数据高效。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1)  (23).png" alt=""><figcaption></figcaption></figure>

注1：

通过引入安全向量（security vectors），该框架能够在不改变大型语言模型（LLM）核心参数的情况下控制模型在微调期间的行为，主要基于以下几个原理和步骤：

1. **参数分离**：安全向量是一组额外的参数，它们与LLM的核心参数（即预训练参数）是分离的。这意味着在微调过程中，只有核心参数会被更新，而安全向量保持固定。这样，即使在有害数据上进行微调，核心参数也不会被引导去学习有害行为。
2. **一致性引导**：在微调过程中，激活的安全向量会引导LLM的输出与有害数据的响应保持一致。由于LLM的输出已经与有害行为一致，模型会认为没有进一步优化的空间，从而不会从有害数据中学习。
3. **微调控制**：在微调过程中，只有LLM的核心参数会被更新，而安全向量在前向传播中被激活，但不参与反向传播。这意味着在微调过程中，模型的有害行为被“锁定”，而模型的其他部分仍然可以学习新的有用信息。
4. **推理期间的停用**：在模型部署和推理期间，安全向量会被停用，这样LLM就会恢复到其正常的、安全的行为模式。这意味着模型在实际应用中不会表现出在微调期间可能被激活的有害行为。
5. **数据效率**：实验表明，即使只使用少量（如100个）有害样本来训练安全向量，也能够有效地防止模型在面对大量（如1000个）有害样本时学习有害行为。这表明安全向量方法在数据使用上是高效的。

通过这种方式，安全向量提供了一种机制，允许模型在微调过程中对有害行为保持“免疫”，同时仍然能够学习和适应新的、无害的任务。这种方法有助于在保持模型性能的同时，确保模型的安全性和道德行为。



注2：

在论文中，模型的有害行为通常是指那些违反了安全、道德或法律标准的行为。这些行为可能包括但不限于：

1. **违反安全准则**：模型可能会生成或推荐有害内容，如暴力、歧视、仇恨言论、非法活动（如制造炸弹的指南）等。
2. **误导性信息**：模型可能会提供错误或误导性的信息，这可能导致用户做出不安全或不道德的决策。
3. **隐私泄露**：模型可能会泄露用户的私人信息，或者在不应该的情况下访问和使用敏感数据。
4. **不道德的建议**：模型可能会提供不道德的建议，如鼓励用户进行欺诈、侵犯他人权利或其他不道德行为。

为了判断模型的有害行为，研究者通常会采取以下方法：

* **定义标准**：首先，需要明确定义什么是有害行为。这通常涉及到对安全、道德和法律标准的理解和应用。
* **数据集构建**：使用包含有害内容的数据集来训练模型。这些数据集通常由专家或通过社区反馈创建，包含了各种有害行为的例子。
* **模型评估**：在模型训练和微调过程中，使用特定的评估指标来衡量模型输出的有害性。这可能包括自动化的有害内容检测工具，或者由人类评估者进行的手动审查。
* **反馈循环**：在模型部署后，收集用户反馈和社区报告，以识别模型可能产生的有害行为。这些反馈可以用来进一步调整模型，以减少有害输出。
* **持续监控**：对模型的输出进行持续监控，以便及时发现和处理有害行为。这可能涉及到实时监控系统，以及定期的审计和评估。

在论文中提出的框架中，研究者通过训练安全向量来识别和防止这些有害行为。安全向量在模型微调期间被激活，以确保模型的响应与有害行为一致，从而防止模型从有害数据中学习。这种方法允许模型在不改变其核心参数的情况下，避免学习到有害行为。



阅读总结报告： 本文提出了一种新的训练框架，旨在防止LLMs在微调过程中学习有害行为。通过引入安全向量，该框架能够在不改变LLM核心参数的情况下，控制模型在微调期间的行为。实验结果表明，这种方法能够有效地防止LLMs学习有害行为，同时保持其在其他任务上的学习能力。这对于减少微调过程中的安全风险、允许用户进行安全的微调和促进企业提供更安全的API微调服务具有重要意义。
