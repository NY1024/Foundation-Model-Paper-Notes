# Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning

<figure><img src="../.gitbook/assets/image (118).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

大型语言模型（LLMs）在多种应用中取得了巨大成功，但它们也容易受到特定提示的影响，这些提示可以绕过内置的安全措施，提供危险或非法内容，这种现象被称为“越狱”（jailbreak）。为了保护LLMs不产生有害信息，研究者们提出了各种防御策略，主要集中在内容过滤或对抗性训练模型上。

### 2. 过去方案和缺点

现有的防御方法主要关注特定类型的对抗性攻击或模型训练过程，忽略了越狱攻击的根本原因：LLMs的输入提示。这些方法通常需要大量的计算成本，因为它们需要对整个大型语言模型进行微调。

<figure><img src="../.gitbook/assets/image (119).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种名为Prompt Adversarial Tuning (PAT)的方法，通过训练一个防御控制机制，然后将该机制作为前缀嵌入到用户提示中以实施防御策略。PAT的设计类似于对抗性训练，通过交替更新攻击和防御控制来实现优化目标。

### 4. 本文创新点与贡献

* 提出了PAT方法，这是首次从提示调整的角度考虑提高模型防御能力。
* PAT方法在不影响LLMs操作效率的情况下，能够有效地防御越狱攻击，同时保持对简单良性问题的良性回答率。
* 实验结果表明，PAT在多种设置下有效，能够将高级攻击的成功率降低到接近0，同时在模型的可用性上几乎没有显著影响。

### 5. 本文实验

实验在Vicuna-7B和Llama-2模型上进行，使用了三种数据集：有害提示和目标、有害提示和安全目标、良性提示和目标。实验设置了多种防御策略，并与PPL、ICD等基线防御方法进行了比较。

### 6. 实验结论

PAT在单模型和多模型设置下都显示出了出色的防御性能，能够有效降低越狱攻击的成功率，同时保持对正常请求的响应能力。PAT在白盒和黑盒设置下都表现出良好的转移性。

### 7. 全文结论

本文提出了PAT方法，这是一种新的防御越狱攻击的方法，它通过在用户提示前添加防御控制来提高模型的鲁棒性。PAT在不影响模型正常使用的情况下，能够有效地防御对抗性攻击，具有很好的转移性和通用性。这项工作可能为未来LLMs安全性研究提供新的视角。



注1：

Prompt Adversarial Tuning (PAT) 是一种旨在提高大型语言模型（LLMs）对越狱攻击（jailbreak）鲁棒性的方法。PAT 的核心思想是通过调整用户输入提示（prompts）来训练一个防御控制机制，然后将这个机制作为前缀嵌入到用户的实际提示中。以下是PAT的主要步骤和方法：

1. **威胁模型定义**：PAT 首先定义了一个威胁模型，考虑了攻击者可能拥有对模型的完全知识（白盒模型）或部分知识（灰盒模型）的情况。
2. **防御公式化**：PAT 设计了用户提示的格式，包括有害目标和安全目标。有害目标是攻击者试图让模型输出的内容，而安全目标是模型应该拒绝的请求。PAT 通过交替更新攻击控制和防御控制来优化防御策略。
3. **个体提示对抗性调整（IPAT）**：在这种设置中，PAT 为每个特定的恶意请求生成相应的防御控制。这涉及到使用梯度信息来生成候选令牌，并评估这些候选令牌以确定对抗性目标的最优化结果。
4. **提示对抗性调整（PAT）**：PAT 寻求一个可转移的防御控制，它可以在多种恶意请求下表现出良好的防御性能。PAT 通过优化单一的攻击控制和防御控制来处理多个恶意提示，并在正常请求下尽可能保持原始输出。
5. **训练过程**：PAT 的训练过程类似于对抗性训练，它交替更新攻击和防御控制。这包括计算梯度信息，生成候选令牌，评估这些候选令牌，并选择最佳替换。
6. **防御控制的部署**：一旦训练完成，PAT 生成的防御控制将作为前缀添加到用户消息中，以减少模型输出恶意内容的风险。
7. **实验设置**：PAT 在多种数据集上进行训练和测试，包括有害行为数据集、MS MARCO 数据集等。实验设置了不同的防御策略，并与基线防御方法进行了比较。
8. **评估指标**：PAT 的性能通过攻击成功率（ASR）和良性回答率（BAR）来评估。ASR 表示越狱攻击成功绕过模型对齐或防御措施的比例，而 BAR 表示模型成功识别良性请求的精确度。

PAT 的创新之处在于它提供了一种新的视角来增强LLMs的安全性，通过调整提示而不是直接修改模型参数，从而在不显著影响模型性能的情况下提高了对越狱攻击的抵抗力。



注2：

将Prompt Adversarial Tuning (PAT) 生成的防御控制机制作为前缀嵌入到用户提示中可以实施有效防御的原因在于这种方法直接作用于模型接收的输入数据，即提示（prompt）。这种方法的核心优势和原理如下：

1. **输入控制**：LLMs 的输出很大程度上取决于它们接收到的输入。通过在用户提示前添加特定的防御控制前缀，PAT 能够改变模型接收到的信息，从而影响模型的输出。
2. **对抗性训练**：PAT 的训练过程类似于对抗性训练，它通过交替更新攻击控制和防御控制来优化目标。这种训练方式使得生成的防御控制能够有效地对抗特定的攻击策略。
3. **防御泛化**：PAT 生成的防御控制不仅针对已知的攻击模式，而且具有泛化能力，可以应对未知的或新的攻击尝试。这意味着即使攻击者尝试新的越狱策略，PAT 也能够提供一定程度的保护。
4. **模型参数不变**：与需要对整个模型进行微调的方法不同，PAT 不需要改变模型的参数。这使得部署和更新更加灵活和高效，因为只需要调整用户输入的前缀部分。
5. **保持模型性能**：由于防御控制前缀通常较短，它对模型的操作效率影响很小。这意味着在提供防御的同时，模型对正常请求的响应能力和性能几乎不受影响。
6. **实时防御**：PAT 允许模型开发者在模型推理阶段实时地对用户输入进行处理，从而在模型与用户交互时提供即时的保护。

通过这些机制，PAT 能够有效地提高模型对越狱攻击的抵抗力，同时保持对合法请求的响应能力，从而在不牺牲用户体验的情况下增强了模型的安全性。



注3：

PAT（Prompt Adversarial Tuning）的前缀是经过特别设计和优化的，以实现对大型语言模型（LLMs）的保护。这个前缀的特殊之处在于：

1. **对抗性训练产物**：PAT 前缀是通过对抗性训练过程生成的，这意味着它专门设计来抵抗已知和未知的越狱攻击。在训练过程中，PAT 交替更新攻击控制和防御控制，以找到能够有效阻止恶意输出的前缀。
2. **针对性优化**：PAT 前缀针对特定的恶意目标（如生成危险内容的请求）进行优化。它通过调整输入提示的结构，使得模型更倾向于拒绝这些恶意请求，同时保持对正常请求的合理响应。
3. **可转移性**：PAT 前缀旨在具有较好的可转移性，这意味着它可以在不同的模型和不同的攻击场景中使用，而不仅仅是针对特定的模型或攻击类型。
4. **长度控制**：为了确保对模型操作效率的影响最小，PAT 前缀的长度被控制在较短范围内。这使得它在提供防御的同时，不会显著增加模型处理每个请求的计算负担。
5. **与用户提示结合**：PAT 前缀被设计为可以与用户的实际提示结合使用。在模型接收到用户输入时，这个前缀会被添加到用户提示的前面，从而在模型处理输入时立即发挥作用。
6. **安全性与可用性平衡**：PAT 前缀的设计考虑到了在提高安全性的同时，尽可能减少对模型正常使用的影响。这意味着它旨在保持模型对合法请求的响应能力，避免对用户体验造成负面影响。

总的来说，PAT 前缀是专门为提高LLMs安全性而设计的，它通过在用户输入中引入特定的结构和内容，来引导模型产生更安全、更符合预期的输出。



### 阅读总结

本文针对LLMs的越狱攻击问题，提出了一种新的防御策略PAT。PAT通过在用户输入前添加防御控制前缀，有效地提高了模型对攻击的抵抗力，同时保持了模型对正常请求的响应能力。实验结果表明，PAT在不同模型和攻击类型下都表现出了良好的防御效果。这项工作为LLMs的安全性研究提供了新的思路，并对如何构建免疫越狱攻击的LLMs提供了指导。
