# Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning

<figure><img src="../.gitbook/assets/image (4).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

大型语言模型（LLMs）近年来在功能和使用上都有了显著增长。随着像ChatGPT这样的模型的发布，LLMs受到了前所未有的关注。为了减少生成危险或敏感内容的风险，LLMs通常会经过进一步的微调，以符合人类价值观。然而，随着LLMs的普及，也出现了对抗性提示（即jailbreaks），用户试图绕过这些模型的安全对齐。此外，由于LLMs的庞大规模和需求，部署成为了一个重大挑战，促使人们使用模型压缩技术来实现良好的扩展。但是，压缩对安全性的影响并不容易描述，特别是在保持对抗性鲁棒性方面。

### 2. 过去方案和缺点

以往的研究中，为了提高LLMs的安全性，采用了多种技术，如通过人类反馈的强化学习（RLHF）进行微调，以及教师-学生蒸馏方法。此外，还有研究开发了基于梯度的防御机制来对抗jailbreak提示，但这些方法计算开销较大。先前的研究还表明，低秩层剪枝实际上可以在不进一步训练的情况下提高LLM的推理能力。然而，这些方法在提高安全性方面的效果并不一致，且可能会影响模型的其他性能。

### 3. 本文方案和步骤

本研究探讨了剪枝对LLMs安全对齐的影响。研究者们策划了一个包含2250个提示的数据库，旨在从LLMs中引出恶意输出。研究的重点是三个7亿参数模型：LLaMA-2 Chat、Vicuna-1.3和Mistral Instruct v0.2。研究方法是比较未剪枝模型与剪枝后的版本在恶意提示中的拒绝率，观察在不同模型压缩水平下的变化。

### 4. 本文创新点与贡献

* 创新了一个用于研究LLMs安全性的数据集，并开源了该数据集。该数据集包括来自五个主要类别的225个恶意任务。
* 利用最近引入的剪枝算法\[25]来提高LLM在jailbreaking攻击下的安全性。研究表明，该方法在各种任务中提供了一致的安全性改进，且改进程度取决于未剪枝模型的安全训练水平。
* 分析了剪枝和未剪枝模型的注意力图，发现剪枝模型的注意力图显著更集中在任务token上。

### 5. 本文实验

实验中，研究者们使用了Wanda方法\[25]对基础模型进行剪枝，达到了10%、20%和30%的稀疏度。对每个恶意任务的每个jailbreaking提示，从未剪枝的基础模型和剪枝模型中生成响应。然后，通过一个定制的微调ChatGPT-3.5 Turbo模型对每个响应进行分类。

### 6. 实验结论

实验结果表明，对于不同类型的恶意任务，剪枝可以显著提高LLM的安全性。特别是，剪枝20%的权重可以提高Vicuna 1.3和LLaMA-2 Chat模型的拒绝率。然而，当剪枝达到30%时，LLaMA-2的安全性会下降，而Vicuna的改进减少。相比之下，Mistral Instruct v0.2在剪枝后显示出小但不一致的变化。

### 7. 全文结论

本文通过应用Wanda剪枝方法，研究了剪枝对LLMs抵抗jailbreak攻击的影响。结果表明，如果未剪枝模型已经接受了充分的安全训练，那么在较低的稀疏度下剪枝可以提高安全性，但是过度剪枝会降低安全性。这表明可以通过仔细选择剪枝量来帮助安全LLMs的部署。

### 阅读总结

本文通过实验研究了剪枝技术在提高大型语言模型抵抗jailbreak攻击方面的潜力。通过对比不同稀疏度下的剪枝模型，研究者们发现适度的剪枝可以提高模型的安全性，但是过度剪枝可能会适得其反。此外，研究还发现剪枝有助于模型集中注意力在任务相关的token上，从而提高了对恶意任务的检测能力。这些发现为未来在确保LLMs安全性的同时实现可扩展部署提供了重要的见解和技术路径。
