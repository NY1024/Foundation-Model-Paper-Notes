# Improving the Robustness of Large Language Models via Consistency Alignment

<figure><img src="../.gitbook/assets/image (6) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 研究背景

大型语言模型（LLMs）在遵循用户指令和生成有用回应方面取得了巨大成功。然而，它们的鲁棒性仍远非理想，因为它们可能会因指令的微小变化而产生显著不一致的回应。最近的文献探讨了这种不一致性问题，强调了继续提高响应生成鲁棒性的重要性。然而，系统的分析和解决方案仍然缺乏。

<figure><img src="../.gitbook/assets/image (7) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 过去方案和缺点

以往的研究通过指令微调对预训练模型进行优化，以帮助LLMs理解指令并生成人类期望的回应。例如，使用PPO优化微调模型以学习人类偏好，或者使用奖励排名微调方法选择模型输出进行基础LLMs的微调。尽管如此，这些方法缺乏对当前LLMs在生成响应的一致性方面的量化分析，也没有提出系统性的解决方案。

<figure><img src="../.gitbook/assets/image (8) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 本文方案和步骤

本文提出了一个通过一致性对齐来提高LLMs鲁棒性的新颖训练框架。具体来说，该框架包括两个阶段的训练：指令增强的监督微调和一致性对齐训练。

1. 在增强的监督微调（SFT）阶段，首先对SFT数据集中的原始指令进行释义，然后将每个释义后的指令与原始回应配对形成新的增强训练样本，最后将所有增强的训练样本添加到SFT数据集中以微调LLMs。
2. 在一致性对齐阶段，将释义后的指令输入LLMs以生成候选回应，然后构建<好，坏>回应对，其中每个回应通过一致性得分单独评估。最后，通过离线训练算法优化LLMs，直接学习偏好。

## 本文创新点与贡献

1. 提出了一个集成的训练框架来增强LLMs的鲁棒性。
2. 提出了使用自我奖励来提高大型语言模型性能的方法，无需参考外部人类偏好资源或外部奖励模型。
3. 在多个公共LLMs上进行了广泛的实验，验证了训练框架方法的有效性。

## 本文实验

实验使用了Super Natural Instructions数据集，包括1600多个不同的NLP任务。在实验中，训练了Vicuna-7B、Vicuna-13B、Llama2-7B和Llama2-13B模型，并与原始LLM、标准监督微调（SFT）方法以及现有的SOTA LLMs（如ChatGPT和GPT-4）进行了比较。评估指标包括一致性率（CR）、最大一致性率（MCR）、ROUGE-1和ROUGE-L分数。

## 实验结论

实验结果表明，通过明确添加一致性自我对齐，这些LLMs可以获得鲁棒性改进，并在遵循指令方面更好地泛化。特别是，Vicuna13B + SFT (IA) + CAT在实验设置中超过了SOTA LLM GPT-4。

## 全文结论

本文通过引入新颖的训练框架，包括指令增强的监督微调和响应一致性对齐训练，提高了LLMs在遵循指令方面的鲁棒性和泛化能力。广泛的实验验证了该训练框架的有效性，并证明了它在提高现有LLMs性能方面的潜力。

## 阅读总结报告

本篇论文提出了一个针对大型语言模型（LLMs）在生成响应一致性方面的训练框架，旨在解决LLMs在面对语义等价但表述不同指令时产生不一致回应的问题。通过量化分析，作者们发现现有的LLMs在一致性方面仍有较大的提升空间。为此，他们设计了一个两阶段的训练框架，包括指令增强的监督微调和一致性对齐训练，以提高模型的鲁棒性。 在实验部分，作者们使用了公开的数据集和多种模型进行测试，并将提出的训练方法与现有方法进行了比较。结果显示，通过一致性对齐训练，模型在一致性率和ROUGE分数上都有显著提升，证明了该训练框架的有效性。 总体而言，本文的研究不仅揭示了LLMs在一致性方面的不足，还提供了一种切实可行的改进方案，对推动LLMs的发展具有重要意义。
