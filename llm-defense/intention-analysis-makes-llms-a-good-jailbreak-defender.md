# Intention Analysis Makes LLMs A Good Jailbreak Defender

<figure><img src="../.gitbook/assets/image (10) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 研究背景

本研究关注的是大型语言模型（LLMs）在面对隐蔽和复杂的越狱攻击时，如何与人类价值观保持一致的挑战。越狱攻击是专门设计来绕过安全策略，操纵LLMs产生受限输出的攻击方式，这在实际应用中带来了巨大的风险。尽管已有多种策略专注于防御LLMs遭受越狱攻击，但当处理那些具有隐蔽和复杂意图的攻击时，这些策略的有效性显著降低。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 过去方案和缺点

以往的努力，如人类反馈的强化学习（RLHF），旨在减轻这些风险并增强LLMs与人类价值观的一致性，使LLMs能够拒绝有害的问题，如如何抢劫银行。然而，LLMs仍然容易受到一些对抗性输入的影响，尤其是在所谓的“越狱”攻击的背景下。这些越狱攻击通常是手工制作的提示，使用更多的指令和特定技术，例如创建一个虚拟场景，让LLMs扮演“DAN”（Do Anything Now）的角色。这种隐蔽和复杂的意图对LLMs在面对这些越狱攻击时保持安全性构成了巨大挑战。

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 本文方案和步骤

本文提出了一种简单但高效的防御策略，即意图分析（Intention Analysis, IA）。IA的核心是通过两阶段过程触发LLMs的内在自我纠正和改进能力：1) 基本意图分析；2) 策略一致的响应。IA是一种仅基于推理的方法，可以在不损害其有用性的情况下显著增强LLMs的安全性。具体步骤如下：

* **阶段1：基本意图分析** - 指导LLMs分析用户查询的基本意图，重点关注安全性、伦理和合法性。
* **阶段2：策略一致的响应** - 在识别了基本意图后，引导LLMs提供与安全策略一致的最终响应。



意图分析（Intention Analysis, IA）是一种防御策略，旨在增强大型语言模型（LLMs）在面对复杂越狱攻击时的安全性。IA的核心思想是通过分析用户查询背后的意图，来识别和防止可能导致不安全或有害输出的请求。IA的实现分为两个阶段：

#### 阶段1：基本意图分析（Essential Intention Analysis）

在这个阶段，IA指导LLMs识别用户查询的基本意图。这一步骤的目的是让模型理解用户的真实目的，特别是在涉及安全性、伦理和合法性方面。为了实现这一点，研究者们设计了一种特定的指令格式，要求模型以“查询的基本意图是...”这样的句式开始其响应。这样，模型在生成回答之前，首先需要对用户的意图进行分析。

#### 阶段2：策略一致的响应（Policy-Aligned Response）

在成功识别了用户查询的基本意图之后，IA进入第二阶段，即生成与安全策略一致的最终响应。在这个阶段，模型被指导在考虑已识别的意图的同时，提供一个符合安全政策和伦理标准的响应。这意味着模型需要在不提供任何不安全或不道德信息的前提下，给出回答。

#### 实现细节

1. **指令构造**：研究者们构造了两套指令，一套用于第一阶段的意图分析，另一套用于第二阶段的响应生成。这些指令被设计成能够引导模型进行必要的分析和响应。
2. **模型交互**：在实际应用中，这些指令与用户输入一起被送入LLMs。模型首先处理第一阶段的指令，进行意图分析，然后基于分析结果处理第二阶段的指令，生成最终响应。
3. **安全性评估**：为了评估响应的安全性，研究者们采用了二元自动注释函数来确定响应的有害性。如果响应被判定为无害，那么IA就成功地防御了越狱攻击。
4. **实验验证**：通过在多个数据集上的实验，研究者们验证了IA策略的有效性。实验结果表明，IA能够显著降低攻击成功率，同时保持模型的有用性。

#### 创新点

IA的创新之处在于它是一种仅在推理阶段工作的策略，这意味着它可以在不进行额外安全训练的情况下增强LLMs的安全性。此外，IA通过利用LLMs的内在意图识别能力，巧妙地规避了安全性与有用性之间的权衡，实现了在保持有用性的同时提高安全性的目标。

####





## 本文创新点与贡献

* **IA方法的引入** - 提出了一种新的方法，通过意图分析机制显著增强LLMs在面对复杂越狱攻击时的安全性。
* **IA作为即插即用推理方法** - 巧妙地规避了安全与有用性之间的权衡，可以在任何LLMs上灵活有效地部署。
* **实验验证** - IA在多个基准测试中显著且一致地降低了LLMs输出的有害性，同时保持了有用性，实现了新的最先进性能。

## 本文实验

实验在SAP200和DAN基准测试上进行，涵盖了Vicuna、ChatGLM、MPT、DeepSeek和GPT3.5等模型。实验结果表明，IA能够一致且显著地降低响应的有害性（平均降低46.5%的攻击成功率），并保持了一般有用性。特别是在IA的帮助下，Vicuna-7b在攻击成功率方面甚至超过了GPT-3.5。

## 实验结论

IA方法在不同规模和对齐水平的模型上，都能显著提高安全性，同时保持或提高有用性。IA在多语言越狱攻击场景下也表现出色，即使在资源较少的语言中也能保持性能。此外，IA在与现有先进防御方法的比较中，始终表现出优越性。

## 全文结论

为了应对广泛存在的复杂和隐蔽的越狱攻击，这些攻击对大型语言模型的部署构成风险，本文提出了一种简单但高效的策略，称为意图分析（IA）。IA在推理阶段工作，利用LLMs的内在意图识别能力，指导它们在响应之前先分析用户查询的基本意图。广泛的实验表明，IA能够一致且显著地降低响应的有害性（平均降低46.5%的攻击成功率），同时保持一般有用性。此外，IA对指令的具体表达和意图分析质量都表现出鲁棒性。

## 阅读总结报告

本研究针对大型语言模型在面对越狱攻击时的安全性问题，提出了一种新的防御策略——意图分析（IA）。IA通过两阶段的推理过程，有效地提高了模型在面对复杂攻击时的安全性，同时保持了其有用性。实验结果表明，IA在多个模型和数据集上都取得了显著的性能提升，尤其是在攻击成功率的降低上。此外，IA方法的即插即用特性和对不同指令表达的鲁棒性，使其在实际应用中具有广泛的适用性。研究还指出，尽管IA在防御越狱攻击方面表现出色，但仍需进一步测试以验证其在现实世界场景中的实用性。未来的工作可以探索将IA集成到模型训练中，以降低推理成本，并开发更有效、更健壮的防御策略。
