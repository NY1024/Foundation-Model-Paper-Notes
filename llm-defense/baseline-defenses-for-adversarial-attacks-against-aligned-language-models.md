# BASELINE DEFENSES FOR ADVERSARIAL ATTACKS  AGAINST ALIGNED LANGUAGE MODELS

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1)  (19).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 随着大型语言模型（LLMs）在专业和社会应用中的广泛部署，这些模型的安全性变得至关重要。LLMs的安全性活动主要集中在平台内容审核上，以防止模型对有害问题作出回应。然而，随着LLMs在商业应用中的部署范围和复杂性的增加，出现了更广泛的漏洞。例如，设计不当的客户服务聊天机器人可能被操纵执行交易、退款、泄露用户保护信息或未能正确验证身份。随着LLMs的作用范围和复杂性的扩大，它们的攻击面也在增加。
2. 过去方案和缺点： 以往的研究主要集中在对抗性机器学习领域，但这些研究通常针对的是图像等连续模态的攻击。对于LLMs，早期的攻击尝试由于文本优化的复杂性而受阻。这些攻击通常是通过手动试验和错误或半自动化测试发现的。然而，这些方法在自动化发现和绕过基于手工微调数据和强化学习的保护措施方面存在问题。此外，现有的离散文本优化器在文本攻击中的表现不佳，且优化成本相对较高，这使得标准的自适应攻击对LLMs更具挑战性。
3. 本文方案和步骤： 本文研究了针对LLMs的对抗性攻击的防御策略。作者提出了三个问题：在这一领域中哪些威胁模型在实践中有用？基线防御技术在这一新领域的表现如何？LLMs的安全性与计算机视觉有何不同？作者评估了几种基线防御策略对抗LLMs的主要对抗性攻击，讨论了每种策略在不同设置中的可行性和有效性。特别地，研究了三种类型的防御：检测（基于困惑度）、输入预处理（改述和重新分词）以及对抗性训练。作者还讨论了白盒和灰盒设置，并讨论了考虑的每种防御的鲁棒性-性能权衡。
4. 本文实验和性能： 作者使用Zou等人（2023）提出的通用和可转移攻击作为测试基准，考虑了对抗性机器学习文献中发现的三类防御的基线。这些基线包括通过困惑度过滤检测攻击、通过改述和重新分词移除攻击，以及对抗性训练。对于每种防御，作者探索了白盒攻击变体，并讨论了鲁棒性/性能权衡。实验结果表明，困惑度过滤和改述即使简单，也是有前景的，因为即使在白盒场景下，基于困惑度的检测系统也难以绕过，这使得攻击的有效性受到损害。此外，作者讨论了视觉领域的对抗性训练方法并不直接适用于LLMs，并尝试了自己的变体，表明这仍然是一个开放的问题。

阅读总结报告： 本文深入研究了LLMs面临的对抗性攻击问题，并提出了一系列基线防御策略。作者通过实验评估了这些策略在不同设置下的有效性，并探讨了LLMs安全性与计算机视觉领域的不同之处。研究发现，尽管现有的文本优化器在对抗性攻击中存在弱点，但通过困惑度过滤和改述等策略，可以在一定程度上提高LLMs的安全性。然而，对抗性训练在LLMs领域的应用仍然面临挑战，需要进一步的研究来开发更强大的优化器或探索更有效的防御方法。本文为LLMs的安全性研究提供了宝贵的见解，并为未来的研究方向奠定了基础。
