# MART: Improving LLM Safety with Multi-round Automatic Red-Teaming

<figure><img src="../.gitbook/assets/image (196).png" alt=""><figcaption></figcaption></figure>

## 研究背景

大型语言模型（LLMs）在生成类人文本和进行自然对话方面展现出了显著的能力。然而，人们对于不受控制的生成可能带来的潜在风险（例如偏见或违反社会规范和法律规则的有毒回应）表示担忧。确保LLMs的安全性是一个具有挑战性但至关重要的任务，如果我们希望在避免潜在陷阱的同时获得它们的好处。为了提高LLMs的安全性，通常在模型开发过程中采用手动红队评估（manual redteaming），这涉及主动风险识别，其中人类红队成员使用精心设计的输入来探测LLMs以引发不安全或危险的行为。尽管这种方法通常是有效的，但手动设计恶意提示和提供答案存在显著的局限性。

## 过去方案和缺点

现有的LLMs开发主要依赖于人类红队评估，例如Claude和Llama 2-Chat。这些方法需要大量的人类注释者不断编写提示和响应，这不仅成本极高，而且速度缓慢。尽管通过训练一个代表人类偏好的奖励模型（reward model）来提供模型生成的反馈，从而允许LLM在没有手动响应策划的情况下进行改进，但提示编写仍然主要由人类红队成员驱动。此外，现有的自动红队评估通常只是发现安全风险，而没有解决这些问题。

## 本文方案和步骤

本文提出了一种名为多轮自动红队评估（MART）的方法，该方法结合了自动对抗性提示编写和安全响应生成，显著提高了红队评估的可扩展性和目标LLM的安全性。具体来说，一个对抗性LLM和一个目标LLM以迭代方式相互交互，其中对抗性LLM旨在生成挑战性的提示以引发目标LLM的不安全响应，而目标LLM则在这些对抗性提示上使用与安全对齐的数据进行微调。在每一轮中，对抗性LLM都会针对更新后的目标LLM制定更好的攻击策略，而目标LLM也通过安全微调进行自我改进。

<figure><img src="../.gitbook/assets/image (197).png" alt=""><figcaption></figcaption></figure>

本文提出的多轮自动红队评估（MART）方法通过以下几个关键步骤提高了目标模型的安全性：

1. **初始化模型**：
   * 使用两个监督式微调数据集（LIMA和Open Assistant）来初始化目标模型（Mtgt）和对抗性模型（Madv），以建立对指令遵循的基础能力。
   * 手动策划了一个包含约2400个提示的种子数据集，用于探测大型语言模型已知的局限性，作为红队评估的起始点。
2. **对抗性训练**：
   * 在每一轮迭代中，对抗性模型（Madv）基于之前的成功攻击提示生成新的攻击提示。这些提示旨在激发目标模型产生不安全的响应。
   * 使用一个奖励模型（例如，安全奖励模型Ss和有用性奖励模型Sh）作为代理来提供对模型生成的反馈，以代替昂贵的人类反馈。
3. **安全微调目标模型**：
   * 目标模型（Mtgt）使用对抗性提示和安全对齐的数据进行微调，以提高其对新攻击的防御能力。
   * 通过选择那些安全分数高于特定阈值的提示和响应作为训练数据，目标模型学习如何对这些攻击性提示产生安全和有用的回答。
4. **迭代训练**：
   * 随着目标模型参数的更新，可能会出现新的漏洞或失败模式。MART通过迭代优化Madv和Mtgt，确保Madv能够持续提供有效的攻击，而Mtgt能够防御这些攻击。
   * 在每一轮迭代中，根据奖励模型的反馈，识别并使用成功的攻击提示来训练Madv，同时使用目标模型生成的安全响应来进一步微调Mtgt。
5. **上下文蒸馏和拒绝采样**：
   * 在第一轮迭代中，为了提高目标模型的安全性，使用上下文蒸馏技术生成更安全的回答。
   * 在最后一轮迭代中，为了丰富候选集并提高样本多样性，使用拒绝采样技术从目标模型中为每个提示采样多个回答，并在不同运行中改变采样温度。

通过这些步骤，MART方法不仅能够识别和攻击目标模型的潜在安全漏洞，还能够通过安全对齐的数据微调来强化模型的防御能力。这种方法实现了在保持模型有用性的同时，显著提高模型安全性的目标。经过多轮迭代后，模型在对抗性提示上的违反率显著降低，证明了MART方法在提高LLMs安全性方面的有效性。





## 本文创新点与贡献

MART的主要创新点在于它结合了自动对抗性提示生成和安全响应生成，通过迭代的对抗性红队评估，提高了目标模型的安全性。MART使用一个对抗性模型来不断生成新的攻击提示，同时使用安全对齐数据对目标模型进行微调。这种方法显著提高了红队评估的可扩展性，并在保持模型有用性的同时显著提高了模型的安全性。

## 本文实验

实验部分详细介绍了MART的实验设置，包括使用的数据集、评估提示、自动和人类评估方法，以及与几个基线方法的比较。实验结果表明，MART在经过4轮迭代后，与经过大量对抗性提示编写的LLMs相比，达到了可比的安全性能，同时在非对抗性提示上保持了稳定的有用性。

## 实验结论

实验结果表明，MART能够有效地提高模型的安全性，同时保持其有用性。在对抗性提示评估集上，与指令调整基线相比，违反率降低了高达84.7%。此外，模型在非对抗性提示上的有用性在整个迭代过程中保持稳定，表明目标LLM在遵循指令方面保持了强大的性能。

## 全文结论

本文提出的MART方法通过多轮自动红队评估显著提高了LLM的安全性，同时保持了其有用性。MART通过迭代的对抗性竞争，使目标模型能够防御来自对抗性模型的攻击，同时通过安全对齐数据进行自我改进。这种方法突出了对抗性训练在自动化、可扩展和有效红队评估中的作用，为构建更安全的AI系统提供了一种实用的技术。

## 阅读总结报告

本论文提出了一种新的多轮自动红队评估（MART）方法，旨在提高大型语言模型（LLMs）的安全性。MART通过结合自动对抗性提示生成和安全响应生成，使目标LLM能够在对抗性环境中自我改进。实验结果表明，MART能够有效地降低模型的违反率，同时保持其在非对抗性任务上的有用性。这项工作为减少对人类红队评估的依赖、缩短模型开发周期提供了一种有效的解决方案，并为未来的研究探索提供了新的方向。
