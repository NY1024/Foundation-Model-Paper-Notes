# Defending LLMs against Jailbreaking Attacks via Backtranslation

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

大型语言模型（LLMs）在提供广泛应用的同时，也面临着安全性挑战。尽管LLMs被训练以拒绝有害请求，但它们仍然容易受到越狱攻击（jailbreaking attacks），这些攻击通过重写原始提示来隐藏其有害意图。越狱攻击使得LLMs无法拒绝有害请求，反而生成有害回应。

### 2. 过去方案和缺点

以往的防御方法主要依赖于检测和拒绝对抗性提示，例如通过困惑度过滤器或重构提示。然而，一些越狱攻击生成的对抗性提示更自然、隐蔽，难以被基于检测的方法识别。此外，现有的防御方法可能需要额外的训练或大量查询，效率较低。

### 3. 本文方案和步骤

本文提出了一种新的防御方法，即通过“回译”（backtranslation）来防御LLMs的越狱攻击。具体步骤如下：

* 使用目标LLM从一个输入提示生成初始回应。
* 通过回译模型推断可能导致该回应的输入提示（回译提示）。
* 使用目标LLM再次运行回译提示，并检查模型是否拒绝回译提示。
* 如果模型拒绝了回译提示，则拒绝原始提示。

### 4. 本文创新点与贡献

* 提出的回译防御方法在目标模型生成的回应上操作，而不是直接被攻击者操纵的提示，因此更难以被攻击。
* 利用目标模型固有的能力来拒绝有害请求，无需为额外任务（如分类或回归任务）特别训练模型。
* 防御方法对良性请求的生成质量影响很小，只要回译提示不被拒绝。
* 防御方法成本低，不需要额外训练，且在推理过程中效率高。

### 5. 本文实验

实验使用了三种广泛使用的LLMs作为目标模型，并采用了AdvBench数据集来评估各种防御方法对抗越狱攻击的有效性。实验结果表明，回译防御方法在多种攻击下都取得了较高的防御成功率。

### 6. 实验结论

回译防御方法在对抗现有越狱攻击方面非常有效，且在保持良性输入提示的生成质量方面表现良好。此外，该方法对不同的回译模型选择不敏感，表明其鲁棒性。

### 7. 全文结论

本文提出了一种新颖的回译防御方法，通过在目标模型的回应上操作，有效地防御了LLMs的越狱攻击。该方法不仅效率高、成本低，而且对良性输入的生成质量影响小，为LLMs的安全性提供了新的视角。

### 阅读总结

本文针对LLMs在面对越狱攻击时的脆弱性，提出了一种基于回译的防御策略。通过在模型回应的基础上进行操作，该策略能够有效地识别并拒绝有害请求，同时保持对良性请求的高质量生成。实验结果证明了该方法的有效性和鲁棒性，为LLMs的安全性研究提供了有价值的贡献。
