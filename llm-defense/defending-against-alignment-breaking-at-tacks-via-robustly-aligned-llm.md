# DEFENDING AGAINST ALIGNMENT-BREAKING AT TACKS VIA ROBUSTLY ALIGNED LLM

<figure><img src="../.gitbook/assets/image (10) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 本文的研究背景是大型语言模型（LLMs）在各种领域的广泛应用，以及它们在生成有害或恶意内容方面的潜在风险。尽管已有研究致力于将LLMs与人类价值观对齐，以防止生成不当内容，但这些对齐通常是脆弱的，可以通过对抗性优化或手工制作的越狱提示（jailbreaking prompts）来绕过。这些攻击可以有效地破坏LLMs的安全对齐，导致生成有害内容。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 过去方案和缺点： 过去的防御机制主要依赖于外部工具重新评估LLM响应的潜在危害，例如将每个潜在响应输入到第三方LLM以确定其是否有害。然而，这种方法存在几个主要缺点：1) LLM对输入中的有害关键词非常敏感，即使整个句子并未讨论任何有害行为，也可能导致高误报率；2) 该方法严重依赖于用作有害判别器的LLM的性能，而LLM本身并非设计为准确的有害判别器；3) 存在更多类型的对齐，不能简单地总结为“有害”，因此这种方法无法同时涵盖这些情况。

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤： 本文提出了一种名为Robustly Aligned LLM（RA-LLM）的防御机制，以抵御潜在的对齐破坏攻击。RA-LLM可以直接在现有的对齐LLM上构建，通过一个健壮的对齐检查功能，无需对原始LLM进行昂贵的重新训练或微调。RA-LLM的核心思想是，即使对齐LLM能在一定程度上识别输入请求是否良性，我们也不能直接依赖这一点，因为它可能不够健壮。我们认为，只有当随机丢弃请求的一部分后，LLM仍然认为它是良性的，我们才将其视为良性。这种随机丢弃操作会使得对齐破坏攻击中的对抗性提示失效，因为这些提示通常对小的扰动很敏感。因此，这种机制自然地导致了RA-LLM的健壮性。
2. 本文实验和性能： 实验结果表明，RA-LLM可以成功地防御最先进的对抗性提示和流行的手工制作的越狱提示，将它们的攻击成功率从近100%降低到大约10%或更低。实验在开源的大型语言模型上进行，包括Vicuna-7B-v1.3-HF和Guanaco-7B-HF模型，以及GPT-3.5-turbo-0613模型。RA-LLM在保持良性回答率的同时，显著降低了攻击成功率，表明其在防御对齐破坏攻击方面的有效性。

阅读总结报告： 本文针对LLMs在对齐破坏攻击面前的脆弱性，提出了一种新的防御机制RA-LLM。RA-LLM通过在现有对齐LLM的基础上增加一个健壮的对齐检查功能，有效地提高了模型对对抗性攻击的抵抗力。实验结果表明，RA-LLM能够在不显著影响良性输入响应的情况下，显著降低对抗性提示的攻击成功率。这种方法不需要对原始LLM进行重新训练或微调，因此在实际应用中具有较高的可行性。尽管RA-LLM在计算成本上有所增加，但考虑到其提供的防御性能，这种成本是可接受的。未来的工作可以探索如何进一步优化RA-LLM，以减少计算成本并提高防御效率。
