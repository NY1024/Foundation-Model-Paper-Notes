# THE POISON OF ALIGNMENT

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 本文探讨了大型语言模型（LLMs）在内容安全问题上的表现，特别是在对齐（alignment）对模型性能的影响。对齐是一种强化模型的方法，旨在限制模型对某些用户输入的响应，以防止生成有害内容。然而，作者指出，尽管对齐在现代开源指令调整数据集中普遍存在，如OpenAssistant或Guanaco，但它可能对模型在各种推理基准上的表现产生负面影响。
2. 过去方案和缺点： 过去的研究主要集中在通过监督式微调（SFT）来提高LLMs的性能，尤其是在复杂基准测试和专业考试设置中。然而，Gudibande等人的研究表明，现有的知识蒸馏模型主要模仿对话格式，而不是提高推理能力或事实准确性。此外，对齐可能导致模型在推理任务上的表现不如未对齐的模型。
3. 本文方案和步骤： 作者提出了一种新的数据集清洗方法，用于SFT，特别是关注对齐如何影响指令数据集的质量。他们从GoatChat应用中收集数据，并进行了基本的质量过滤、数据合并、精确和模糊去重以及对齐去除。通过这些步骤，他们创建了一个更高质量的数据集，用于微调模型。
4. 本文实验和性能： 实验结果表明，与未对齐的模型相比，对齐模型在Big Bench (BBH)、Massive Multitask Language Understanding (MMLU)、Human Eval和Discrete Reasoning Over Paragraphs (DROP)等推理基准测试上的表现显著下降，性能差距在4-33%之间。这表明对齐可能像数据集中的“毒药”一样，损害了模型的推理能力。

阅读总结报告： 本文提供了对大型语言模型在对齐过程中可能遇到的问题的新见解。作者通过实验验证了对齐对模型在推理任务上性能的负面影响，并提出了一种新的数据集清洗方法，以提高模型的推理能力。他们的研究强调了在进行监督式微调时，数据集质量的重要性，并为如何构建有效的数据集提供了有价值的见解。尽管本研究基于LLaMA 2模型，并且存在数据偏见、缺乏世界理解和幻觉等局限性，但它为理解如何通过数据清洗和准备来提高SFT的有效性提供了重要的参考。
