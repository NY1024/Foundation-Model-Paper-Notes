# StruQ: Defending Against Prompt Injection with Structured Queries

<figure><img src="../.gitbook/assets/image (6) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>



#### 1. 研究背景

大型语言模型（LLMs）的快速发展为各种应用带来了革命性的变化，特别是在自然语言处理领域。LLMs能够理解和生成文本，使得开发者可以通过简单的指令（prompts）来实现复杂的任务。然而，随着LLMs的能力提升，针对它们的攻击手段也日益复杂，尤其是提示注入（prompt injection）攻击。这种攻击通过在用户数据中注入恶意指令，诱使模型偏离原始应用的指令，从而执行攻击者预设的任务。这种攻击方式对LLM集成应用的安全性构成了严重威胁。

<figure><img src="../.gitbook/assets/image (7) (1).png" alt=""><figcaption></figcaption></figure>

#### 2. 过去方案和缺点

以往的防御方法并未完全解决提示注入攻击的问题。例如，一些方法通过在提示中添加额外的文本来提醒模型注意攻击，但这种方法并不安全。还有的方法通过替换输入中的命令词来防止攻击，但这些方法没有开发或评估一个可以接受任意提示的完整防御系统，因此其有效性尚不明确。此外，这些方法通常无法防御复杂和自适应的攻击，如Completion攻击和Tree-of-Attack（TAP）。

#### 3. 本文方案和步骤

本文提出了一种名为StruQ的系统，它通过结构化查询（structured queries）来防御提示注入攻击。结构化查询将提示和数据分开，通过一个安全的前端将它们格式化为特殊格式，并由一个特别训练的LLM处理这些输入。StruQ的核心是一个经过特殊微调的LLM，它只会响应查询中提示部分的指令。为了实现这一目标，作者提出了一种新的微调策略——结构化指令调整（structured instruction tuning），通过在训练数据中加入包含数据部分指令的样本，并微调模型以忽略这些指令。

#### 4. 本文创新点与贡献

* 提出了结构化查询的概念，将提示和数据分离，以提高模型对提示注入攻击的抵抗力。
* 开发了一种新的微调策略，即结构化指令调整，使模型能够区分提示和数据中的指令。
* 引入了特殊的分隔符和前端过滤机制，以防止恶意指令通过用户数据注入。
* 对多种提示注入攻击技术进行了广泛的评估，并展示了StruQ在这些攻击上的防御效果。

#### 5. 本文实验

实验部分评估了StruQ在多种提示注入攻击上的安全性和实用性。使用了Alpaca和Mistral两种模型进行测试，并采用了AlpacaEval作为实用性的度量标准。实验结果显示，StruQ在大多数攻击上的防御成功率低于2%，但对TAP攻击的防御成功率仍有提升空间。

#### 6. 实验结论

实验结果表明，StruQ能够有效地防御大多数提示注入攻击，特别是那些使用Completion攻击和TAP攻击的复杂攻击。尽管对TAP攻击的防御还未完全成功，但StruQ在保持模型实用性的同时，显著提高了对提示注入攻击的抵抗力。

#### 7. 全文结论

StruQ为保护LLM集成应用免受提示注入攻击提供了一种有前景的方法。通过结构化查询和特殊的微调策略，StruQ在提高安全性的同时，对模型的实用性影响较小。尽管还有待进一步研究以完全防御TAP等高级攻击，但StruQ为未来的研究提供了一个坚实的基础。

#### 阅读总结报告

本论文提出了一种新的防御机制StruQ，旨在保护大型语言模型免受提示注入攻击。通过结构化查询和前端过滤，StruQ能够有效地区分提示和数据，从而防止恶意指令的注入。实验结果表明，StruQ在多种攻击场景下都表现出了较高的安全性，尤其是在面对复杂和自适应攻击时。尽管对TAP攻击的防御还有待加强，但StruQ为LLM的安全性研究提供了有价值的见解和方法。未来的研究可以在此基础上进一步探索和完善LLM的安全性防护措施。
