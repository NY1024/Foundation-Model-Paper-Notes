# Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield

<figure><img src="../.gitbook/assets/image (15) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

随着大型语言模型（LLMs）在各种应用中的广泛使用，其安全性问题日益凸显。LLMs 容易受到对抗性攻击，这些攻击可能导致模型产生有害的回应。为了应对这一挑战，研究者们开发了安全分类器，这是一种计算模型，旨在识别并减轻潜在有害、冒犯性或不道德的输出。然而，现有的安全分类器在面对含有对抗性噪声的输入时往往表现不佳。

### 2. 过去方案和缺点

过去的研究主要集中在通过各种方法增强LLMs的安全性，例如使用困惑度过滤器、预处理阶段的改写、重新标记化等。这些方法在一定程度上有效，但存在局限性，如对随机性敏感、降低LLM响应质量、增加计算成本等。此外，现有的安全分类器在面对复杂的对抗性攻击时，尤其是那些利用优化器自动发现后缀的攻击，显得尤为脆弱。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1)   (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种名为对抗性提示盾（Adversarial Prompt Shield, APS）的轻量级模型，它在检测准确性方面表现出色，并展示了对对抗性提示的韧性。研究者们还提出了一种名为Bot Adversarial Noisy Dialogue (BAND) 数据集的新型策略，用于自主生成对抗性训练数据集。这些数据集旨在增强安全分类器的鲁棒性，并通过实验评估了将对抗性示例纳入训练过程的影响。



APS（Adversarial Prompt Shield）模型是一个为大型语言模型（LLMs）设计的轻量级安全分类器，旨在检测和减轻潜在有害、冒犯性或不道德的输出。以下是APS模型实现的详细说明：

#### 模型架构

APS模型基于DistilBERT模型，这是一个预训练的BERT模型的轻量级版本，它保留了BERT的大部分能力，但参数数量减少了约40%。DistilBERT模型通过添加两个全连接的线性层和一个sigmoid函数来进行二分类任务。模型的输出基于\[CLS] token的输出，产生0（表示“安全”）或1（表示“不安全”）的标签。

#### 数据处理

APS模型处理多轮对话数据时，会选择每个对话中的最后8轮话语来捕捉上下文信息。这些话语包括一个目标话语和七个之前的对话。对于单轮数据，模型会将它们标注为“Human:”。这些标注的话语被连接起来，形成分类器模型的输入。

#### 训练数据集

APS模型在多个安全分类语料库上进行微调，包括Wikipedia Toxic Comments (WTC) 语料库、Build-it, Break-it, Fix-it (BBF) 语料库中的标准、对抗性和多轮对话，Bot-Adversarial Dialogue (BAD) 语料库，以及ANTHROPIC的红队攻击语料库。为了处理类别不平衡问题，模型在训练过程中使用了加权二元交叉熵损失函数，并使用Adam优化器进行优化。

#### Bot Adversarial Noisy Dialogue (BAND) 方法

为了增强模型对对抗性攻击的鲁棒性，研究者们提出了BAND方法，这是一种自主生成对抗性训练数据集的方法。BAND方法通过向原始语料库添加随机字符串作为后缀，生成包含对抗性噪声的对话数据集。这些随机字符串使用三种不同的方法生成：随机方法（Random）、WP 10（单词和标点符号）、WP 20（扩展的随机方法）。这些方法旨在在不增加显著计算成本的情况下，通过在训练过程中引入随机噪声，提高模型对对抗性攻击的抵抗力。

#### 训练过程

APS模型在包含BAND数据集的训练过程中，通过微调来学习区分安全和不安全的话语。模型在训练时会考虑到对抗性噪声，这使得模型在面对实际的对抗性攻击时能够更好地保持其性能。

#### 性能评估

APS模型的性能通过在不同的测试集上评估其不安全F1分数来衡量。这些测试集包括原始语料库和添加了随机噪声的BAND测试集。通过这些评估，研究者们能够了解APS模型在面对对抗性提示时的鲁棒性。

总结来说，APS模型通过使用轻量级的DistilBERT架构、处理多轮对话数据、在多个安全分类语料库上进行微调，以及引入BAND方法来生成对抗性训练数据集，实现了对LLMs安全性的有效增强。这些方法共同提高了模型在面对对抗性攻击时的检测能力和鲁棒性。





### 4. 本文创新点与贡献

* 提出了新的APS分类器，其在准确性和韧性方面超越了现有模型。
* 引入了BAND方法，这是一种创新的方法，可以在不增加通常与创建这些攻击相关的高成本的情况下，增强数据集对抗性攻击的鲁棒性。
* 展示了APS作为系统核心的工作原理，作为一个保护层，确保与LLMs的更安全交互。

### 5. 本文实验

研究者们通过涉及LLMs的评估，展示了他们的分类器有潜力将对抗性攻击导致的攻击成功率降低高达60%。实验包括使用不同的训练数据集训练的五种不同的APS模型，并在不同的测试集上评估了它们的性能。

### 6. 实验结论

实验结果表明，APS模型在面对对抗性提示时表现出显著的韧性，即使在面对复杂的攻击时也能保持较高的性能。特别是，APS Random模型在BAND随机数据集上达到了最先进的性能。此外，通过在训练过程中加入随机噪声数据，可以提高模型对对抗性噪声攻击的鲁棒性。

### 7. 全文结论

本文的研究为下一代更可靠和韧性的对话代理铺平了道路。通过引入APS和BAND方法，研究者们不仅提高了LLMs的安全性，还为如何有效地对抗对抗性攻击提供了新的视角。这些成果对于确保LLMs在实际应用中的安全性具有重要意义。

### 阅读总结

本文针对大型语言模型在面对对抗性攻击时的安全性问题，提出了一种新的安全分类器APS，以及一种生成对抗性训练数据集的方法BAND。通过实验，作者证明了APS在提高模型对抗性攻击鲁棒性方面的有效性，并展示了BAND方法在增强数据集鲁棒性方面的潜力。这些贡献对于提升LLMs的安全性和可靠性具有重要意义，为未来在这一领域的研究提供了新的方向。
