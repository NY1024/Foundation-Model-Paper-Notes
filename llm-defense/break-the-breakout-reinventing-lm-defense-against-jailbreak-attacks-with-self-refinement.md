# Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks  with Self-Refinement

<figure><img src="../.gitbook/assets/image (15) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 本研究的背景是针对语言模型（LMs）在安全性方面的脆弱性。尽管LMs在多个任务上取得了显著进步，但它们仍然面临着安全风险，尤其是在对抗性攻击（如越狱攻击）面前。越狱攻击通过特别设计的提示（prompts）来削弱LM的安全对齐，使得用户可以从LM的响应中获取不道德、非法的知识。这不仅对现实世界服务构成威胁，也可能导致LM提供有害的回应。因此，研究者们致力于开发能够有效防御这类攻击的方法。

<figure><img src="../.gitbook/assets/image (16) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 过去方案和缺点： 以往的研究主要集中在通过安全对齐训练来提高LMs的安全性。然而，这种方法存在两个主要缺点：一是资源消耗大，难以迅速应对快速发展的攻击；二是安全对齐可能导致用户体验下降，即所谓的“对齐税”。此外，现有的训练自由方法（如InContext Defense、Self-Reminder和SmoothLLM）主要针对已经进行了安全对齐的LMs，但在非安全对齐的LMs上的应用仍然显示出脆弱性。

<figure><img src="../.gitbook/assets/image (17) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤： 本文提出了一种名为“自我精炼”（Self-Refine）的方法，该方法通过格式化实现，即使在非安全对齐的LMs中也能实现出色的安全性。自我精炼是一个迭代过程，LM通过自我反馈和精炼来改进其响应。研究者们还提出了一种格式化方法，以提高自我精炼过程的效率，同时减少攻击成功率。此外，研究还观察到非安全对齐的LMs在安全性任务中的表现优于安全对齐的LMs，因为它们提供了更有帮助且安全的响应。

<figure><img src="../.gitbook/assets/image (18) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文实验和性能： 研究者们通过广泛的实验来评估自我精炼方法的有效性，并与其他几种防御基线进行了比较。实验结果表明，自我精炼方法在防御越狱攻击方面表现最佳，且不需要额外的训练。此外，通过格式化的自我精炼方法能够更有效地在更少的迭代次数内达到安全响应，同时保持或提高LM的安全性。在安全性和有用性方面，自我精炼方法都显示出了优越性。

阅读总结报告： 本文针对语言模型在面对越狱攻击时的安全性问题，提出了一种创新的自我精炼方法。该方法通过迭代反馈和精炼过程，能够有效地提高LMs的安全性，即使在未进行安全对齐的LMs上也表现出色。研究者们还通过实验验证了该方法的有效性，并与其他防御策略进行了比较，结果表明自我精炼方法在减少攻击成功率方面具有显著优势。此外，该方法还能够在保持安全性的同时，提供更有帮助的响应，这对于实际应用中的LMs来说是一个重要的进步。尽管如此，研究也指出了自我精炼方法在实际应用中可能面临的计算成本问题，并提出了通过格式化来提高效率的解决方案。总体而言，这项研究为提高LMs的安全性提供了一种新的视角，并为未来在更广泛的LMs中应用安全性措施提供了实用的解决方案。



注：

自我精炼方法（Self-Refine）是一种迭代的提示过程，它允许语言模型（LM）在生成响应后，通过自我反馈来识别和修正潜在的有害内容。这种方法的核心思想是利用LM自身的能力，而不是依赖于额外的训练数据或模型调整，来提高其在面对攻击时的安全性。以下是自我精炼方法的详细步骤：

1. **初始响应生成**：
   * LM首先根据给定的提示（prompt）生成一个初始响应。
   * 这个响应没有经过任何防御机制的处理，可能包含有害内容。
2. **有害内容识别**：
   * 使用成本模型（Cost Model）来评估初始响应的有害程度。成本模型是一个训练有素的模型，能够根据提示和响应对来评估潜在的有害性。
   * 如果响应被认为是有害的（成本模型给出的分数为正），则进入自我精炼过程。
3. **自我反馈**：
   * LM生成反馈，指出初始响应中存在的问题，如非法、不道德或仇恨内容。
   * 为了引导LM更准确地识别有害内容，研究者设计了一个反馈提示，要求LM指出响应中的具体问题。
4. **响应精炼**：
   * LM根据自我反馈，尝试构建一个新的响应，该响应不仅解决了识别出的问题，而且符合道德准则和用户意图。
   * 这个过程可能包括拒绝不当的请求，并提供安全、有帮助的替代方案。
5. **迭代过程**：
   * 自我精炼是一个迭代过程，LM会不断重复反馈和精炼步骤，直到生成一个安全的响应，或者达到预设的迭代次数限制。
6. **格式化策略**：
   * 为了提高自我精炼的效率，研究者提出了格式化方法。这包括使用JSON格式化和代码格式化，以帮助LM更好地集中注意力在精炼任务上，而不是遵循可能导致有害输出的越狱提示。
7. **性能评估**：
   * 自我精炼方法的性能通过攻击成功率（ASR）、成本（Cost）和JB分数（JB Score）来评估。这些指标帮助研究者了解LM在面对越狱攻击时的安全性表现。

自我精炼方法的优点在于它不需要对LM进行额外的训练，可以直接应用于现有的LMs，并且能够有效地提高其在面对攻击时的安全性。此外，这种方法还能够在保持安全性的同时，提供更有帮助的响应，这对于实际应用中的LMs来说是一个重要的进步。然而，这种方法也面临着计算成本的挑战，尤其是在需要多次迭代才能达到安全响应的情况下。为了解决这个问题，研究者提出了格式化策略，以减少迭代次数并提高效率。
