# LLMGuard: Guarding against Unsafe LLM Behavior

<figure><img src="../.gitbook/assets/image (8) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

随着大型语言模型（LLMs）在企业设置中的兴起，它们在各种NLP任务中展现出卓越的性能，如文本生成、翻译、摘要、问答和情感分析等。LLMs在一定程度上作为通用语言任务解决者，研究范式正转向使用它们。然而，LLMs也常表现出在各种企业设置中不安全的行为，例如文本可能包含机密或个人信息，导致隐私泄露，或者LLMs的回应中存在偏见，引发伦理问题。

### 2. 过去方案和缺点

为了使LLMs与人类偏好一致，已提出各种技术，如基于安全和有用性目标的模型微调（RLHF）。另一种方法专注于标记和纠正不良语言行为。然而，这些技术通常需要不断重新训练，这在许多情况下是不切实际的。

### 3. 本文方案和步骤

本文提出了一个名为LLMGuard的工具，它通过一系列检测器来监控用户与LLM应用的交互，并针对特定行为或对话主题标记内容。LLMGuard通过将每个用户提示和LLM响应通过检测器集合来进行工作。如果检测器检测到不安全文本，系统会向用户发送自动化消息，而不是LLM生成的响应。检测器集合包括种族偏见检测器、暴力检测器、黑名单话题检测器、个人可识别信息（PII）检测器和毒性检测器。

### 4. 本文创新点与贡献

LLMGuard的创新之处在于它提供了一个模块化的框架，可以轻松地添加、修改或移除检测器集合中的检测器。每个检测器都是检测特定不安全行为的专家，并且独立于其他检测器运作。此外，LLMGuard通过后处理直接对LLM输出应用防护措施，确保它们保持在特定参数内。

### 5. 本文实验

实验中，LLMGuard在两个最近的LLMs上进行了演示：FLAN-T5和GPT-2。用户可以选择激活哪些检测器，并提供输入。界面展示了LLM的未过滤响应和启用防护措施后的响应，以及检测器在提示中标记的不安全术语。

### 6. 实验结论

LLMGuard能够有效地标记和预防不良LLM行为，实验结果表明，该工具在不同的检测器上取得了令人满意的准确率和F1分数。

### 7. 全文结论

本文提出了一套可以与任何LLM集成的防护措施，以标记用户与LLM之间的交互，如果检测器检测到不良交互。这些防护措施有助于确保LLMs的使用更加安全和符合伦理标准。

### 阅读总结

本文介绍了LLMGuard，这是一个旨在提高大型语言模型在企业环境中安全性的工具。通过一系列专业的检测器
