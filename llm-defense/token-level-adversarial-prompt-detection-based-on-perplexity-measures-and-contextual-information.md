# TOKEN-LEVEL ADVERSARIAL PROMPT DETECTION BASED ON PERPLEXITY MEASURES AND CONTEXTUAL INFORMATION

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) ( (2).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

近年来，大型语言模型（LLMs）在各种应用中发挥了关键作用，如聊天机器人和自动客户服务系统。然而，LLMs存在一个内在的脆弱性：对抗性提示攻击。攻击者可以精心设计输入字符串，误导LLMs生成不正确或不期望的输出。这种脆弱性不仅损害了LLMs的可用性和可信度，还可能导致恶意利用。

### 2. 过去方案和缺点

以往的研究揭示了基于离散优化的简单但有效的攻击方法，可以生成绕过模型审查和对齐的对抗性提示。这些方法主要依赖于模型输出的困惑度（perplexity）作为检测指标，但通常只提供序列级别的检测，无法精确定位文本中的具体对抗性标记。

### 3. 本文方案和步骤

本文提出了一种新的对抗性提示检测方法，专注于标记级别的分析。该方法基于LLMs预测下一个标记的概率的能力，通过测量模型的困惑度来检测对抗性提示。方法包括两个算法：一个基于优化技术，另一个基于概率图模型（PGM）。这些方法都配备了高效的求解方法，确保了对抗性提示检测的效率。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 4. 本文创新点与贡献

* 提出了一种新的对抗性提示检测方法，可以在标记级别上识别对抗性内容。
* 通过结合相邻标记的信息，提高了检测的可靠性。
* 设计了两种算法：基于优化的方法和基于PGM的方法，都能够有效检测对抗性提示。
* 提供了一种直观的热图可视化方法，帮助用户更好地理解和解释检测结果。

### 5. 本文实验

实验部分详细介绍了实现细节、数据集构建、实验结果和模型依赖性分析。实验结果表明，所提出的方法在序列级别上达到了完美的分类性能，并且在标记级别上也表现出色。

### 6. 实验结论

实验结果证明了所提出方法的有效性。在序列级别上，模型能够可靠地识别包含对抗性提示的序列。在标记级别上，尽管性能不是完美的，但模型仍然能够有效地定位序列中的对抗性标记。

### 7. 全文结论

本文提出了一种新的对抗性提示检测方法，该方法基于语言模型输出的困惑度和邻近标记信息，有效地识别了对抗性内容。这种方法不仅实用，而且即使在较小的模型（如GPT-2）上也表现出色，显著降低了计算需求和硬件要求。

### 阅读总结

本文针对LLMs在对抗性提示攻击面前的脆弱性，提出了一种新的检测方法。该方法通过分析模型输出的困惑度，并结合上下文信息，有效地在标记级别上检测对抗性提示。实验结果表明，这种方法在不同大小的模型上都表现出了良好的性能，尤其是在较小的模型上，这为实际应用提供了一种既高效又实用的解决方案。然而，该方法的有效性依赖于对抗性提示的生成方式，如果攻击者改变生成策略，可能需要对检测方法进行相应的调整。
