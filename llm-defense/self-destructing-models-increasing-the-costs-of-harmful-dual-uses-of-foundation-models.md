# Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

随着大型开源基础模型（Foundation Models, FMs）的兴起，机器学习在许多新问题上的应用变得更加容易。然而，这些模型也存在明显的双重用途风险，即它们可以被用于有益的目的，也可能被恶意行为者用于有害的目的。目前，限制模型访问和出口控制是主要的风险缓解方法。但这些方法在面对开源模型时存在局限性，因为模型参数最终可能被广泛获取。因此，需要新的技术来更精确地控制开源基础模型的下游使用。

### 2. 过去方案和缺点

过去的方案主要分为结构性安全机制和技术性安全机制。结构性机制通过许可证或访问限制来防止有害使用，而技术性策略则通过调整模型以减少在推理时产生有害内容的可能性。然而，这些策略各自存在不足：结构性机制难以防止恶意行为者获取模型参数，技术性策略则可能被通过微调和提示工程绕过。

<figure><img src="../.gitbook/assets/image (5) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种新的方法：自毁模型（Self-Destructing Models），通过任务阻塞（Task Blocking）来训练模型，使其在不牺牲期望任务性能的同时，增加对有害任务的适应成本。具体步骤包括：

* 使用元学习（Meta-Learning）和对抗学习（Adversarial Learning）技术训练自毁模型。
* 提出了一种名为元学习对抗性审查（Meta-Learned Adversarial Censoring, MLAC）的算法。
* 在小规模实验中，展示了MLAC能够防止BERT风格的模型被重新用于性别识别，同时不影响其执行职业分类的能力。

### 4. 本文创新点与贡献

* 提出了自毁模型的概念，这是一种新的技术策略，旨在通过增加对有害任务的适应成本来提高安全性。
* 开发了MLAC算法，这是一种结合元学习和对抗学习的方法，用于训练自毁模型。
* 在实验中展示了MLAC的有效性，证明了自毁模型在保持期望任务性能的同时，能够显著降低在有害任务上的微调性能。

### 5. 本文实验

实验使用了“Bias in Bios”数据集，该数据集包含职业和性别标签。实验的目标是保持职业检测任务的性能，同时阻止性别识别任务。通过MLAC训练的模型在性别识别任务上的表现接近随机初始化模型，而在职业分类任务上的表现优于BERT和随机初始化模型。



任务阻塞（Task Blocking）是一种训练方法，旨在通过特定的训练策略来阻止模型在特定有害任务上的有效适应或微调。这种方法的核心思想是在不损害模型在期望任务上的性能的同时，增加模型在有害任务上的适应成本。以下是任务阻塞训练模型的详细说明：

#### 目标

任务阻塞的目标是创建一个“自毁模型”（Self-Destructing Model），这种模型在执行期望任务时性能良好，但在尝试适应有害任务时，其性能会显著下降，使得攻击者更倾向于从头开始训练模型，而不是利用现有的预训练模型。

#### 方法

1. **定义有害任务**：首先，需要明确哪些任务被视为有害。这通常涉及到对模型可能被滥用的场景进行风险评估，并确定需要阻止的具体任务。
2. **双任务训练**：在训练过程中，模型同时接受期望任务和有害任务的数据。期望任务的数据用于正常训练，而有害任务的数据则用于训练模型在这些任务上的性能下降。
3. **对抗性训练**：在训练过程中，模型会尝试适应有害任务，但同时会有一个对抗性机制（例如，对抗性损失）来惩罚这种适应。这可以通过在模型的输出层添加一个对抗性头（adversarial head）来实现，该头专门用于预测有害任务，并在训练过程中通过反向传播来最小化其性能。
4. **元学习（Meta-Learning）**：为了使模型在有害任务上的适应变得更加困难，可以采用元学习技术。元学习允许模型学习如何快速适应新任务，但在任务阻塞的上下文中，它被用来学习如何避免在有害任务上适应。
5. **优化目标**：在训练过程中，模型的参数被优化以在期望任务上保持高性能，同时在有害任务上保持低性能。这通常涉及到一个平衡过程，其中模型在期望任务上的损失被最小化，而在有害任务上的损失被最大化。

#### 实现

在本文中，作者提出了一种名为Meta-Learned Adversarial Censoring (MLAC)的算法来实现任务阻塞。MLAC通过以下步骤训练自毁模型：

1. 初始化一个对抗性有害任务头和学习率。
2. 在元训练的每个步骤中，从可能的适应方法中随机选择一个，并在有害任务上对特征提取器和对抗性头进行适应。
3. 在外循环中，对抗性参数被训练以最小化适应模型在有害任务上的负对数似然，同时保持期望任务的性能。

#### 结论

任务阻塞通过增加在有害任务上适应模型的成本，提供了一种减少模型被用于有害目的的风险的方法。这种方法在实验中显示出潜力，但仍需要进一步的研究来扩展到更大规模的模型和更复杂的任务。





### 6. 实验结论

实验结果表明，MLAC能够有效地阻止模型在性别识别任务上的微调性能，同时保持在职业分类任务上的高性能。这证明了自毁模型在减少有害双重用途方面的潜力。

### 7. 全文结论

本文提出了自毁模型的概念，并展示了通过MLAC算法训练的模型能够有效地提高对有害任务的适应成本。尽管这是一个初步的研究，但它为如何通过技术策略来提高模型安全性提供了新的视角，并为未来的研究指明了方向。



注意1：

"Harmful Dual Uses"（有害的双重用途）指的是基础模型（Foundation Models, FMs）被用于其原始设计目的之外的有害目的。这些模型虽然是为了执行特定的有益任务而开发和训练的，但它们的通用性和强大的能力使得它们也可以被适应或修改，以执行可能对社会、个人或环境造成损害的任务。例如：

* **性别或种族识别**：在某些情况下，模型可能被用于自动识别个人的性别或种族，这可能侵犯隐私权，或被用于歧视性目的。
* **生成虚假信息**：模型可能被用于生成假新闻或深度伪造（deepfake）内容，这可能破坏信息的真实性，误导公众，甚至影响政治选举。
* **网络攻击**：模型可能被用于改进恶意软件或自动化网络攻击，增加网络犯罪的效率和破坏性。
* **生物武器开发**：在极端情况下，模型可能被用于帮助设计生物武器或化学武器，对人类健康和安全构成威胁。

这些有害用途可能与模型开发者的初衷相违背，因此，研究如何防止或减少这些风险成为了AI安全领域的一个重要课题。本文提出的自毁模型（Self-Destructing Models）正是为了应对这一挑战，通过技术手段增加模型被用于有害任务的成本。









### 阅读总结

本文针对大型基础模型的双重用途风险提出了自毁模型的概念，并开发了MLAC算法来训练这些模型。实验结果支持了自毁模型在提高安全性方面的有效性。这项工作为AI安全领域提供了新的研究方向，尤其是在如何通过技术手段来控制模型的有害用途方面。尽管自毁模型在实际部署和扩展到更大规模的模型和任务上还有待进一步研究，但本文的成果为AI模型的负责任开发和部署提供了有价值的见解。
