# How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?

<figure><img src="../.gitbook/assets/image (150).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

深度学习模型，尤其是预训练语言模型（如BERT），在自然语言处理（NLP）领域取得了显著的成功。然而，这些模型对于对抗性示例（例如，仅使用同义词替换的词替换攻击）非常脆弱。这种脆弱性对现代NLP系统（如垃圾邮件过滤和恶意软件检测）构成了严重的安全挑战。

### 2. 过去方案和缺点

对抗性训练是提高模型鲁棒性的最有效方法之一，它通过使用实时生成的对抗样本来更新模型参数。然而，对抗性训练在预训练语言模型的微调场景中并不适用，因为它会导致灾难性遗忘：模型在微调过程中失去了预训练阶段捕获的通用和鲁棒的语言特征。

<figure><img src="../.gitbook/assets/image (151).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种名为Robust Informative Fine-Tuning (RIFT)的新方法，从信息论的角度出发，通过最大化目标模型输出与预训练模型输出之间的互信息来解决遗忘问题。RIFT在微调过程中鼓励目标模型保留从预训练模型中学到的特征，而不仅仅是使用预训练权重进行初始化。具体步骤如下：

* 使用预训练模型初始化目标模型。
* 在微调过程中，通过最大化互信息来保留预训练模型的特征。
* 使用对抗性样本进行训练，以提高模型对对抗性攻击的鲁棒性。

### 4. 本文创新点与贡献

* 首次研究了在对抗性微调中预训练语言模型的灾难性遗忘现象。
* 提出了RIFT方法，从信息论角度出发，通过最大化互信息来解决遗忘问题，增强模型对对抗性示例的鲁棒性。
* 在情感分析和自然语言推理两个流行的NLP任务上，RIFT在不同攻击下一致性地超越了现有技术。

### 5. 本文实验

实验在IMDB和SNLI数据集上进行，使用BERT和RoBERTa作为预训练模型。攻击算法包括基于种群算法的遗传攻击和基于词显著性的PWWS攻击。实验结果表明，RIFT在对抗性鲁棒性方面优于现有技术。

### 6. 实验结论

RIFT在对抗性攻击下的情感分析和自然语言推理任务中，表现出了更好的鲁棒性。此外，RIFT在保持原始预训练模型性能的同时，提高了模型对对抗性攻击的抵抗力。

### 7. 全文结论

RIFT通过信息论的方法解决了预训练语言模型在对抗性微调中的遗忘问题，提高了模型的鲁棒性，对于构建更可靠的NLP系统具有重要意义。

### 阅读总结

本文针对预训练语言模型在对抗性攻击下的脆弱性问题，提出了一种新的微调方法RIFT。该方法通过最大化目标模型与预训练模型输出之间的互信息，有效地解决了微调过程中的遗忘问题，并在实验中证明了其有效性。RIFT的提出为提高NLP系统的安全性和鲁棒性提供了新的视角和解决方案。
