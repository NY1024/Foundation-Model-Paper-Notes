# Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language

<figure><img src="../.gitbook/assets/image (122).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

在自然语言处理（NLP）领域，预训练语言模型（PLMs）被广泛采用，并可以在多种下游任务中进行微调。然而，微调过程中的安全性无法得到保证，因为微调过程对用户是不可见的。因此，微调后的PLMs容易受到后门攻击和偏见的影响，这些攻击和偏见可能在微调过程中通过数据投毒被恶意或无意识地注入。本文考虑了微调PLMs被疑似攻击者注入后门或偏见的威胁，并提出了一种名为Fine-purifying的方法来净化这些潜在有毒的PLMs。

### 2. 过去方案和缺点

现有的防御方法包括基于鲁棒学习的方法、检测方法、缓解方法和蒸馏方法。然而，这些方法在面对预训练PLMs时的性能并不理想。例如，Fine-mixing方法随机混合初始干净预训练权重和被攻击的微调权重，但这种方法不能精确地缓解后门或偏见。此外，Fine-mixing需要访问初始干净的预训练权重，这在实际中可能难以实现。

<figure><img src="../.gitbook/assets/image (123).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了Fine-purifying方法，该方法利用扩散理论来研究微调的动态过程，以找到潜在有毒的维度。Fine-purifying方法包括两个步骤：（1）净化过程，通过提出的指标检测有毒维度，并将它们重置为干净的预训练权重；（2）微调过程，对净化后的权重在一个小的干净数据集上进行微调。这种方法是首次将扩散理论应用于安全或防御目的。

### 4. 本文创新点与贡献

* 本文首次研究了由扩散理论指导的微调动态，以区分干净和有毒维度。
* 提出了Fine-purifying方法，它保留了干净维度并将有毒维度重置为初始权重。实验结果表明，Fine-purifying在现有防御方法中表现最佳，能够更精确地检测有毒维度。

### 5. 本文实验

实验在四个数据集上进行，包括两个单句分类任务（AgNews和IMDB）和两个句子对分类任务（GLUE中的QQP和QNLI）。实验结果表明，Fine-purifying在不同训练大小和阈值Delta ACC下，始终优于现有防御方法。

### 6. 实验结论

Fine-purifying方法在各种攻击下都显示出了优越的性能，尤其是在后门攻击中。与Fine-mixing和Fine-pruning相比，Fine-purifying在所有Delta ACC下都有更低的ASR（攻击成功率），表明其净化效果更好。

### 7. 全文结论

本文提出了一种新的防御方法Fine-purifying，用于净化可能被注入后门或偏见的预训练语言模型。通过利用扩散理论，Fine-purifying能够精确地检测和净化有毒维度，从而提高了模型的安全性。实验结果证明了该方法的有效性，并在多种攻击下表现出了优越的性能。



注：

### Fine-purifying的工作原理

Fine-purifying方法的核心在于利用扩散理论来检测和净化预训练语言模型（PLMs）中的潜在有毒维度。这些有毒维度可能是由于后门攻击或偏见而在微调过程中被注入的。Fine-purifying的工作流程可以分为以下几个关键步骤：

1. **参数漂移与Hessian矩阵的关系建立**：
   * 扩散理论用于建立参数漂移（即微调过程中的权重更新）与Hessian矩阵（损失函数的二阶偏导数）之间的关系。这种关系有助于识别在微调过程中表现出异常动态的维度。
2. **有毒维度的检测**：
   * 通过计算一个指标`ri`，即参数漂移的平方除以对应维度的Hessian矩阵，来检测有毒维度。有毒维度的Hessian矩阵与干净数据集上的Hessian矩阵有显著差异，导致`ri`值异常。
3. **净化过程**：
   * 一旦检测到有毒维度，Fine-purifying方法会将这些维度的权重重置为初始的干净预训练权重。这样做可以移除在微调过程中可能被注入的后门或偏见。
4. **微调过程**：
   * 在净化过程之后，对重置后的权重进行微调，以确保模型在干净数据集上的性能。这一步骤有助于模型恢复其在干净数据上的预测能力。

### 为什么Fine-purifying有效

Fine-purifying方法之所以有效，主要基于以下几点：

1. **扩散理论的应用**：
   * 扩散理论提供了一种分析学习动态的数学框架，使得研究者能够理解和预测参数在训练过程中的变化。通过这种理论，Fine-purifying能够识别出在微调过程中表现出异常行为的维度，这些维度很可能是被攻击者操纵的有毒维度。
2. **精确的有毒维度检测**：
   * 通过`ri`指标，Fine-purifying能够精确地检测出有毒维度。这种方法比简单的随机混合或删除维度更为精确，因为它基于模型内部的数学关系来识别问题维度。
3. **重置有毒维度**：
   * 将有毒维度重置为初始的干净权重是一种直接且有效的净化手段。这种方法可以确保这些维度不再包含任何由攻击者注入的信息。
4. **保留干净维度**：
   * 在净化过程中，Fine-purifying保留了那些没有被检测为有毒的维度。这样可以最大限度地减少对模型性能的影响，同时确保模型在干净数据上的表现。
5. **实验验证**：
   * 实验结果表明，Fine-purifying在多种攻击场景下都能有效地提高模型的安全性，并且在性能上优于现有的防御方法。这进一步证明了Fine-purifying方法的有效性。

综上所述，Fine-purifying方法通过结合扩散理论和精确的维度检测，提供了一种有效的策略来净化预训练语言模型，使其免受后门攻击和偏见的影响。





### 阅读总结

本文针对预训练语言模型在微调过程中可能受到的后门攻击和偏见问题，提出了一种新的防御策略Fine-purifying。该策略通过扩散理论来分析微调过程中的动态变化，从而精确地识别和净化潜在的有毒维度。实验结果表明，Fine-purifying在多种攻击场景下都能有效地提高模型的安全性，且在性能上优于现有的防御方法。这一研究为NLP领域的模型安全性提供了新的视角和解决方案。
