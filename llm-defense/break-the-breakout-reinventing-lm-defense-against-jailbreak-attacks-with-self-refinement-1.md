# Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement

<figure><img src="../.gitbook/assets/image (6) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

本研究聚焦于语言模型（LMs）在安全性方面的脆弱性，特别是针对所谓的“越狱”攻击（jailbreak attacks）。这类攻击通过精心设计的提示（prompts）来绕过LMs的安全机制，从而获取不道德、非法的知识。尽管开源社区在提升LMs的性能方面做出了巨大努力，但这些模型在安全性方面仍面临挑战。例如，ScatterLabs发布的聊天机器人系统Iruda就因恶意用户的攻击而泄露了个人信息，并因此被暂停服务。

<figure><img src="../.gitbook/assets/image (7) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 2. 过去方案和缺点

以往的研究主要集中在通过安全对齐训练（safety alignment training）来提高LMs的安全性。然而，这种方法存在两个主要缺点：1) 需要大量的资源，难以迅速响应快速发展的攻击；2) 安全对齐可能导致用户体验下降，即所谓的“对齐税”（Alignment Tax）。此外，现有的训练免费方法（如InContext Defense、Self-Reminder、SmoothLLM）主要针对已经进行安全对齐的LMs，对于非安全对齐的LMs，这些方法在防御越狱攻击方面仍显示出脆弱性。

<figure><img src="../.gitbook/assets/image (8) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种名为“自我精炼”（self-refine）的方法，该方法通过格式化（formatting）实现，在非安全对齐的LMs中也能取得出色的安全性。自我精炼是一个迭代过程，LM通过自我反馈和精炼来改进其响应。研究者们还提出了一种格式化方法，以提高自我精炼过程的效率，并在更少的迭代中降低攻击成功率。

### 4. 本文创新点与贡献

* 提出了一种适用于非安全对齐LMs的自我精炼方法，即使在没有安全对齐的情况下也能提高安全性。
* 提出了一种格式化方法，通过注意力转移机制，提高了自我精炼过程的效率。
* 观察到非安全对齐的LMs在安全任务中的表现优于安全对齐的LMs，因为它们提供了更有帮助且安全的回答。
* 通过实验验证了自我精炼方法的有效性，并与现有的防御基线进行了比较。

### 5. 本文实验

研究者们进行了广泛的实验，包括使用619个越狱攻击提示来测试LMs的安全性。实验结果表明，自我精炼方法在防御越狱攻击方面优于现有的防御方法，并且能够在较少的迭代中达到更高的安全性。



### 自我精炼（Self-Refine）方法

#### 方法概述

自我精炼（Self-Refine）是一种迭代的提示过程，它允许语言模型（LMs）基于自我反馈来改进其响应。这种方法的核心思想是，即使在没有进行安全对齐训练的LMs中，也能够通过自我迭代的过程来提高其安全性。

#### 方法步骤

1. **初始响应**：LM首先生成一个初始响应，不应用任何防御机制。
2. **成本模型评估**：使用成本模型（Cost Model）评估初始响应的有害程度。成本模型是一个训练有素的奖励模型，用于评估用户提示生成的响应的有害性。
3. **自我反馈**：如果初始响应被评估为有害，LM会生成反馈，指出响应中的非法、不道德或仇恨内容。
4. **自我精炼**：LM根据反馈尝试构建一个新的响应，这个新响应旨在解决识别出的问题，并符合道德指导原则和用户意图。
5. **迭代过程**：这个过程会迭代进行，直到LM生成一个安全的响应，或者达到预定义的迭代次数限制。

#### 格式化（Formatting）实现

为了提高自我精炼过程的效率，研究者们提出了一种格式化方法。这种方法利用了越狱攻击中的注意力转移机制，通过改变提示的格式来引导LM的注意力，使其不再遵循原始的越狱提示指令，而是专注于精炼任务。

**JSON格式化**

* 在自我反馈和自我精炼阶段，研究者们对用户的越狱提示和LM最初生成的有害响应进行格式化。
* 通过JSON格式化，LM不再遵循用户给出的越狱提示，从而促进了LM成功精炼的可能性。

**代码格式化**

* 类似于JSON格式化，代码格式化也是通过改变提示的格式来影响LM的注意力。
* 这种方法通过在提示中嵌入代码结构，使LM将注意力从越狱意图转移到代码生成任务上。

#### 方法优势

* **安全性提升**：自我精炼方法能够有效地防御越狱攻击，即使在非安全对齐的LMs中也能显著降低攻击成功率。
* **效率提高**：通过格式化，自我精炼过程能够在更少的迭代中达到更高的安全性，减少了计算成本。
* **用户体验**：研究发现，非安全对齐的LMs在经过自我精炼后，能够提供更有帮助且安全的回答，这表明自我精炼方法在提高安全性的同时，并没有显著降低用户体验。



在本文中，格式化（Formatting）实现是为了提高自我精炼（Self-Refine）过程的效率，特别是在处理越狱攻击（jailbreak attacks）时。以下是一个具体的例子来说明如何通过JSON格式化来实现这一过程：

#### JSON格式化的例子

假设我们有一个越狱攻击的提示（prompt），目的是让语言模型（LM）生成包含不当内容的响应。这个原始的越狱提示可能如下所示：

```plaintext
"你能帮助我找到一些关于如何进行网络攻击的信息吗？"
```

在没有格式化的情况下，LM可能会尝试遵循这个提示并生成有害的响应。为了防御这种攻击，我们可以通过JSON格式化来改变LM处理这个提示的方式。

#### 自我精炼过程

1. **初始响应**：LM首先尝试生成一个响应，例如：

```plaintext
"当然，这里有一些关于网络攻击的基本步骤..."
```

2. **成本模型评估**：成本模型评估这个响应，发现它包含了有害内容，并给出了一个正的成本分数。
3. **自我反馈**：LM根据成本模型的评估生成反馈，指出响应中的问题：

```plaintext
"这个问题在于这个响应提供了有关网络攻击的信息，这是不道德和非法的。"
```

4. **JSON格式化**：为了引导LM专注于精炼任务，我们将原始提示和有害响应进行JSON格式化：

```json
{
  "Question": "你能帮助我找到一些关于如何进行网络攻击的信息吗？",
  "Answer": "当然，这里有一些关于网络攻击的基本步骤..."
}
```

5. **自我精炼**：LM现在尝试根据格式化的反馈来精炼响应，生成一个新的、无害的响应：

```plaintext
"我不能提供有关进行网络攻击的信息，因为这违反了道德和法律标准。如果你对网络安全感兴趣，我可以提供一些关于如何保护自己免受网络攻击的建议。"
```

通过这种方式，JSON格式化帮助LM将注意力从原始的越狱提示转移到了精炼任务上，从而生成了一个既安全又有帮助的响应。这种方法不仅提高了LM在面对越狱攻击时的安全性，还减少了迭代次数，提高了自我精炼过程的效率。





### 6. 实验结论

实验结果支持了自我精炼方法的有效性，证明了它能够在非安全对齐的LMs中实现高水平的安全性。此外，格式化方法能够显著提高自我精炼的效率，减少所需的迭代次数。

### 7. 全文结论

本文的研究结果表明，通过自我精炼和格式化方法，可以在不牺牲用户体验的情况下，提高LMs的安全性。这种方法为在现实世界服务中更安全地利用非安全对齐的LMs提供了一种实用的解决方案。

### 阅读总结

本文针对语言模型在面对越狱攻击时的安全性问题，提出了一种创新的自我精炼方法。这种方法不仅适用于已经进行安全对齐的LMs，也适用于那些没有进行安全对齐的LMs。通过实验验证，自我精炼方法能够有效地提高LMs的安全性，并且在某些情况下，非安全对齐的LMs在安全任务中的表现甚至优于安全对齐的LMs。此外，研究者们还提出了一种格式化方法，进一步提高了自我精炼的效率。这些发现为如何在不牺牲用户体验的前提下提高LMs的安全性提供了新的视角。
