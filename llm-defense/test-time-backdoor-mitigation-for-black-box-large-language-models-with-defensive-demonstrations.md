# Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 研究背景

大型语言模型（LLMs）在多种自然语言处理（NLP）任务中取得了显著进展，但它们也容易受到后门攻击的威胁。这些攻击通过在训练数据中植入触发特征并将其与恶意输出关联起来，扭曲了模型的预测结果。现有研究主要集中在训练阶段的后门防御，忽视了测试时防御的重要性，特别是在LLMs作为Web服务部署的情况下，通常只提供黑盒访问，使得训练时防御变得不切实际。

### 过去方案和缺点

以往的后门防御研究主要关注于训练时的防御，例如通过仔细过滤可疑的训练数据或在训练过程中识别输入触发器。然而，这些方法在测试时防御中应用有限，因为黑盒模型的访问限制和缺乏对模型内部机制的了解。此外，测试时防御面临的挑战更大，因为后门攻击的形式和水平不断演变，包括单个令牌、触发句子、指令和甚至句法结构，这增加了开发通用解决方案的难度。

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 本文方案和步骤

本文提出了一种名为“防御性示范”（defensive demonstrations）的测试时后门防御策略，适用于黑盒大型语言模型。该方法涉及识别任务并从未受污染的池中检索与任务相关的示范，然后将这些示范与用户查询结合，在测试时呈现给模型，无需对黑盒模型进行任何修改或调整，也无需了解其内部机制。防御性示范旨在对抗触发器的不利影响，旨在在测试时评估中校准和纠正被污染模型的行为。



本方案的防御流程可以概括为以下几个步骤：

1. **任务识别**：首先，防御者需要识别出当前测试查询的任务类型。这是因为防御性示范的有效性依赖于能够准确地找到与任务相关的清洁示范数据。
2. **示范数据源选择**：选择一个未受污染的清洁数据集作为示范数据的来源。这个数据集应该没有经过任何恶意操作，能够准确反映每个实例的真实意图。
3. **示范数据检索**：根据识别出的任务，从未受污染的数据集中检索与任务相关的示范数据。这些示范数据可以是随机选择的、与测试实例语义上相似的，或者是通过自我推理生成的。
4. **示范数据与查询结合**：将检索到的示范数据与用户的测试查询结合起来。这一步骤是在测试时进行的，即在模型进行预测之前，将示范数据和查询一起呈现给模型。
5. **模型评估**：将结合了示范数据的查询传递给模型，模型根据这些上下文信息进行评估，并生成预测结果。
6. **结果输出**：模型的输出（预测结果）被传递回给用户。如果防御成功，模型的预测应该能够忽略或减轻恶意触发器的影响，从而提供更准确的结果。
7. **防御效果评估**：通过比较防御前后模型的表现（例如，攻击成功率和清洁标签准确率），评估防御性示范的效果。

这个流程的关键在于利用LLMs的上下文学习能力，通过提供与任务相关的清洁示范数据来指导模型正确理解和响应测试查询，即使查询中可能包含恶意触发器。通过这种方式，防御性示范能够在不修改模型本身的情况下，有效地减轻后门攻击的负面影响。





### 本文创新点与贡献

1. 提出了一种新的测试时后门防御策略，特别适用于作为Web服务部署的黑盒LLMs。
2. 通过使用未受污染的示范数据，展示了如何利用LLMs的上下文学习能力来纠正被污染模型的行为。
3. 研究了两种关键问题：防御性示范机制在纠正模型行为方面的有效性，以及检索最有效示范以减轻触发器影响的方法。
4. 在不同的数据集上实施防御性示范，并证明了该方法在防御各种后门攻击方面的显著有效性。

### 本文实验

实验在三个常用的后门攻击数据集上进行评估：SST-2、Tweet Emotion和TREC COARSE。使用7B Llama2模型作为评估的LLM backbone。实验结果显示，防御性示范有效地降低了攻击成功率（ASR），并保持了较高的清洁标签准确率（CACC）。此外，引入理由的示范（self-reasoning demonstrations）显著提高了防御性能。

### 实验结论

防御性示范是一种有效的测试时后门防御方法，能够显著降低被污染模型的攻击成功率，同时保持较高的清洁数据准确率。特别是，包含自我推理的示范在大多数情况下都优于传统的基线方法。

### 全文结论

本文介绍的防御性示范是一种针对测试时后门攻击的创新防御策略，它利用LLMs的上下文学习能力来减轻模型中潜在后门的影响。通过广泛的实验，我们证明了防御性示范在对抗从实例级到指令级的后门攻击方面的显著有效性。包含推理的示范在大多数情况下都显著优于传统的基线方法，提供了一种实用且高效的解决方案，以减轻LLMs中的后门漏洞。

### 阅读总结报告

本篇论文提出了一种新的测试时后门防御方法——防御性示范，该方法专为黑盒大型语言模型设计。与传统的基于训练时的防御方法不同，防御性示范利用模型的上下文学习能力，通过结合用户查询和未受污染的示范数据来纠正模型的行为。实验结果表明，该方法在多个数据集上对多种后门攻击都显示出了显著的防御效果，尤其是当示范中包含自我推理时。这种方法为测试时后门防御提供了一种新的视角，并为未来在多样化和资源受限的环境中增强防御性示范的适应性和效率提供了方向。
