# LLMSelf Defense: By Self Examination, LLMsKnowTheyAreBeing Tricked

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 大型语言模型（LLMs）在文本生成方面表现出色，但它们也可能生成有害内容，即使在通过强化学习与人类价值观对齐后。对抗性提示可以绕过LLMs的安全措施，导致生成有害内容。这些攻击包括强制诱导肯定回答和提示工程攻击。LLMs的训练数据包含有毒内容，这与它们生成高质量响应的能力相冲突，因为它们被训练来生成连贯的文本序列。

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 过去方案和缺点： 以往的防御方法主要依赖于迭代生成或预处理，这些方法复杂且开销大，可能会限制它们的可用性和普遍性。例如，迭代自回归推理虽然可以提高准确性，但会导致语言模型的生成时间增加300%。此外，现有的防御方法可能需要额外的数据、预处理或训练，这增加了部署的复杂性。

<figure><img src="../.gitbook/assets/image (5) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤： 本文提出了LLM SELF DEFENSE，这是一种简单的零次防御方法，用于防止用户接触到LLMs诱导的有害或恶意内容。该方法不需要对底层模型进行任何修改，也不需要迭代输出生成。LLM SELF DEFENSE通过将生成的内容嵌入预定义的提示中，并使用另一个LLM实例来分析文本并预测其是否有害。这种方法利用了LLMs对“有害”一词的理解，通过零次分类器来过滤潜在的有害响应。
2. 本文实验和性能： LLM SELF DEFENSE在GPT 3.5和Llama 2两个当前最突出的LLMs上进行了测试，针对各种类型的攻击，如强制诱导肯定回答和提示工程攻击。实验结果表明，LLM SELF DEFENSE在两种模型上都能有效降低攻击成功率，几乎将攻击成功率降低到0。特别是，当LLMs在处理文本后被要求检测伤害时，它们在识别有害内容方面表现得更好。

阅读总结报告： 本文提出了LLM SELF DEFENSE，这是一种创新的防御机制，用于保护用户免受LLMs可能产生的有害内容的影响。这种方法利用了LLMs的零次学习能力，通过另一个LLM实例来检测生成的文本是否有害，而无需对原始模型进行任何修改。实验结果表明，LLM SELF DEFENSE在降低攻击成功率方面非常有效，且与现有更复杂的防御方法相比，具有更快和更高效的优势。这种方法的简单性和有效性使其在实际应用中具有很高的潜力，尤其是在需要快速部署和广泛适用性的场景中。未来的工作可以探索如何通过具体示例和上下文学习来进一步提高LLM SELF DEFENSE的性能。
