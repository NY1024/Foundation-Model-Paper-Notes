# Detoxifying Large Language Models via Knowledge Editing

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

随着大型语言模型（LLMs）如ChatGPT、LLaMA和Mistral的发展，人们越来越关注它们处理有害查询的潜力，这强调了需要谨慎的安全防护措施。目前采用的方法，如监督式微调（SFT）、基于人类反馈的强化学习（RLHF）和直接偏好优化（DPO），虽然显著提高了LLMs的安全性，使它们能够拒绝像“我可以在哪里出售偷来的艺术品？”这样的有害查询，但这些方法可能仍然容易受到精心设计的攻击提示的影响。

### 2. 过去方案和缺点

以往的方法，如DPO，主要通过压制有毒参数的激活来防止LLMs生成有害内容，但并没有从根本上消除模型中的有毒区域，因此仍然容易受到新型恶意输入的攻击。此外，现有的数据集和评估指标主要关注于对现有恶意输入的防御成功率，忽视了对各种恶意输入的泛化防御能力。

### 3. 本文方案和步骤

文章提出了一种新的基准测试集SafeEdit，涵盖了九种不安全类别和多种强大的攻击模板，并扩展了评估指标以包括防御成功率、防御泛化能力和一般性能。提出了一种简单而有效的知识编辑基线方法——手术中神经监测下的解毒（DINM），通过仅使用一个实例进行几次调整，就可以减少LLMs的毒性。

### 4. 本文创新点与贡献

* 构建了全面的SafeEdit基准测试集，用于评估通过知识编辑进行解毒的任务。
* 提出了DINM方法，它通过定位LLMs中的有毒区域并进行直接编辑，从而减少了模型的毒性。
* 对比了多种知识编辑方法，发现知识编辑有潜力在对一般性能影响有限的情况下有效解毒LLMs。
* 对不同解毒方法的内部机制进行了深入分析，揭示了DINM与现有方法在处理有毒参数方面的不同。

### 5. 本文实验

实验部分详细介绍了SafeEdit基准测试集的构建过程，包括有害问题的定义、攻击提示的收集、响应生成和一般知识的收集。同时，提出了评估解毒性能的指标，包括防御成功率（DS）、防御泛化（DG）和一般性能（如流畅性和知识问答任务性能）。

### 6. 实验结论

实验结果表明，DINM在解毒性能上表现出色，特别是在泛化防御各种恶意输入方面。与其他基线方法相比，DINM在LLaMA2-7B-Chat和Mistral-7B-v0.1上都取得了显著的性能提升，同时对一般性能的影响相对较小。

### 7. 全文结论

本文通过构建SafeEdit基准测试集和提出DINM方法，展示了知识编辑在解毒LLMs方面的潜力。DINM方法能够有效地减少LLMs的毒性，同时保持对一般查询的高性能响应。这些发现为未来开发解毒方法和理解LLMs的潜在知识机制提供了新的见解。



注1：

知识编辑是一种针对大型语言模型（LLMs）的技术，它允许研究人员或开发者在不重新训练整个模型的情况下，对模型的特定行为或知识进行精确的修改。这种方法特别关注于模型中存储的特定信息或参数，并对其进行调整，以纠正错误、更新过时的知识或减少有害内容的生成。

在本文中，知识编辑被用来“解毒”LLMs，即减少或消除模型生成有害内容（如歧视性、攻击性或违法信息）的能力。这是通过以下步骤实现的：

1. **定位有毒区域**：首先确定LLM中负责生成有毒内容的参数或“有毒区域”。这通常涉及到分析模型的隐藏层状态，以找出在生成不安全响应时表现出显著不同的部分。
2. **编辑参数**：一旦确定了有毒区域，就通过对这些特定参数进行调整来修改模型的行为。这可能涉及到改变权重或激活函数，以减少或消除模型对有害输入的响应。
3. **保持一般性能**：在编辑过程中，需要确保模型对正常输入的响应能力不受影响，即保持模型在其他任务上的性能。

本文提出的DINM（Detoxifying with Intraoperative Neural Monitoring）方法是一种知识编辑的实现，它通过定位和编辑LLM中的有毒区域来减少模型的毒性，同时尽量不影响模型的其他功能。这种方法的关键在于它能够通过少量的调整步骤，使用单个实例来实现对模型的快速编辑，从而提高了效率并减少了对模型整体性能的影响。



注2：\
在本文中，定位LLMs中的有毒区域并进行直接编辑的过程是通过提出的DINM（Detoxifying with Intraoperative Neural Monitoring）方法实现的。以下是该过程的详细步骤：

#### 1. 定位有毒区域（Toxic Regions Location）

* **语义差异分析**：首先，通过比较安全响应（Ysafe）和不安全响应（Yunsafe）的隐藏状态，来确定哪个Transformer层在区分这两种响应方面最为有效。这可以通过计算各层隐藏状态之间的欧氏距离来实现。
* **选择有毒层**：计算每一层的隐藏状态对于安全和不安全序列的分布差异，并选择差异最大的层作为有毒层（ℓtoxic）。这一层的参数被认为是控制生成有毒内容的关键。

#### 2. 直接编辑参数（Detoxifying Editor）

* **选择编辑目标**：在有毒层中，选择特定的参数进行编辑，通常是权重矩阵（WV ℓtoxic），这些参数与输入数据的交互导致了有毒内容的生成。
* **编辑过程**：使用一个输入-输出对（即攻击性输入X和安全响应Ysafe），通过一系列调整步骤（T步骤）来编辑模型参数。在这个过程中，只有有毒区域的参数是可调的，而其他参数保持不变。
* **损失函数**：定义一个损失函数（Ltotal），它包含两部分：一是增加生成安全内容的概率（Le），二是保持对合理用户请求的正常响应（Lc）。通过平衡这两部分（使用cedit系数），来指导参数的调整。
* **反向传播**：使用Ltotal进行反向传播，更新有毒区域的参数。这个过程类似于标准的神经网络训练，但是只针对特定的有毒区域进行。

#### 3. 实验和评估

* **效果评估**：通过一系列的实验来评估编辑后模型的性能，包括防御成功率（DS）、防御泛化能力（DG）以及一般性能（如流畅性、知识问答（KQA）和内容摘要（CSum）任务）。
* **机制分析**：进一步分析编辑前后模型的内部机制，比如通过训练一个有毒探针模型来量化编辑前后参数的毒性水平，以及计算信息流向有毒区域的变化。

通过这种方法，DINM能够在不重新训练整个模型的情况下，精确地定位并编辑LLMs中的有毒区域，从而减少模型生成有害内容的可能性，同时尽量保持模型在其他任务上的性能。





### 阅读总结

本文针对大型语言模型在处理有害内容时的安全性问题，提出了一种新的知识编辑方法——DINM，以及一个全面的评估基准SafeEdit。通过实验验证，DINM能够有效地减少模型的毒性，同时保持对正常输入的高性能响应。这项工作不仅为LLMs的安全性研究提供了新的工具和方法，也为未来在这一领域的研究提供了新的方向。
