# Defending Jailbreak Prompts via In-Context Adversarial Game

<figure><img src="../.gitbook/assets/image (7) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

大型语言模型（LLMs）在多种应用中展现出卓越能力，但它们的安全性问题，尤其是越狱攻击（jailbreak attacks）的威胁，仍然是一个重要的关注点。越狱攻击通过添加特定设计的提示（prompt）到输入数据中，诱使LLMs生成可能包含有害或恶意内容的响应。这些攻击源于LLMs训练过程中使用的安全约束目标之间的冲突。因此，如何有效地防御这些攻击，提高LLMs的安全性，成为一个亟待解决的问题。

### 2. 过去方案和缺点

以往的防御策略包括内容过滤、提示编辑、微调（fine-tuning）和安全指令的实施。这些方法在实现防御目标时面临不同的挑战。例如，过滤机制可能导致对良性查询的过度防御，而提示编辑可能影响对非恶意输入的响应准确性。微调旨在通过将成功的越狱提示与拒绝响应相关联来提高模型对齐，但缺乏有效的攻击策略，限制了其防御效果。安全指令的整合虽然可以在一定程度上提高模型对齐，但这些方法要么缺乏训练组件，要么依赖于静态数据集来制定安全指令，无法全面覆盖对越狱提示的防御。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种名为In-Context Adversarial Game (ICAG)的方法，通过利用代理学习（agent learning）进行对抗游戏，动态扩展知识以防御越狱攻击，而无需对模型进行微调。ICAG使用迭代过程来增强防御和攻击代理。这种方法与传统方法不同，它通过持续的对抗游戏动态扩展对新生成的越狱提示的防御能力。



In-Context Adversarial Game (ICAG) 是一种用于防御大型语言模型（LLMs）越狱攻击的方法。该方法的核心思想是通过代理学习（agent learning）来进行对抗游戏，动态地增强模型对越狱攻击的防御能力，而无需对模型进行微调。下面是ICAG方法的详细说明：

#### 1. 代理学习与对抗游戏

ICAG方法引入了两个代理：攻击代理（Attack Agent）和防御代理（Defense Agent）。这两个代理通过与目标LLM的交互来学习和进化，从而提高它们在攻击和防御方面的能力。

#### 2. 攻击代理

攻击代理的目标是生成能够绕过LLM安全防护的越狱提示。它通过以下步骤工作：

* **洞察提取（Insight Extraction）**：攻击代理分析被LLM拒绝的越狱提示，通过比较成功的越狱提示来提取有用的洞察，这些洞察有助于理解为何某些提示能够成功绕过防御。
* **越狱提示的优化（Refinement of Jailbreak Prompts）**：利用提取的洞察，攻击代理对失败的越狱提示进行优化，生成新的越狱提示，以提高其绕过当前防御机制的成功率。

#### 3. 防御代理

防御代理的目标是加强LLM的防御能力，以抵御越狱攻击。它的工作流程包括：

* **反思（Reflection）**：防御代理识别导致有害输出的提示，并通过反思过程生成自我反思，以了解为何这些提示能够成功绕过防御。
* **洞察提取**：与攻击代理类似，防御代理也进行洞察提取，但重点是识别反思如何促进成功的防御，并从这些洞察中提取规则来强化防御。

#### 4. 迭代过程

ICAG方法通过迭代过程不断优化攻击和防御策略：

1. 将一组手动创建的越狱提示输入到目标LLM中。
2. 使用评估器（如GPT-4）分析结果，区分成功和失败的越狱尝试。
3. 将失败和成功的越狱提示转发给攻击代理，以优化失败的提示并提取洞察。
4. 将经过优化的越狱提示和成功的尝试提供给防御代理，以创建针对性的安全指令。
5. 这些安全指令随后作为目标LLM的系统提示，在后续迭代中使用，不断细化对抗游戏。

#### 5. 实验评估

ICAG方法通过一系列实验进行了评估，这些实验使用了不同的数据集和LLMs。实验结果表明，ICAG能够有效地降低越狱攻击的成功率，并且在不同的LLMs之间展现出良好的转移性。

#### 6. 局限性与未来工作

ICAG方法依赖于相对静态的对手模型，可能在攻击者以更复杂方式不断调整策略的场景中受限。此外，ICAG的成功依赖于初始提示集的质量和多样性。未来的工作可以探索更可扩展的策略，扩展到多模态上下文，并提高对抗游戏对动态威胁环境的适应性。





### 4. 本文创新点与贡献

* 首次提出在上下文中为LLMs设计对抗游戏的方法，旨在动态加强攻击和防御能力，而无需微调。
* 首次将代理学习应用于越狱攻击领域，自动探索LLMs在攻击和防御方面的知识。
* 通过对四种不同的LLMs进行全面测试，证明了所提出方法的有效性及其在不同模型之间的防御能力转移性。

### 5. 本文实验

实验使用了三个数据集来评估ICAG的性能，并与其他基线方法进行比较。实验结果表明，ICAG在各种攻击场景中显著降低了越狱成功率，并且表现出卓越的跨模型转移性。

### 6. 实验结论

实验结果表明，ICAG能够有效地防御越狱攻击，且在多种LLMs上都表现出良好的效果。ICAG的防御策略在不同模型之间具有很好的转移性，表明其作为一种通用防御机制的潜力。

### 7. 全文结论

本文通过引入攻击代理和防御代理，利用代理学习的概念，提出了一种新的防御越狱攻击的方法。这种方法通过动态对抗游戏，增强了攻击和防御能力。ICAG不仅在提高LLMs安全性方面取得了显著进展，而且其防御策略在不同模型之间具有很好的转移性。尽管存在一些局限性，如对静态对手模型的依赖，以及对初始提示集质量和多样性的依赖，但ICAG为未来LLMs安全性研究提供了新的视角和方法。

### 阅读总结

本文针对LLMs面临的越狱攻击问题，提出了一种新颖的防御方法In-Context Adversarial Game (ICAG)。ICAG通过代理学习进行动态对抗游戏，无需对模型进行微调即可增强对越狱攻击的防御能力。实验结果证明了ICAG的有效性和跨模型的防御能力转移性，为LLMs安全性研究提供了新的思路。尽管存在局限性，ICAG的提出为未来在更动态和复杂的威胁环境中提高LLMs安全性提供了可能。
