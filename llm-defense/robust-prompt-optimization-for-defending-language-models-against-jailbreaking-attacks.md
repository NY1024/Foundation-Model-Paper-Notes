# Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks

<figure><img src="../.gitbook/assets/image (12) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

本研究的背景是现代大型语言模型（LLMs）在面对对抗性攻击或“越狱”（jailbreaking）时的脆弱性。越狱攻击是指攻击者通过修改输入提示（prompts）来诱导模型产生有害行为。尽管已有一些防御措施被提出，但它们通常只针对狭窄的威胁模型，并且无法提供一个强大、通用且实用的防御。为了解决这个问题，研究者提出了一种新的对抗性目标，用于保护语言模型免受越狱攻击，并开发了一种名为鲁棒提示优化（Robust Prompt Optimization, RPO）的算法。

### 2. 过去方案和缺点

过去的防御机制包括输入过滤器、输入平滑和少量示例等。这些方法通常缺乏有效性，无法泛化到多种越狱攻击，或者会增加额外的推理成本，因此没有达到强大且实用的防御标准。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出的RPO算法使用基于梯度的标记优化来强制无害输出。RPO通过在输入级别间接修改基础模型来提高鲁棒性。算法包括两个主要步骤：（1）选择一个最坏情况的越狱来修改提示；（2）优化后缀以保持无害行为。RPO在优化过程中适应最坏情况的自适应修改，可以生成对各种攻击（包括未见过的攻击）都具有鲁棒性的防御后缀或触发标记。

### 4. 本文创新点与贡献

* 提出了第一个针对越狱攻击的防御目标。
* 正式化了确保在最坏情况下无害输出的防御和联合最小最大目标。
* 提出了RPO算法，可以直接解决防御目标，结合了原则性攻击选择和离散优化。
* RPO防御在手动和基于梯度的越狱攻击中实现了最先进的有效性和通用性，并且对良性使用的影响和成本可以忽略不计。

### 5. 本文实验

实验在Starling-7B模型上进行，使用了AdvBench数据集中的有害指令。RPO在多种已知和未知的越狱攻击上进行了优化和评估。实验结果表明，RPO在已知越狱攻击上的攻击成功率（ASR）从84%降低到8.66%，并且在未知越狱攻击上也表现出色。

### 6. 实验结论

RPO在多种越狱攻击上都显示出了显著的鲁棒性，即使在适应性攻击下也能保持有效。此外，RPO后缀在黑盒模型GPT-4上也能转移，显著降低了最强攻击的成功率。

### 7. 全文结论

RPO为语言模型提供了一种新的、有效的防御机制，可以抵御已知和未知的越狱攻击。尽管RPO在当前的模型上表现出色，但随着攻击技术的进步，可能需要进一步的改进。研究者希望这项工作能够激励未来的AI安全研究，确保强大的模型在所有环境中都是可靠的。

### 阅读总结

本文提出了一种新的防御机制RPO，用于保护大型语言模型免受越狱攻击。通过在输入级别进行优化，RPO能够生成鲁棒的后缀，即使在面对适应性攻击时也能保持有效。实验结果表明，RPO在多种攻击场景下都能显著降低攻击成功率，且对正常使用的影响极小。这项工作为提高语言模型的安全性和鲁棒性提供了有价值的贡献，并为未来的研究提供了新的方向。
