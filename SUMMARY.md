# Table of contents

* [MM-LLM](README.md)
  * [MM-LLMs: Recent Advances in MultiModal Large Language Models](mm-llm/mm-llms-recent-advances-in-multimodal-large-language-models.md)
  * [Multimodal datasets: misogyny, pornography, and  malignant stereotypes](mm-llm/multimodal-datasets-misogyny-pornography-and-malignant-stereotypes.md)
* [LLM-Attack](llm-attack/README.md)
  * [SHADOW ALIGNMENT: THE EASE OF SUBVERTING  SAFELY-ALIGNED LANGUAGE MODELS](llm-attack/shadow-alignment-the-ease-of-subverting-safely-aligned-language-models.md)
  * [Defending LLMs against Jailbreaking Attacks via Backtranslation](llm-attack/defending-llms-against-jailbreaking-attacks-via-backtranslation.md)
  * [EMULATED DISALIGNMENT: SAFETY ALIGNMENT  FOR LARGE LANGUAGE MODELS MAY BACKFIRE!](llm-attack/emulated-disalignment-safety-alignment-for-large-language-models-may-backfire.md)
  * [ON THE SAFETY OF OPEN-SOURCED LARGE LAN GUAGE MODELS: DOES ALIGNMENT REALLY PREVENT  THEM FROM BEING](llm-attack/on-the-safety-of-open-sourced-large-lan-guage-models-does-alignment-really-prevent-them-from-being.md)
  * [Unveiling the Implicit Toxicity in Large Language Models](llm-attack/unveiling-the-implicit-toxicity-in-large-language-models.md)
  * [Forcing Generative Models to Degenerate Ones:  The Power of Data Poisoning Attacks](forcing-generative-models-to-degenerate-ones-the-power-of-data-poisoning-attacks.md)
  * [Make Them Spill the Beans!  Coercive Knowledge Extraction from (Production) LLMs](llm-attack/make-them-spill-the-beans-coercive-knowledge-extraction-from-production-llms.md)
  * [Learning to Poison Large Language Models During Instruction Tuning](llm-attack/learning-to-poison-large-language-models-during-instruction-tuning.md)
  * [ALIGNMENT IS NOT SUFFICIENT TO PREVENT LARGE  LANGUAGE MODELS FROM GENERATING HARMFUL IN FORMATION:](alignment-is-not-sufficient-to-prevent-large-language-models-from-generating-harmful-in-formation.md)
  * [LANGUAGE MODEL UNALIGNMENT: PARAMETRIC  RED-TEAMING TO EXPOSE HIDDEN HARMS AND BI ASES](llm-attack/language-model-unalignment-parametric-red-teaming-to-expose-hidden-harms-and-bi-ases.md)
  * [Stealthy and Persistent Unalignment on Large Language Models via  Backdoor Injections](llm-attack/stealthy-and-persistent-unalignment-on-large-language-models-via-backdoor-injections.md)
  * [IMMUNIZATION AGAINST HARMFUL FINE-TUNING AT TACKS](llm-attack/immunization-against-harmful-fine-tuning-at-tacks.md)
  * [EMULATED DISALIGNMENT: SAFETY ALIGNMENT  FOR LARGE LANGUAGE MODELS MAY BACKFIRE!](llm-attack/emulated-disalignment-safety-alignment-for-large-language-models-may-backfire-1.md)
  * [Composite Backdoor Attacks Against Large Language Models](llm-attack/composite-backdoor-attacks-against-large-language-models.md)
  * [AWolf in Sheepâ€™s Clothing: Generalized Nested Jailbreak Prompts can  Fool Large Language Models Easi](llm-attack/awolf-in-sheeps-clothing-generalized-nested-jailbreak-prompts-can-fool-large-language-models-easi.md)
  * [ALL IN HOW YOU ASK FOR IT: SIMPLE BLACK-BOX METHOD FOR JAILBREAK ATTACKS](llm-attack/all-in-how-you-ask-for-it-simple-black-box-method-for-jailbreak-attacks.md)
  * [LLMJailbreak Attack versus Defense Techniques- A Comprehensive  Study](llm-attack/llmjailbreak-attack-versus-defense-techniques-a-comprehensive-study.md)
  * [Weak-to-Strong Jailbreaking on Large Language Models](llm-attack/weak-to-strong-jailbreaking-on-large-language-models.md)
  * [MULTIVERSE: Exposing Large Language Model Alignment Problems in  Diverse Worlds](llm-attack/multiverse-exposing-large-language-model-alignment-problems-in-diverse-worlds.md)
  * [Universal and Transferable Adversarial Attacks  on Aligned Language Models](llm-attack/universal-and-transferable-adversarial-attacks-on-aligned-language-models.md)
  * [COERCING LLMS TO DO AND REVEAL  (ALMOST) ANYTHING](llm-attack/coercing-llms-to-do-and-reveal-almost-anything.md)
  * [Generating Valid and Natural Adversarial Examples  with Large Language Models](llm-attack/generating-valid-and-natural-adversarial-examples-with-large-language-models.md)
  * [Stealthy and Persistent Unalignment on Large Language Models via  Backdoor Injections](llm-attack/stealthy-and-persistent-unalignment-on-large-language-models-via-backdoor-injections-1.md)
* [VLM-Defense](vlm-defense/README.md)
  * [Removing NSFW Concepts from Vision-and-Language Models  for Text-to-Image Retrieval and Generation](vlm-defense/removing-nsfw-concepts-from-vision-and-language-models-for-text-to-image-retrieval-and-generation.md)
  * [Image Safeguarding: Reasoning with Conditional Vision Language Model  and Obfuscating Unsafe Content](vlm-defense/image-safeguarding-reasoning-with-conditional-vision-language-model-and-obfuscating-unsafe-content.md)
  * [Towards Safe Self-Distillation of  Internet-Scale Text-to-Image Diffusion Models](vlm-defense/towards-safe-self-distillation-of-internet-scale-text-to-image-diffusion-models.md)
  * [Typographic Attacks in Large Multimodal Models Can be Alleviated by More  Informative Prompts](typographic-attacks-in-large-multimodal-models-can-be-alleviated-by-more-informative-prompts.md)
  * [Onthe Robustness of Large Multimodal Models Against Image Adversarial  Attacks](vlm-defense/onthe-robustness-of-large-multimodal-models-against-image-adversarial-attacks.md)
  * [Removing NSFW Concepts from Vision-and-Language Models  for Text-to-Image Retrieval and Generation](vlm-defense/removing-nsfw-concepts-from-vision-and-language-models-for-text-to-image-retrieval-and-generation-1.md)
  * [Safety Fine-Tuning at (Almost) No Cost:  ABaseline for Vision Large Language Models](vlm-defense/safety-fine-tuning-at-almost-no-cost-abaseline-for-vision-large-language-models.md)
  * [Partially Recentralization Softmax Loss for Vision-Language Models Robustness](vlm-defense/partially-recentralization-softmax-loss-for-vision-language-models-robustness.md)
  * [Adversarial Prompt Tuning for Vision-Language Models](vlm-defense/adversarial-prompt-tuning-for-vision-language-models.md)
  * [Defense-Prefix for Preventing Typographic Attacks on CLIP](vlm-defense/defense-prefix-for-preventing-typographic-attacks-on-clip.md)
  * [RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from  Fine-grained Correctional Human Feedb](vlm-defense/rlhf-v-towards-trustworthy-mllms-via-behavior-alignment-from-fine-grained-correctional-human-feedb.md)
  * [AMutation-Based Method for Multi-Modal Jailbreaking Attack](vlm-defense/amutation-based-method-for-multi-modal-jailbreaking-attack.md)
  * [HowEasy is It to Fool Your Multimodal LLMs?  AnEmpirical Analysis on Deceptive Prompts](vlm-defense/howeasy-is-it-to-fool-your-multimodal-llms-anempirical-analysis-on-deceptive-prompts.md)
* [VLM](vlm/README.md)
  * [Scalable Performance Analysis for Vision-Language Models](vlm/scalable-performance-analysis-for-vision-language-models.md)
* [VLM-Attack](vlm-attack/README.md)
  * [Circumventing Concept Erasure Methods For  Text-to-Image Generative Models](vlm-attack/circumventing-concept-erasure-methods-for-text-to-image-generative-models.md)
  * [Test-Time Backdoor Attacks on Multimodal Large Language Models](vlm-attack/test-time-backdoor-attacks-on-multimodal-large-language-models.md)
  * [JAILBREAK IN PIECES: COMPOSITIONAL ADVERSARIAL  ATTACKS ON MULTI-MODAL LANGUAGE MODELS](vlm-attack/jailbreak-in-pieces-compositional-adversarial-attacks-on-multi-modal-language-models.md)
  * [Jailbreaking Attack against Multimodal Large Language Model](vlm-attack/jailbreaking-attack-against-multimodal-large-language-model.md)
  * [Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts](vlm-attack/jailbreaking-gpt-4v-via-self-adversarial-attacks-with-system-prompts.md)
  * [IMAGE HIJACKS: ADVERSARIAL IMAGES CAN  CONTROL GENERATIVE MODELS AT RUNTIME](vlm-attack/image-hijacks-adversarial-images-can-control-generative-models-at-runtime.md)
  * [VISUAL ADVERSARIAL EXAMPLES  JAILBREAK ALIGNED LARGE LANGUAGE MODELS](vlm-attack/visual-adversarial-examples-jailbreak-aligned-large-language-models.md)
  * [Vision-LLMs Can Fool Themselves with Self-Generated Typographic  Attacks](vlm-attack/vision-llms-can-fool-themselves-with-self-generated-typographic-attacks.md)
  * [Query-Relevant Images Jailbreak Large Multi-Modal Models](vlm-attack/query-relevant-images-jailbreak-large-multi-modal-models.md)
  * [Towards Adversarial Attack on Vision-Language Pre-training Models](vlm-attack/towards-adversarial-attack-on-vision-language-pre-training-models.md)
  * [HowMany Are Unicorns in This Image? ASafety Evaluation Benchmark for Vision LLMs](vlm-attack/howmany-are-unicorns-in-this-image-asafety-evaluation-benchmark-for-vision-llms.md)
  * [SA-Attack: Improving Adversarial Transferability of  Vision-Language Pre-training Models via Self-Au](vlm-attack/sa-attack-improving-adversarial-transferability-of-vision-language-pre-training-models-via-self-au.md)
  * [MISUSING TOOLS IN LARGE LANGUAGE MODELS  WITH VISUAL ADVERSARIAL EXAMPLES](vlm-attack/misusing-tools-in-large-language-models-with-visual-adversarial-examples.md)
  * [VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models](vlm-attack/vl-trojan-multimodal-instruction-backdoor-attacks-against-autoregressive-visual-language-models.md)
  * [INSTRUCTTA: Instruction-Tuned Targeted Attack  for Large Vision-Language Models](vlm-attack/instructta-instruction-tuned-targeted-attack-for-large-vision-language-models.md)
  * [Set-level Guidance Attack: Boosting Adversarial Transferability of  Vision-Language Pre-training Mod](vlm-attack/set-level-guidance-attack-boosting-adversarial-transferability-of-vision-language-pre-training-mod.md)
  * [Shadowcast: STEALTHY DATA POISONING ATTACKS AGAINST  VISION-LANGUAGE MODELS](vlm-attack/shadowcast-stealthy-data-poisoning-attacks-against-vision-language-models.md)
  * [FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts](vlm-attack/figstep-jailbreaking-large-vision-language-models-via-typographic-visual-prompts.md)
  * [THE WOLF WITHIN: COVERT INJECTION OF MALICE  INTO MLLM SOCIETIES VIA AN MLLM OPERATIVE](vlm-attack/the-wolf-within-covert-injection-of-malice-into-mllm-societies-via-an-mllm-operative.md)
  * [Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning  Meets Adversarial Images](vlm-attack/stop-reasoning-when-multimodal-llms-with-chain-of-thought-reasoning-meets-adversarial-images.md)
* [Survey](survey/README.md)
  * [Generative AI Security: Challenges and Countermeasures](survey/generative-ai-security-challenges-and-countermeasures.md)
  * [Survey of Vulnerabilities in Large Language Models  Revealed by Adversarial Attacks](survey/survey-of-vulnerabilities-in-large-language-models-revealed-by-adversarial-attacks.md)
* [LLM-Defense](llm-defense/README.md)
  * [LANGUAGE MODELS ARE HOMER SIMPSON!](llm-defense/language-models-are-homer-simpson.md)
  * [THE POISON OF ALIGNMENT](llm-defense/the-poison-of-alignment.md)
  * [GAINING WISDOM FROM SETBACKS : ALIGNING  LARGE LANGUAGE MODELS VIA MISTAKE ANALYSIS](llm-defense/gaining-wisdom-from-setbacks-aligning-large-language-models-via-mistake-analysis.md)
  * [Making Harmful Behaviors Unlearnable for Large Language Models](llm-defense/making-harmful-behaviors-unlearnable-for-large-language-models.md)
  * [Fake Alignment: Are LLMs Really Aligned Well?](llm-defense/fake-alignment-are-llms-really-aligned-well.md)
  * [Red-Teaming Large Language Models using Chain of  Utterances for Safety-Alignment](llm-defense/red-teaming-large-language-models-using-chain-of-utterances-for-safety-alignment.md)
  * [Vaccine: Perturbation-aware Alignment for Large Language Model](llm-defense/vaccine-perturbation-aware-alignment-for-large-language-model.md)
  * [DEFENDING LARGE LANGUAGE MODELS  AGAINST JAILBREAK ATTACKS VIA SEMANTIC SMOOTHING](llm-defense/defending-large-language-models-against-jailbreak-attacks-via-semantic-smoothing.md)
  * [Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks  with Self-Refinement](llm-defense/break-the-breakout-reinventing-lm-defense-against-jailbreak-attacks-with-self-refinement.md)
  * [DEFENDING AGAINST ALIGNMENT-BREAKING AT TACKS VIA ROBUSTLY ALIGNED LLM](llm-defense/defending-against-alignment-breaking-at-tacks-via-robustly-aligned-llm.md)
  * [LLMSelf Defense: By Self Examination, LLMsKnowTheyAreBeing Tricked](llm-defense/llmself-defense-by-self-examination-llmsknowtheyarebeing-tricked.md)
  * [BASELINE DEFENSES FOR ADVERSARIAL ATTACKS  AGAINST ALIGNED LANGUAGE MODELS](llm-defense/baseline-defenses-for-adversarial-attacks-against-aligned-language-models.md)
  * [Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced  Alignment](llm-defense/mitigating-fine-tuning-jailbreak-attack-with-backdoor-enhanced-alignment.md)
* [LVM-Attack](lvm-attack/README.md)
  * [Adversarial Attacks on Foundational Vision Models](lvm-attack/adversarial-attacks-on-foundational-vision-models.md)
* [For Good](for-good/README.md)
  * [Image Safeguarding: Reasoning with Conditional Vision Language Model  and Obfuscating Unsafe Content](for-good/image-safeguarding-reasoning-with-conditional-vision-language-model-and-obfuscating-unsafe-content.md)
* [Benchmark](benchmark/README.md)
  * [HALLUSIONBENCH: An Advanced Diagnostic Suite for Entangled Language  Hallucination and Visual Illusi](benchmark/hallusionbench-an-advanced-diagnostic-suite-for-entangled-language-hallucination-and-visual-illusi.md)
* [MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance](mllm-protector-ensuring-mllms-safety-without-hurting-performance.md)
