# Table of contents

* [MM-LLM](README.md)
  * [MM-LLMs: Recent Advances in MultiModal Large Language Models](mm-llm/mm-llms-recent-advances-in-multimodal-large-language-models.md)
  * [Multimodal datasets: misogyny, pornography, and  malignant stereotypes](mm-llm/multimodal-datasets-misogyny-pornography-and-malignant-stereotypes.md)
  * [Sight Beyond Text: Multi-Modal Training Enhances  LLMsinTruthfulness and Ethics](mm-llm/sight-beyond-text-multi-modal-training-enhances-llmsintruthfulness-and-ethics.md)
  * [FOUNDATION MODELS AND FAIR USE](mm-llm/foundation-models-and-fair-use.md)
* [VLM-Defense](vlm-defense/README.md)
  * [Removing NSFW Concepts from Vision-and-Language Models  for Text-to-Image Retrieval and Generation](vlm-defense/removing-nsfw-concepts-from-vision-and-language-models-for-text-to-image-retrieval-and-generation.md)
  * [AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Sh](vlm-defense/adashield-safeguarding-multimodal-large-language-models-from-structure-based-attack-via-adaptive-sh.md)
  * [CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning](vlm-defense/cleanclip-mitigating-data-poisoning-attacks-in-multimodal-contrastive-learning.md)
  * [Image Safeguarding: Reasoning with Conditional Vision Language Model  and Obfuscating Unsafe Content](vlm-defense/image-safeguarding-reasoning-with-conditional-vision-language-model-and-obfuscating-unsafe-content.md)
  * [Towards Safe Self-Distillation of  Internet-Scale Text-to-Image Diffusion Models](vlm-defense/towards-safe-self-distillation-of-internet-scale-text-to-image-diffusion-models.md)
  * [Typographic Attacks in Large Multimodal Models Can be Alleviated by More  Informative Prompts](typographic-attacks-in-large-multimodal-models-can-be-alleviated-by-more-informative-prompts.md)
  * [Onthe Robustness of Large Multimodal Models Against Image Adversarial  Attacks](vlm-defense/onthe-robustness-of-large-multimodal-models-against-image-adversarial-attacks.md)
  * [Removing NSFW Concepts from Vision-and-Language Models  for Text-to-Image Retrieval and Generation](vlm-defense/removing-nsfw-concepts-from-vision-and-language-models-for-text-to-image-retrieval-and-generation-1.md)
  * [Safety Fine-Tuning at (Almost) No Cost:  ABaseline for Vision Large Language Models](vlm-defense/safety-fine-tuning-at-almost-no-cost-abaseline-for-vision-large-language-models.md)
  * [Partially Recentralization Softmax Loss for Vision-Language Models Robustness](vlm-defense/partially-recentralization-softmax-loss-for-vision-language-models-robustness.md)
  * [Adversarial Prompt Tuning for Vision-Language Models](vlm-defense/adversarial-prompt-tuning-for-vision-language-models.md)
  * [Defense-Prefix for Preventing Typographic Attacks on CLIP](vlm-defense/defense-prefix-for-preventing-typographic-attacks-on-clip.md)
  * [RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from  Fine-grained Correctional Human Feedb](vlm-defense/rlhf-v-towards-trustworthy-mllms-via-behavior-alignment-from-fine-grained-correctional-human-feedb.md)
  * [AMutation-Based Method for Multi-Modal Jailbreaking Attack](vlm-defense/amutation-based-method-for-multi-modal-jailbreaking-attack.md)
  * [HowEasy is It to Fool Your Multimodal LLMs?  AnEmpirical Analysis on Deceptive Prompts](vlm-defense/howeasy-is-it-to-fool-your-multimodal-llms-anempirical-analysis-on-deceptive-prompts.md)
  * [MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance](vlm-defense/mllm-protector-ensuring-mllms-safety-without-hurting-performance.md)
  * [EFUF: Efficient Fine-grained Unlearning Framework for Mitigating  Hallucinations in Multimodal Large](vlm-defense/efuf-efficient-fine-grained-unlearning-framework-for-mitigating-hallucinations-in-multimodal-large.md)
  * [Aligning Modalities in Vision Large Language Models via Preference Fine-tuning](vlm-defense/aligning-modalities-in-vision-large-language-models-via-preference-fine-tuning.md)
  * [Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings  for Robust Large Vision-Lang](vlm-defense/robust-clip-unsupervised-adversarial-fine-tuning-of-vision-embeddings-for-robust-large-vision-lang.md)
  * [Machine Vision Therapy: Multimodal Large Language  Models Can Enhance Visual Robustness  via Denoisi](vlm-defense/machine-vision-therapy-multimodal-large-language-models-can-enhance-visual-robustness-via-denoisi.md)
  * [Robust Contrastive Language-Image Pre-training  against Data Poisoning and Backdoor Attacks](vlm-defense/robust-contrastive-language-image-pre-training-against-data-poisoning-and-backdoor-attacks.md)
  * [HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data](vlm-defense/hallucidoctor-mitigating-hallucinatory-toxicity-in-visual-instruction-data.md)
* [VLM](vlm/README.md)
  * [Scalable Performance Analysis for Vision-Language Models](vlm/scalable-performance-analysis-for-vision-language-models.md)
* [VLM-Attack](vlm-attack/README.md)
  * [Circumventing Concept Erasure Methods For  Text-to-Image Generative Models](vlm-attack/circumventing-concept-erasure-methods-for-text-to-image-generative-models.md)
  * [Images are Achilles’ Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimoda](vlm-attack/images-are-achilles-heel-of-alignment-exploiting-visual-vulnerabilities-for-jailbreaking-multimoda.md)
  * [AN IMAGE IS WORTH 1000 LIES: ADVERSARIAL TRANSFERABILITY ACROSS PROMPTS ON VISIONLANGUAGE MODELS](vlm-attack/an-image-is-worth-1000-lies-adversarial-transferability-across-prompts-on-visionlanguage-models.md)
  * [Test-Time Backdoor Attacks on Multimodal Large Language Models](vlm-attack/test-time-backdoor-attacks-on-multimodal-large-language-models.md)
  * [JAILBREAK IN PIECES: COMPOSITIONAL ADVERSARIAL  ATTACKS ON MULTI-MODAL LANGUAGE MODELS](vlm-attack/jailbreak-in-pieces-compositional-adversarial-attacks-on-multi-modal-language-models.md)
  * [Jailbreaking Attack against Multimodal Large Language Model](vlm-attack/jailbreaking-attack-against-multimodal-large-language-model.md)
  * [Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts](vlm-attack/jailbreaking-gpt-4v-via-self-adversarial-attacks-with-system-prompts.md)
  * [IMAGE HIJACKS: ADVERSARIAL IMAGES CAN  CONTROL GENERATIVE MODELS AT RUNTIME](vlm-attack/image-hijacks-adversarial-images-can-control-generative-models-at-runtime.md)
  * [VISUAL ADVERSARIAL EXAMPLES  JAILBREAK ALIGNED LARGE LANGUAGE MODELS](vlm-attack/visual-adversarial-examples-jailbreak-aligned-large-language-models.md)
  * [Vision-LLMs Can Fool Themselves with Self-Generated Typographic  Attacks](vlm-attack/vision-llms-can-fool-themselves-with-self-generated-typographic-attacks.md)
  * [Query-Relevant Images Jailbreak Large Multi-Modal Models](vlm-attack/query-relevant-images-jailbreak-large-multi-modal-models.md)
  * [Towards Adversarial Attack on Vision-Language Pre-training Models](vlm-attack/towards-adversarial-attack-on-vision-language-pre-training-models.md)
  * [HowMany Are Unicorns in This Image? ASafety Evaluation Benchmark for Vision LLMs](vlm-attack/howmany-are-unicorns-in-this-image-asafety-evaluation-benchmark-for-vision-llms.md)
  * [SA-Attack: Improving Adversarial Transferability of  Vision-Language Pre-training Models via Self-Au](vlm-attack/sa-attack-improving-adversarial-transferability-of-vision-language-pre-training-models-via-self-au.md)
  * [MISUSING TOOLS IN LARGE LANGUAGE MODELS  WITH VISUAL ADVERSARIAL EXAMPLES](vlm-attack/misusing-tools-in-large-language-models-with-visual-adversarial-examples.md)
  * [VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models](vlm-attack/vl-trojan-multimodal-instruction-backdoor-attacks-against-autoregressive-visual-language-models.md)
  * [INSTRUCTTA: Instruction-Tuned Targeted Attack  for Large Vision-Language Models](vlm-attack/instructta-instruction-tuned-targeted-attack-for-large-vision-language-models.md)
  * [Set-level Guidance Attack: Boosting Adversarial Transferability of  Vision-Language Pre-training Mod](vlm-attack/set-level-guidance-attack-boosting-adversarial-transferability-of-vision-language-pre-training-mod.md)
  * [Shadowcast: STEALTHY DATA POISONING ATTACKS AGAINST  VISION-LANGUAGE MODELS](vlm-attack/shadowcast-stealthy-data-poisoning-attacks-against-vision-language-models.md)
  * [FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts](vlm-attack/figstep-jailbreaking-large-vision-language-models-via-typographic-visual-prompts.md)
  * [THE WOLF WITHIN: COVERT INJECTION OF MALICE  INTO MLLM SOCIETIES VIA AN MLLM OPERATIVE](vlm-attack/the-wolf-within-covert-injection-of-malice-into-mllm-societies-via-an-mllm-operative.md)
  * [Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning  Meets Adversarial Images](vlm-attack/stop-reasoning-when-multimodal-llms-with-chain-of-thought-reasoning-meets-adversarial-images.md)
  * [Agent Smith: A Single Image Can Jailbreak One Million  Multimodal LLM Agents Exponentially Fast](vlm-attack/agent-smith-a-single-image-can-jailbreak-one-million-multimodal-llm-agents-exponentially-fast.md)
  * [How Robust is Google’s Bard to Adversarial Image  Attacks?](vlm-attack/how-robust-is-googles-bard-to-adversarial-image-attacks.md)
  * [OnEvaluating Adversarial Robustness of  Large Vision-Language Models](vlm-attack/onevaluating-adversarial-robustness-of-large-vision-language-models.md)
  * [Onthe Adversarial Robustness of Multi-Modal Foundation Models](vlm-attack/onthe-adversarial-robustness-of-multi-modal-foundation-models.md)
  * [Are aligned neural networks adversarially aligned?](vlm-attack/are-aligned-neural-networks-adversarially-aligned.md)
  * [READING ISN’T BELIEVING:  ADVERSARIAL ATTACKS  ON MULTI-MODAL NEURONS](vlm-attack/reading-isnt-believing-adversarial-attacks-on-multi-modal-neurons.md)
  * [Black Box Adversarial Prompting  for Foundation Models](vlm-attack/black-box-adversarial-prompting-for-foundation-models.md)
  * [Evaluation and Analysis of Hallucination in Large Vision-Language Models](vlm-attack/evaluation-and-analysis-of-hallucination-in-large-vision-language-models.md)
  * [FOOL YOUR (VISION AND) LANGUAGE MODEL WITH  EMBARRASSINGLY SIMPLE PERMUTATIONS](vlm-attack/fool-your-vision-and-language-model-with-embarrassingly-simple-permutations.md)
  * [VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models](vlm-attack/vl-trojan-multimodal-instruction-backdoor-attacks-against-autoregressive-visual-language-models-1.md)
  * [Transferable Multimodal Attack on Vision-Language Pre-training Models](vlm-attack/transferable-multimodal-attack-on-vision-language-pre-training-models.md)
  * [BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning](vlm-attack/badclip-dual-embedding-guided-backdoor-attack-on-multimodal-contrastive-learning.md)
  * [AdvCLIP: Downstream-agnostic Adversarial Examples in Multimodal Contrastive Learning](vlm-attack/advclip-downstream-agnostic-adversarial-examples-in-multimodal-contrastive-learning.md)
* [Survey](survey/README.md)
  * [Generative AI Security: Challenges and Countermeasures](survey/generative-ai-security-challenges-and-countermeasures.md)
  * [Safety of Multimodal Large Language Models on Images and Text](survey/safety-of-multimodal-large-language-models-on-images-and-text.md)
  * [LLM Jailbreak Attack versus Defense Techniques - A Comprehensive Study](survey/llm-jailbreak-attack-versus-defense-techniques-a-comprehensive-study.md)
  * [Survey of Vulnerabilities in Large Language Models  Revealed by Adversarial Attacks](survey/survey-of-vulnerabilities-in-large-language-models-revealed-by-adversarial-attacks.md)
  * [ASurvey on Safe Multi-Modal Learning System](survey/asurvey-on-safe-multi-modal-learning-system.md)
  * [TRUSTWORTHY LARGE MODELS IN VISION: A SURVEY](survey/trustworthy-large-models-in-vision-a-survey.md)
  * [A Pathway Towards Responsible AI Generated Content](survey/a-pathway-towards-responsible-ai-generated-content.md)
  * [A Survey of Hallucination in “Large” Foundation Models](survey/a-survey-of-hallucination-in-large-foundation-models.md)
  * [An Early Categorization of Prompt Injection Attacks on Large Language Models](survey/an-early-categorization-of-prompt-injection-attacks-on-large-language-models.md)
  * [Comprehensive Assessment of Jailbreak Attacks Against LLMs](survey/comprehensive-assessment-of-jailbreak-attacks-against-llms.md)
  * [A Comprehensive Overview of Backdoor Attacks in Large Language Models within Communication Networks](survey/a-comprehensive-overview-of-backdoor-attacks-in-large-language-models-within-communication-networks.md)
  * [Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks](survey/survey-of-vulnerabilities-in-large-language-models-revealed-by-adversarial-attacks-1.md)
  * [Adversarial Machine Learning for Social Good: Reframing the Adversary as an Ally](survey/adversarial-machine-learning-for-social-good-reframing-the-adversary-as-an-ally.md)
  * [Red-Teaming for Generative AI: Silver Bullet or Security Theater?](survey/red-teaming-for-generative-ai-silver-bullet-or-security-theater.md)
  * [A STRONGREJECT for Empty Jailbreaks](survey/a-strongreject-for-empty-jailbreaks.md)
* [LVM-Attack](lvm-attack/README.md)
  * [Adversarial Attacks on Foundational Vision Models](lvm-attack/adversarial-attacks-on-foundational-vision-models.md)
* [For Good](for-good/README.md)
  * [Image Safeguarding: Reasoning with Conditional Vision Language Model  and Obfuscating Unsafe Content](for-good/image-safeguarding-reasoning-with-conditional-vision-language-model-and-obfuscating-unsafe-content.md)
* [Benchmark](benchmark/README.md)
  * [HALLUSIONBENCH: An Advanced Diagnostic Suite for Entangled Language  Hallucination and Visual Illusi](benchmark/hallusionbench-an-advanced-diagnostic-suite-for-entangled-language-hallucination-and-visual-illusi.md)
  * [ALL LANGUAGES MATTER: ON THE MULTILINGUAL SAFETY OF LARGE LANGUAGE MODELS](benchmark/all-languages-matter-on-the-multilingual-safety-of-large-language-models.md)
  * [Why Should Adversarial Perturbations be Imperceptible? Rethink the Research Paradigm in Adversarial](benchmark/why-should-adversarial-perturbations-be-imperceptible-rethink-the-research-paradigm-in-adversarial.md)
  * [Red Teaming Visual Language Models](benchmark/red-teaming-visual-language-models.md)
  * [Unified Hallucination Detection for Multimodal Large Language Models](benchmark/unified-hallucination-detection-for-multimodal-large-language-models.md)
  * [MLLM-as-a-Judge:  Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark](benchmark/mllm-as-a-judge-assessing-multimodal-llm-as-a-judge-with-vision-language-benchmark.md)
  * [Mitigating Hallucination in Large Multi-Modal  Models via Robust Instruction Tuning](benchmark/mitigating-hallucination-in-large-multi-modal-models-via-robust-instruction-tuning.md)
  * [CAN LANGUAGE MODELS BE INSTRUCTED TO  PROTECT PERSONAL INFORMATION?](benchmark/can-language-models-be-instructed-to-protect-personal-information.md)
  * [Detecting and Preventing Hallucinations in  Large Vision Language Models](benchmark/detecting-and-preventing-hallucinations-in-large-vision-language-models.md)
  * [DRESS : Instructing Large Vision-Language Models to  Align and Interact with Humans via Natural Lang](benchmark/dress-instructing-large-vision-language-models-to-align-and-interact-with-humans-via-natural-lang.md)
  * [ToViLaG: Your Visual-Language Generative Model is Also An Evildoer](benchmark/tovilag-your-visual-language-generative-model-is-also-an-evildoer.md)
  * [SC-Safety: A Multi-round Open-ended Question Adversarial Safety Benchmark for Large Language Models](benchmark/sc-safety-a-multi-round-open-ended-question-adversarial-safety-benchmark-for-large-language-models.md)
  * [PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts](benchmark/promptbench-towards-evaluating-the-robustness-of-large-language-models-on-adversarial-prompts.md)
  * [Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs](benchmark/do-not-answer-a-dataset-for-evaluating-safeguards-in-llms.md)
* [Explainality](explainality/README.md)
  * [Visual Explanations of Image-Text Representations via Multi-Modal Information Bottleneck Attributio](explainality/visual-explanations-of-image-text-representations-via-multi-modal-information-bottleneck-attributio.md)
* [Privacy-Defense](privacy-defense/README.md)
  * [Defending Our Privacy With Backdoors](privacy-defense/defending-our-privacy-with-backdoors.md)
  * [PromptCARE: Prompt Copyright Protection by Watermark Injection and Verification](privacy-defense/promptcare-prompt-copyright-protection-by-watermark-injection-and-verification.md)
* [Privacy-Attack](privacy-attack/README.md)
  * [PANDORA’S WHITE-BOX: INCREASED TRAINING DATA LEAKAGE IN OPEN LLMS](privacy-attack/pandoras-white-box-increased-training-data-leakage-in-open-llms.md)
  * [Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Cali](privacy-attack/practical-membership-inference-attacks-against-fine-tuned-large-language-models-via-self-prompt-cali.md)
  * [Prompt Stealing Attacks Against Large Language Models](privacy-attack/prompt-stealing-attacks-against-large-language-models.md)
  * [PRSA: Prompt Reverse Stealing Attacks against Large Language Models](privacy-attack/prsa-prompt-reverse-stealing-attacks-against-large-language-models.md)
  * [Low-Resource Languages Jailbreak GPT-4](privacy-attack/low-resource-languages-jailbreak-gpt-4.md)
  * [Scalable Extraction of Training Data from (Production) Language Models](privacy-attack/scalable-extraction-of-training-data-from-production-language-models.md)
* [Others](others/README.md)
  * [INFERRING OFFENSIVENESS IN IMAGES FROM NATURAL LANGUAGE SUPERVISION](others/inferring-offensiveness-in-images-from-natural-language-supervision.md)
  * [Disentangling Perceptions of Offensiveness: Cultural and Moral Correlates](others/disentangling-perceptions-of-offensiveness-cultural-and-moral-correlates.md)
  * [Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity](others/red-teaming-chatgpt-via-jailbreaking-bias-robustness-reliability-and-toxicity.md)
  * [LARGE LANGUAGE MODELS AS AUTOMATED ALIGNERS FOR BENCHMARKING VISION-LANGUAGE MODELS](others/large-language-models-as-automated-aligners-for-benchmarking-vision-language-models.md)
  * [Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Pro](others/not-what-youve-signed-up-for-compromising-real-world-llm-integrated-applications-with-indirect-pro.md)
  * [InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance](others/inferaligner-inference-time-alignment-for-harmlessness-through-cross-model-guidance.md)
  * [CAN LANGUAGE MODELS BE INSTRUCTED TO PROTECT PERSONAL INFORMATION?](others/can-language-models-be-instructed-to-protect-personal-information.md)
  * [AART: AI-Assisted Red-Teaming with Diverse Data Generation for New LLM-powered Applications](others/aart-ai-assisted-red-teaming-with-diverse-data-generation-for-new-llm-powered-applications.md)
  * [Prompt Injection Attacks and Defenses in LLM-Integrated Applications](others/prompt-injection-attacks-and-defenses-in-llm-integrated-applications.md)
  * [Removing RLHF Protections in GPT-4 via Fine-Tuning](others/removing-rlhf-protections-in-gpt-4-via-fine-tuning.md)
  * [SPML: A DSL for Defending Language Models Against Prompt Attacks](others/spml-a-dsl-for-defending-language-models-against-prompt-attacks.md)
  * [Stealthy Attack on Large Language Model based Recommendation](others/stealthy-attack-on-large-language-model-based-recommendation.md)
  * [Large Language Models Sometimes Generate Purely Negatively-Reinforced Text](others/large-language-models-sometimes-generate-purely-negatively-reinforced-text.md)
  * [On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective](others/on-the-robustness-of-chatgpt-an-adversarial-and-out-of-distribution-perspective.md)
  * [Learning from data in the mixed adversarial non-adversarial case: Finding the helpers and ignoring t](others/learning-from-data-in-the-mixed-adversarial-non-adversarial-case-finding-the-helpers-and-ignoring-t.md)
  * [longhorns at DADC 2022: How many linguists does it take to fool a Question Answering model? A system](others/longhorns-at-dadc-2022-how-many-linguists-does-it-take-to-fool-a-question-answering-model-a-system.md)
  * [A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning](others/a-prompt-array-keeps-the-bias-away-debiasing-vision-language-models-with-adversarial-learning.md)
  * [Adversarial Examples Generation for Reducing Implicit Gender Bias in Pre-trained Models](others/adversarial-examples-generation-for-reducing-implicit-gender-bias-in-pre-trained-models.md)
  * [Discovering the Hidden Vocabulary of DALLE-2](others/discovering-the-hidden-vocabulary-of-dalle-2.md)
  * [Raising the Cost of Malicious AI-Powered Image Editing](others/raising-the-cost-of-malicious-ai-powered-image-editing.md)
  * [Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the World’s Uglin](others/mitigating-inappropriateness-in-image-generation-can-there-be-value-in-reflecting-the-worlds-uglin.md)
  * [TOWARDS UNDERSTANDING THE INTERPLAY OF GENERATIVE ARTIFICIAL INTELLIGENCE AND THE INTERNET](others/towards-understanding-the-interplay-of-generative-artificial-intelligence-and-the-internet.md)
  * [Evaluating the Social Impact of Generative AI Systems in Systems and Society](others/evaluating-the-social-impact-of-generative-ai-systems-in-systems-and-society.md)
  * [Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities](others/transformation-vs-tradition-artificial-general-intelligence-agi-for-arts-and-humanities.md)
  * [Attacking LLM Watermarks by Exploiting Their Strengths](others/attacking-llm-watermarks-by-exploiting-their-strengths.md)
  * [TOWARDS RESPONSIBLE AI IN THE ERA OF GENERATIVE AI: A REFERENCE ARCHITECTURE FOR DESIGNING FOUNDATIO](others/towards-responsible-ai-in-the-era-of-generative-ai-a-reference-architecture-for-designing-foundatio.md)
  * [RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment](others/raft-reward-ranked-finetuning-for-generative-foundation-model-alignment.md)
  * [Intent-aligned AI systems deplete human agency: the need for agency foundations research in AI safet](others/intent-aligned-ai-systems-deplete-human-agency-the-need-for-agency-foundations-research-in-ai-safet.md)
  * [Risk Assessment and Statistical Significance in the Age of Foundation Models](others/risk-assessment-and-statistical-significance-in-the-age-of-foundation-models.md)
  * [The Foundation Model Transparency Index](others/the-foundation-model-transparency-index.md)
  * [The Privacy Pillar - A Conceptual Framework for Foundation Model-based Systems](others/the-privacy-pillar-a-conceptual-framework-for-foundation-model-based-systems.md)
  * [A Baseline Analysis of Reward Models’ Ability To Accurately Analyze Foundation Models Under Distribu](others/a-baseline-analysis-of-reward-models-ability-to-accurately-analyze-foundation-models-under-distribu.md)
  * [Foundational Moral Values for AI Alignment](others/foundational-moral-values-for-ai-alignment.md)
  * [Hazards from Increasingly Accessible Fine-Tuning of Downloadable Foundation Models](others/hazards-from-increasingly-accessible-fine-tuning-of-downloadable-foundation-models.md)
  * [ON CATASTROPHIC INHERITANCE OF LARGE FOUNDATION MODELS](others/on-catastrophic-inheritance-of-large-foundation-models.md)
  * [Foundation Model Sherpas: Guiding Foundation Models through Knowledge and Reasoning](others/foundation-model-sherpas-guiding-foundation-models-through-knowledge-and-reasoning.md)
  * [Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustmen](others/rewards-in-context-multi-objective-alignment-of-foundation-models-with-dynamic-preference-adjustmen.md)
  * [Foundation Model Transparency Reports](others/foundation-model-transparency-reports.md)
  * [SECURING RELIABILITY: A BRIEF OVERVIEW ON ENHANCING IN-CONTEXT LEARNING FOR FOUNDATION MODELS](others/securing-reliability-a-brief-overview-on-enhancing-in-context-learning-for-foundation-models.md)
  * [EXPLORING THE ADVERSARIAL CAPABILITIES OF LARGE LANGUAGE MODELS](others/exploring-the-adversarial-capabilities-of-large-language-models.md)
  * [TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification](others/trap-targeted-random-adversarial-prompt-honeypot-for-black-box-identification.md)
  * [LLM-Resistant Math Word Problem Generation via Adversarial Attacks](others/llm-resistant-math-word-problem-generation-via-adversarial-attacks.md)
  * [Efficient Black-Box Adversarial Attacks on Neural Text Detectors](others/efficient-black-box-adversarial-attacks-on-neural-text-detectors.md)
  * [Adversarial Preference Optimization](others/adversarial-preference-optimization.md)
  * [Combating Adversarial Attacks with Multi-Agent Debate](others/combating-adversarial-attacks-with-multi-agent-debate.md)
  * [How the Advent of Ubiquitous Large Language Models both Stymie and Turbocharge Dynamic Adversarial Q](others/how-the-advent-of-ubiquitous-large-language-models-both-stymie-and-turbocharge-dynamic-adversarial-q.md)
  * [L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks](others/l-autoda-leveraging-large-language-models-for-automated-decision-based-adversarial-attacks.md)
  * [Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection](others/hidding-the-ghostwriters-an-adversarial-evaluation-of-ai-generated-student-essay-detection.md)
  * [What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detectio](others/what-does-the-bot-say-opportunities-and-risks-of-large-language-models-in-social-media-bot-detectio.md)
  * [Prompted Contextual Vectors for Spear-Phishing Detection](others/prompted-contextual-vectors-for-spear-phishing-detection.md)
  * [Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection](others/token-ensemble-text-generation-on-attacking-the-automatic-ai-generated-text-detection.md)
  * [Recursive Chain-of-Feedback Prevents Performance Degradation from Redundant Prompting](others/recursive-chain-of-feedback-prevents-performance-degradation-from-redundant-prompting.md)
  * [Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents](others/watch-out-for-your-agents-investigating-backdoor-threats-to-llm-based-agents.md)
  * [RADAR: Robust AI-Text Detection via Adversarial Learning](others/radar-robust-ai-text-detection-via-adversarial-learning.md)
  * [OUTFOX: LLM-Generated Essay Detection Through In-Context Learning with Adversarially Generated Examp](others/outfox-llm-generated-essay-detection-through-in-context-learning-with-adversarially-generated-examp.md)
  * [Why do universal adversarial attacks work on large language models?: Geometry might be the answer](others/why-do-universal-adversarial-attacks-work-on-large-language-models-geometry-might-be-the-answer.md)
  * [J-Guard: Journalism Guided Adversarially Robust Detection of AI-generated News](others/j-guard-journalism-guided-adversarially-robust-detection-of-ai-generated-news.md)
  * [Distilling Adversarial Prompts from Safety Benchmarks: Report for the Adversarial Nibbler Challenge](others/distilling-adversarial-prompts-from-safety-benchmarks-report-for-the-adversarial-nibbler-challenge.md)
  * [Healing Unsafe Dialogue Responses with Weak Supervision Signals](others/healing-unsafe-dialogue-responses-with-weak-supervision-signals.md)
* [LLM-Attack](llm-attack/README.md)
  * [SHADOW ALIGNMENT: THE EASE OF SUBVERTING  SAFELY-ALIGNED LANGUAGE MODELS](llm-attack/shadow-alignment-the-ease-of-subverting-safely-aligned-language-models.md)
  * [Using Hallucinations to Bypass RLHF Filters](llm-attack/using-hallucinations-to-bypass-rlhf-filters.md)
  * [Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks](llm-attack/neural-exec-learning-and-learning-from-execution-triggers-for-prompt-injection-attacks.md)
  * [Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks](llm-attack/scaling-behavior-of-machine-translation-with-large-language-models-under-prompt-injection-attacks.md)
  * [FINE-TUNING ALIGNED LANGUAGE MODELS COMPROMISES SAFETY, EVEN WHEN USERS DO NOT INTEND TO!](llm-attack/fine-tuning-aligned-language-models-compromises-safety-even-when-users-do-not-intend-to.md)
  * [CATASTROPHIC JAILBREAK OF OPEN-SOURCE LLMS VIA EXPLOITING GENERATION](llm-attack/catastrophic-jailbreak-of-open-source-llms-via-exploiting-generation.md)
  * [EVALUATING THE SUSCEPTIBILITY OF PRE-TRAINED LANGUAGE MODELS VIA HANDCRAFTED ADVERSARIAL EXAMPLES](llm-attack/evaluating-the-susceptibility-of-pre-trained-language-models-via-handcrafted-adversarial-examples.md)
  * [Defending LLMs against Jailbreaking Attacks via Backtranslation](llm-attack/defending-llms-against-jailbreaking-attacks-via-backtranslation.md)
  * [EMULATED DISALIGNMENT: SAFETY ALIGNMENT  FOR LARGE LANGUAGE MODELS MAY BACKFIRE!](llm-attack/emulated-disalignment-safety-alignment-for-large-language-models-may-backfire.md)
  * [GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts](llm-attack/gptfuzzer-red-teaming-large-language-models-with-auto-generated-jailbreak-prompts.md)
  * [ON THE SAFETY OF OPEN-SOURCED LARGE LAN GUAGE MODELS: DOES ALIGNMENT REALLY PREVENT  THEM FROM BEING](llm-attack/on-the-safety-of-open-sourced-large-lan-guage-models-does-alignment-really-prevent-them-from-being.md)
  * [Unveiling the Implicit Toxicity in Large Language Models](llm-attack/unveiling-the-implicit-toxicity-in-large-language-models.md)
  * [Forcing Generative Models to Degenerate Ones:  The Power of Data Poisoning Attacks](llm-attack/forcing-generative-models-to-degenerate-ones-the-power-of-data-poisoning-attacks.md)
  * [Make Them Spill the Beans!  Coercive Knowledge Extraction from (Production) LLMs](llm-attack/make-them-spill-the-beans-coercive-knowledge-extraction-from-production-llms.md)
  * [Learning to Poison Large Language Models During Instruction Tuning](llm-attack/learning-to-poison-large-language-models-during-instruction-tuning.md)
  * [ALIGNMENT IS NOT SUFFICIENT TO PREVENT LARGE  LANGUAGE MODELS FROM GENERATING HARMFUL IN FORMATION:](llm-attack/alignment-is-not-sufficient-to-prevent-large-language-models-from-generating-harmful-in-formation.md)
  * [LANGUAGE MODEL UNALIGNMENT: PARAMETRIC  RED-TEAMING TO EXPOSE HIDDEN HARMS AND BI ASES](llm-attack/language-model-unalignment-parametric-red-teaming-to-expose-hidden-harms-and-bi-ases.md)
  * [Stealthy and Persistent Unalignment on Large Language Models via  Backdoor Injections](llm-attack/stealthy-and-persistent-unalignment-on-large-language-models-via-backdoor-injections.md)
  * [IMMUNIZATION AGAINST HARMFUL FINE-TUNING AT TACKS](llm-attack/immunization-against-harmful-fine-tuning-at-tacks.md)
  * [EMULATED DISALIGNMENT: SAFETY ALIGNMENT  FOR LARGE LANGUAGE MODELS MAY BACKFIRE!](llm-attack/emulated-disalignment-safety-alignment-for-large-language-models-may-backfire-1.md)
  * [Composite Backdoor Attacks Against Large Language Models](llm-attack/composite-backdoor-attacks-against-large-language-models.md)
  * [AWolf in Sheep’s Clothing: Generalized Nested Jailbreak Prompts can  Fool Large Language Models Easi](llm-attack/awolf-in-sheeps-clothing-generalized-nested-jailbreak-prompts-can-fool-large-language-models-easi.md)
  * [ALL IN HOW YOU ASK FOR IT: SIMPLE BLACK-BOX METHOD FOR JAILBREAK ATTACKS](llm-attack/all-in-how-you-ask-for-it-simple-black-box-method-for-jailbreak-attacks.md)
  * [LLMJailbreak Attack versus Defense Techniques- A Comprehensive  Study](llm-attack/llmjailbreak-attack-versus-defense-techniques-a-comprehensive-study.md)
  * [Weak-to-Strong Jailbreaking on Large Language Models](llm-attack/weak-to-strong-jailbreaking-on-large-language-models.md)
  * [MULTIVERSE: Exposing Large Language Model Alignment Problems in  Diverse Worlds](llm-attack/multiverse-exposing-large-language-model-alignment-problems-in-diverse-worlds.md)
  * [Universal and Transferable Adversarial Attacks  on Aligned Language Models](llm-attack/universal-and-transferable-adversarial-attacks-on-aligned-language-models.md)
  * [COERCING LLMS TO DO AND REVEAL  (ALMOST) ANYTHING](llm-attack/coercing-llms-to-do-and-reveal-almost-anything.md)
  * [Generating Valid and Natural Adversarial Examples  with Large Language Models](llm-attack/generating-valid-and-natural-adversarial-examples-with-large-language-models.md)
  * [Stealthy and Persistent Unalignment on Large Language Models via  Backdoor Injections](llm-attack/stealthy-and-persistent-unalignment-on-large-language-models-via-backdoor-injections-1.md)
  * [Scaling Laws for Adversarial Attacks on Language  Model Activations](llm-attack/scaling-laws-for-adversarial-attacks-on-language-model-activations.md)
  * [Ignore Previous Prompt: Attack Techniques For Language Models](llm-attack/ignore-previous-prompt-attack-techniques-for-language-models.md)
  * [ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages](llm-attack/toolsword-unveiling-safety-issues-of-large-language-models-in-tool-learning-across-three-stages.md)
  * [A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems](llm-attack/a-new-era-in-llm-security-exploring-security-concerns-in-real-world-llm-based-systems.md)
  * [ATTACKING LARGE LANGUAGE MODELS WITH PROJECTED GRADIENT DESCENT](llm-attack/attacking-large-language-models-with-projected-gradient-descent.md)
  * [Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embed](llm-attack/soft-prompt-threats-attacking-safety-alignment-and-unlearning-in-open-source-llms-through-the-embed.md)
  * [Query-Based Adversarial Prompt Generation](llm-attack/query-based-adversarial-prompt-generation.md)
  * [COERCING LLMS TO DO AND REVEAL (ALMOST) ANYTHING](llm-attack/coercing-llms-to-do-and-reveal-almost-anything-1.md)
  * [Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment](llm-attack/is-llm-as-a-judge-robust-investigating-universal-adversarial-attacks-on-zero-shot-llm-assessment.md)
  * [Fast Adversarial Attacks on Language Models In One GPU Minute](llm-attack/fast-adversarial-attacks-on-language-models-in-one-gpu-minute.md)
  * [DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers](llm-attack/drattack-prompt-decomposition-and-reconstruction-makes-powerful-llm-jailbreakers.md)
  * [From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Transla](llm-attack/from-noise-to-clarity-unraveling-the-adversarial-suffix-of-large-language-model-attacks-via-transla.md)
  * [Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts](llm-attack/rainbow-teaming-open-ended-generation-of-diverse-adversarial-prompts.md)
  * [CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models](llm-attack/codechameleon-personalized-encryption-framework-for-jailbreaking-large-language-models.md)
  * [Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content](llm-attack/large-language-models-are-vulnerable-to-bait-and-switch-attacks-for-generating-harmful-content.md)
  * [Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced Alignment](llm-attack/mitigating-fine-tuning-jailbreak-attack-with-backdoor-enhanced-alignment.md)
  * [PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails](llm-attack/prp-propagating-universal-perturbations-to-attack-large-language-model-guard-rails.md)
  * [A Cross-Language Investigation into Jailbreak Attacks in Large Language Models](llm-attack/a-cross-language-investigation-into-jailbreak-attacks-in-large-language-models.md)
  * [DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers](llm-attack/drattack-prompt-decomposition-and-reconstruction-makes-powerful-llm-jailbreakers-1.md)
  * [Attacking LLM Watermarks by Exploiting Their Strengths](llm-attack/attacking-llm-watermarks-by-exploiting-their-strengths.md)
  * [From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Transla](llm-attack/from-noise-to-clarity-unraveling-the-adversarial-suffix-of-large-language-model-attacks-via-transl-1.md)
  * [DeepInception: Hypnotize Large Language Model to Be Jailbreaker](llm-attack/deepinception-hypnotize-large-language-model-to-be-jailbreaker.md)
  * [Hijacking Large Language Models via Adversarial In-Context Learning](llm-attack/hijacking-large-language-models-via-adversarial-in-context-learning.md)
  * [DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions](llm-attack/deceptprompt-exploiting-llm-driven-code-generation-via-adversarial-natural-language-instructions.md)
  * [Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models](llm-attack/syntactic-ghost-an-imperceptible-general-purpose-backdoor-attacks-on-pre-trained-language-models.md)
  * [Conversation Reconstruction Attack Against GPT Models](llm-attack/conversation-reconstruction-attack-against-gpt-models.md)
  * [PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models](llm-attack/poisonedrag-knowledge-poisoning-attacks-to-retrieval-augmented-generation-of-large-language-models.md)
  * [COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability](llm-attack/cold-attack-jailbreaking-llms-with-stealthiness-and-controllability.md)
  * [Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit](llm-attack/play-guessing-game-with-llm-indirect-jailbreak-attack-with-implicit.md)
  * [Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks](llm-attack/leveraging-the-context-through-multi-round-interactions-for-jailbreaking-attacks.md)
  * [Analyzing the Inherent Response Tendency of LLMs: Real-World Instructions-Driven Jailbreak](llm-attack/analyzing-the-inherent-response-tendency-of-llms-real-world-instructions-driven-jailbreak.md)
  * [UNIVERSAL JAILBREAK BACKDOORS FROM POISONED HUMAN FEEDBACK](llm-attack/universal-jailbreak-backdoors-from-poisoned-human-feedback.md)
  * [Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking](llm-attack/cognitive-overload-jailbreaking-large-language-models-with-overloaded-logical-thinking.md)
  * [Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation](llm-attack/scalable-and-transferable-black-box-jailbreaks-for-language-models-via-persona-modulation.md)
  * [POISONPROMPT: BACKDOOR ATTACK ON PROMPT-BASED LARGE LANGUAGE MODELS](llm-attack/poisonprompt-backdoor-attack-on-prompt-based-large-language-models.md)
  * [BACKDOORING INSTRUCTION-TUNED LARGE LANGUAGE MODELS WITH VIRTUAL PROMPT INJECTION](llm-attack/backdooring-instruction-tuned-large-language-models-with-virtual-prompt-injection.md)
  * [Backdoor Attacks for In-Context Learning with Language Models](llm-attack/backdoor-attacks-for-in-context-learning-with-language-models.md)
  * [Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models](llm-attack/instructions-as-backdoors-backdoor-vulnerabilities-of-instruction-tuning-for-large-language-models.md)
  * [UOR: Universal Backdoor Attacks on Pre-trained Language Models](llm-attack/uor-universal-backdoor-attacks-on-pre-trained-language-models.md)
  * [Fake Alignment: Are LLMs Really Aligned Well?](llm-attack/fake-alignment-are-llms-really-aligned-well.md)
  * [Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models](llm-attack/syntactic-ghost-an-imperceptible-general-purpose-backdoor-attacks-on-pre-trained-language-models-1.md)
  * [Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignm](llm-attack/backdoor-activation-attack-attack-large-language-models-using-activation-steering-for-safety-alignm.md)
  * [Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control](llm-attack/imperio-language-guided-backdoor-attacks-for-arbitrary-model-control.md)
  * [Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Agai](llm-attack/large-language-models-are-better-adversaries-exploring-generative-clean-label-backdoor-attacks-agai.md)
  * [Universal Vulnerabilities in Large Language Models: Backdoor Attacks for In-context Learning](llm-attack/universal-vulnerabilities-in-large-language-models-backdoor-attacks-for-in-context-learning.md)
  * [BADCHAIN: BACKDOOR CHAIN-OF-THOUGHT PROMPTING FOR LARGE LANGUAGE MODELS](llm-attack/badchain-backdoor-chain-of-thought-prompting-for-large-language-models.md)
  * [AUTODAN: INTERPRETABLE GRADIENT-BASED ADVERSARIAL ATTACKS ON LARGE LANGUAGE MODELS](llm-attack/autodan-interpretable-gradient-based-adversarial-attacks-on-large-language-models.md)
  * [AN LLM CAN FOOL ITSELF: A PROMPT-BASED ADVERSARIAL ATTACK](llm-attack/an-llm-can-fool-itself-a-prompt-based-adversarial-attack.md)
  * [AUTOMATIC HALLUCINATION ASSESSMENT FOR ALIGNED LARGE LANGUAGE MODELS VIA TRANSFERABLE ADVERSARIAL AT](llm-attack/automatic-hallucination-assessment-for-aligned-large-language-models-via-transferable-adversarial-at.md)
  * [LLM LIES: HALLUCINATIONS ARE NOT BUGS, BUT FEATURES AS ADVERSARIAL EXAMPLES](llm-attack/llm-lies-hallucinations-are-not-bugs-but-features-as-adversarial-examples.md)
  * [LOFT: LOCAL PROXY FINE-TUNING FOR IMPROVING TRANSFERABILITY OF ADVERSARIAL ATTACKS AGAINST LARGE LAN](llm-attack/loft-local-proxy-fine-tuning-for-improving-transferability-of-adversarial-attacks-against-large-lan.md)
  * [Universal and Transferable Adversarial Attacks on Aligned Language Models](llm-attack/universal-and-transferable-adversarial-attacks-on-aligned-language-models-1.md)
  * [Robustness Over Time: Understanding Adversarial Examples’ Effectiveness on Longitudinal Versions of](llm-attack/robustness-over-time-understanding-adversarial-examples-effectiveness-on-longitudinal-versions-of.md)
  * [BASELINE DEFENSES FOR ADVERSARIAL ATTACKS AGAINST ALIGNED LANGUAGE MODELS](llm-attack/baseline-defenses-for-adversarial-attacks-against-aligned-language-models.md)
  * [Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue](llm-attack/speak-out-of-turn-safety-vulnerability-of-large-language-models-in-multi-turn-dialogue.md)
  * [Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Recon](llm-attack/making-them-ask-and-answer-jailbreaking-large-language-models-in-few-queries-via-disguise-and-recon.md)
  * [Adversarial Demonstration Attacks on Large Language Models](llm-attack/adversarial-demonstration-attacks-on-large-language-models.md)
  * [COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models](llm-attack/cover-a-heuristic-greedy-adversarial-attack-on-prompt-based-learning-in-language-models.md)
  * [The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Mod](llm-attack/the-butterfly-effect-of-altering-prompts-how-small-changes-and-jailbreaks-affect-large-language-mod.md)
  * [Open the Pandora’s Box of LLMs: Jailbreaking LLMs through Representation Engineering](llm-attack/open-the-pandoras-box-of-llms-jailbreaking-llms-through-representation-engineering.md)
  * [How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Huma](llm-attack/how-johnny-can-persuade-llms-to-jailbreak-them-rethinking-persuasion-to-challenge-ai-safety-by-huma.md)
  * [Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models](llm-attack/sowing-the-wind-reaping-the-whirlwind-the-impact-of-editing-language-models.md)
  * [PANDORA: Jailbreak GPTs by Retrieval Augmented Generation Poisoning](llm-attack/pandora-jailbreak-gpts-by-retrieval-augmented-generation-poisoning.md)
  * [Weak-to-Strong Jailbreaking on Large Language Models](llm-attack/weak-to-strong-jailbreaking-on-large-language-models-1.md)
  * [Jailbreaking Proprietary Large Language Models using Word Substitution Cipher](llm-attack/jailbreaking-proprietary-large-language-models-using-word-substitution-cipher.md)
  * [Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs](llm-attack/semantic-mirror-jailbreak-genetic-algorithm-based-jailbreak-prompts-against-open-source-llms.md)
  * [Adversarial Attacks and Defenses in Large Language Models: Old and New Threats](llm-attack/adversarial-attacks-and-defenses-in-large-language-models-old-and-new-threats.md)
  * [Jailbroken: How Does LLM Safety Training Fail?](llm-attack/jailbroken-how-does-llm-safety-training-fail.md)
  * [ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs](llm-attack/artprompt-ascii-art-based-jailbreak-attacks-against-aligned-llms.md)
  * [GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large](llm-attack/guard-role-playing-to-generate-natural-language-jailbreakings-to-test-guideline-adherence-of-large.md)
  * [Tastle: Distract Large Language Models for Automatic Jailbreak Attack](llm-attack/tastle-distract-large-language-models-for-automatic-jailbreak-attack.md)
  * [Exploring Safety Generalization Challenges of Large Language Models via Code](llm-attack/exploring-safety-generalization-challenges-of-large-language-models-via-code.md)
  * [PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails](llm-attack/prp-propagating-universal-perturbations-to-attack-large-language-model-guard-rails-1.md)
* [LLM-Defense](llm-defense/README.md)
  * [LANGUAGE MODELS ARE HOMER SIMPSON!](llm-defense/language-models-are-homer-simpson.md)
  * [Detoxifying Large Language Models via Knowledge Editing](llm-defense/detoxifying-large-language-models-via-knowledge-editing.md)
  * [THE POISON OF ALIGNMENT](llm-defense/the-poison-of-alignment.md)
  * [ROSE: Robust Selective Fine-tuning for Pre-trained Language Models](llm-defense/rose-robust-selective-fine-tuning-for-pre-trained-language-models.md)
  * [GAINING WISDOM FROM SETBACKS : ALIGNING  LARGE LANGUAGE MODELS VIA MISTAKE ANALYSIS](llm-defense/gaining-wisdom-from-setbacks-aligning-large-language-models-via-mistake-analysis.md)
  * [Making Harmful Behaviors Unlearnable for Large Language Models](llm-defense/making-harmful-behaviors-unlearnable-for-large-language-models.md)
  * [Fake Alignment: Are LLMs Really Aligned Well?](llm-defense/fake-alignment-are-llms-really-aligned-well.md)
  * [Red-Teaming Large Language Models using Chain of  Utterances for Safety-Alignment](llm-defense/red-teaming-large-language-models-using-chain-of-utterances-for-safety-alignment.md)
  * [Vaccine: Perturbation-aware Alignment for Large Language Model](llm-defense/vaccine-perturbation-aware-alignment-for-large-language-model.md)
  * [DEFENDING LARGE LANGUAGE MODELS  AGAINST JAILBREAK ATTACKS VIA SEMANTIC SMOOTHING](llm-defense/defending-large-language-models-against-jailbreak-attacks-via-semantic-smoothing.md)
  * [Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks  with Self-Refinement](llm-defense/break-the-breakout-reinventing-lm-defense-against-jailbreak-attacks-with-self-refinement.md)
  * [DEFENDING AGAINST ALIGNMENT-BREAKING AT TACKS VIA ROBUSTLY ALIGNED LLM](llm-defense/defending-against-alignment-breaking-at-tacks-via-robustly-aligned-llm.md)
  * [LLMSelf Defense: By Self Examination, LLMsKnowTheyAreBeing Tricked](llm-defense/llmself-defense-by-self-examination-llmsknowtheyarebeing-tricked.md)
  * [BASELINE DEFENSES FOR ADVERSARIAL ATTACKS  AGAINST ALIGNED LANGUAGE MODELS](llm-defense/baseline-defenses-for-adversarial-attacks-against-aligned-language-models.md)
  * [Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced  Alignment](llm-defense/mitigating-fine-tuning-jailbreak-attack-with-backdoor-enhanced-alignment.md)
  * [LLMsCanDefend Themselves Against Jailbreaking in  a Practical Manner: A Vision Paper](llm-defense/llmscandefend-themselves-against-jailbreaking-in-a-practical-manner-a-vision-paper.md)
  * [Detoxifying Text with MARCO:  Controllable Revision with Experts and Anti-Experts](llm-defense/detoxifying-text-with-marco-controllable-revision-with-experts-and-anti-experts.md)
  * [Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models](llm-defense/self-destructing-models-increasing-the-costs-of-harmful-dual-uses-of-foundation-models.md)
  * [Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Lang](llm-defense/whispers-that-shake-foundations-analyzing-and-mitigating-false-premise-hallucinations-in-large-lang.md)
  * [CAMOUFLAGE IS ALL YOU NEED: EVALUATING AND ENHANCING LANGUAGE MODEL ROBUSTNESS AGAINST CAMOUFLAGE AD](llm-defense/camouflage-is-all-you-need-evaluating-and-enhancing-language-model-robustness-against-camouflage-ad.md)
  * [Defending Jailbreak Prompts via In-Context Adversarial Game](llm-defense/defending-jailbreak-prompts-via-in-context-adversarial-game.md)
  * [Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement](llm-defense/break-the-breakout-reinventing-lm-defense-against-jailbreak-attacks-with-self-refinement-1.md)
  * [Defending LLMs against Jailbreaking Attacks via Backtranslation](llm-defense/defending-llms-against-jailbreaking-attacks-via-backtranslation.md)
  * [IMMUNIZATION AGAINST HARMFUL FINE-TUNING ATTACKS](llm-defense/immunization-against-harmful-fine-tuning-attacks.md)
  * [Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield](llm-defense/robust-safety-classifier-for-large-language-models-adversarial-prompt-shield.md)
  * [JAB: Joint Adversarial Prompting and Belief Augmentation](llm-defense/jab-joint-adversarial-prompting-and-belief-augmentation.md)
  * [TOKEN-LEVEL ADVERSARIAL PROMPT DETECTION BASED ON PERPLEXITY MEASURES AND CONTEXTUAL INFORMATION](llm-defense/token-level-adversarial-prompt-detection-based-on-perplexity-measures-and-contextual-information.md)
  * [Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks](llm-defense/robust-prompt-optimization-for-defending-language-models-against-jailbreaking-attacks.md)
  * [Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning](llm-defense/studious-bob-fight-back-against-jailbreaking-via-prompt-adversarial-tuning.md)
  * [Vaccine: Perturbation-aware Alignment for Large Language Model](llm-defense/vaccine-perturbation-aware-alignment-for-large-language-model-1.md)
  * [SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding](llm-defense/safedecoding-defending-against-jailbreak-attacks-via-safety-aware-decoding.md)
  * [Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization](llm-defense/defending-large-language-models-against-jailbreaking-attacks-through-goal-prioritization.md)
  * [Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks](llm-defense/defending-pre-trained-language-models-as-few-shot-learners-against-backdoor-attacks.md)
  * [LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors](llm-defense/lmsanitator-defending-prompt-tuning-against-task-agnostic-backdoors.md)
  * [Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language](llm-defense/diffusion-theory-as-a-scalpel-detecting-and-purifying-poisonous-dimensions-in-pre-trained-language.md)
  * [Analyzing And Editing Inner Mechanisms of Backdoored Language Models](llm-defense/analyzing-and-editing-inner-mechanisms-of-backdoored-language-models.md)
  * [Setting the Trap: Capturing and Defeating Backdoors in Pretrained Language Models through Honeypots](llm-defense/setting-the-trap-capturing-and-defeating-backdoors-in-pretrained-language-models-through-honeypots.md)
  * [ROBUSTIFYING LANGUAGE MODELS WITH TESTTIME ADAPTATION](llm-defense/robustifying-language-models-with-testtime-adaptation.md)
  * [Jailbreaker in Jail: Moving Target Defense for Large Language](llm-defense/jailbreaker-in-jail-moving-target-defense-for-large-language.md)
  * [DETECTING LANGUAGE MODEL ATTACKS WITH PERPLEXITY](llm-defense/detecting-language-model-attacks-with-perplexity.md)
  * [Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation an](llm-defense/adversarial-fine-tuning-of-language-models-an-iterative-optimisation-approach-for-the-generation-an.md)
  * [From Adversarial Arms Race to Model-centric Evaluation Motivating a Unified Automatic Robustness Eva](llm-defense/from-adversarial-arms-race-to-model-centric-evaluation-motivating-a-unified-automatic-robustness-eva.md)
  * [LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked](llm-defense/llm-self-defense-by-self-examination-llms-know-they-are-being-tricked.md)
  * [Intention Analysis Makes LLMs A Good Jailbreak Defender](llm-defense/intention-analysis-makes-llms-a-good-jailbreak-defender.md)
  * [Defending Against Disinformation Attacks in Open-Domain Question Answering](llm-defense/defending-against-disinformation-attacks-in-open-domain-question-answering.md)
  * [Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning](llm-defense/pruning-for-protection-increasing-jailbreak-resistance-in-aligned-llms-without-fine-tuning.md)
  * [Round Trip Translation Defence against Large Language Model Jailbreaking Attacks](llm-defense/round-trip-translation-defence-against-large-language-model-jailbreaking-attacks.md)
  * [How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?](llm-defense/how-should-pre-trained-language-models-be-fine-tuned-towards-adversarial-robustness.md)
  * [SELF-GUARD: Empower the LLM to Safeguard Itself](llm-defense/self-guard-empower-the-llm-to-safeguard-itself.md)
  * [Precisely the Point: Adversarial Augmentations for Faithful and Informative Text Generation](llm-defense/precisely-the-point-adversarial-augmentations-for-faithful-and-informative-text-generation.md)
  * [Is the System Message Really Important to Jailbreaks in Large Language Models?](llm-defense/is-the-system-message-really-important-to-jailbreaks-in-large-language-models.md)
