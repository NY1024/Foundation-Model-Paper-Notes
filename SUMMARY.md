# Table of contents

* [MM-LLM](README.md)
  * [MM-LLMs: Recent Advances in MultiModal Large Language Models](mm-llm/mm-llms-recent-advances-in-multimodal-large-language-models.md)
  * [Multimodal datasets: misogyny, pornography, and  malignant stereotypes](mm-llm/multimodal-datasets-misogyny-pornography-and-malignant-stereotypes.md)
* [LLM-Attack](llm-attack/README.md)
  * [SHADOW ALIGNMENT: THE EASE OF SUBVERTING  SAFELY-ALIGNED LANGUAGE MODELS](llm-attack/shadow-alignment-the-ease-of-subverting-safely-aligned-language-models.md)
  * [Defending LLMs against Jailbreaking Attacks via Backtranslation](llm-attack/defending-llms-against-jailbreaking-attacks-via-backtranslation.md)
  * [EMULATED DISALIGNMENT: SAFETY ALIGNMENT  FOR LARGE LANGUAGE MODELS MAY BACKFIRE!](llm-attack/emulated-disalignment-safety-alignment-for-large-language-models-may-backfire.md)
  * [ON THE SAFETY OF OPEN-SOURCED LARGE LAN GUAGE MODELS: DOES ALIGNMENT REALLY PREVENT  THEM FROM BEING](llm-attack/on-the-safety-of-open-sourced-large-lan-guage-models-does-alignment-really-prevent-them-from-being.md)
  * [Unveiling the Implicit Toxicity in Large Language Models](llm-attack/unveiling-the-implicit-toxicity-in-large-language-models.md)
  * [Forcing Generative Models to Degenerate Ones:  The Power of Data Poisoning Attacks](forcing-generative-models-to-degenerate-ones-the-power-of-data-poisoning-attacks.md)
  * [Make Them Spill the Beans!  Coercive Knowledge Extraction from (Production) LLMs](llm-attack/make-them-spill-the-beans-coercive-knowledge-extraction-from-production-llms.md)
  * [Learning to Poison Large Language Models During Instruction Tuning](llm-attack/learning-to-poison-large-language-models-during-instruction-tuning.md)
  * [ALIGNMENT IS NOT SUFFICIENT TO PREVENT LARGE  LANGUAGE MODELS FROM GENERATING HARMFUL IN FORMATION:](alignment-is-not-sufficient-to-prevent-large-language-models-from-generating-harmful-in-formation.md)
  * [LANGUAGE MODEL UNALIGNMENT: PARAMETRIC  RED-TEAMING TO EXPOSE HIDDEN HARMS AND BI ASES](llm-attack/language-model-unalignment-parametric-red-teaming-to-expose-hidden-harms-and-bi-ases.md)
  * [Stealthy and Persistent Unalignment on Large Language Models via  Backdoor Injections](llm-attack/stealthy-and-persistent-unalignment-on-large-language-models-via-backdoor-injections.md)
  * [IMMUNIZATION AGAINST HARMFUL FINE-TUNING AT TACKS](llm-attack/immunization-against-harmful-fine-tuning-at-tacks.md)
  * [EMULATED DISALIGNMENT: SAFETY ALIGNMENT  FOR LARGE LANGUAGE MODELS MAY BACKFIRE!](llm-attack/emulated-disalignment-safety-alignment-for-large-language-models-may-backfire-1.md)
  * [Composite Backdoor Attacks Against Large Language Models](llm-attack/composite-backdoor-attacks-against-large-language-models.md)
  * [AWolf in Sheepâ€™s Clothing: Generalized Nested Jailbreak Prompts can  Fool Large Language Models Easi](llm-attack/awolf-in-sheeps-clothing-generalized-nested-jailbreak-prompts-can-fool-large-language-models-easi.md)
  * [ALL IN HOW YOU ASK FOR IT: SIMPLE BLACK-BOX METHOD FOR JAILBREAK ATTACKS](llm-attack/all-in-how-you-ask-for-it-simple-black-box-method-for-jailbreak-attacks.md)
  * [LLMJailbreak Attack versus Defense Techniques- A Comprehensive  Study](llm-attack/llmjailbreak-attack-versus-defense-techniques-a-comprehensive-study.md)
  * [Weak-to-Strong Jailbreaking on Large Language Models](llm-attack/weak-to-strong-jailbreaking-on-large-language-models.md)
  * [MULTIVERSE: Exposing Large Language Model Alignment Problems in  Diverse Worlds](llm-attack/multiverse-exposing-large-language-model-alignment-problems-in-diverse-worlds.md)
  * [Universal and Transferable Adversarial Attacks  on Aligned Language Models](llm-attack/universal-and-transferable-adversarial-attacks-on-aligned-language-models.md)
  * [COERCING LLMS TO DO AND REVEAL  (ALMOST) ANYTHING](llm-attack/coercing-llms-to-do-and-reveal-almost-anything.md)
  * [Generating Valid and Natural Adversarial Examples  with Large Language Models](llm-attack/generating-valid-and-natural-adversarial-examples-with-large-language-models.md)
  * [Stealthy and Persistent Unalignment on Large Language Models via  Backdoor Injections](llm-attack/stealthy-and-persistent-unalignment-on-large-language-models-via-backdoor-injections-1.md)
* [VLM-Defense](vlm-defense/README.md)
  * [Removing NSFW Concepts from Vision-and-Language Models  for Text-to-Image Retrieval and Generation](vlm-defense/removing-nsfw-concepts-from-vision-and-language-models-for-text-to-image-retrieval-and-generation.md)
  * [Image Safeguarding: Reasoning with Conditional Vision Language Model  and Obfuscating Unsafe Content](vlm-defense/image-safeguarding-reasoning-with-conditional-vision-language-model-and-obfuscating-unsafe-content.md)
  * [Towards Safe Self-Distillation of  Internet-Scale Text-to-Image Diffusion Models](vlm-defense/towards-safe-self-distillation-of-internet-scale-text-to-image-diffusion-models.md)
  * [Typographic Attacks in Large Multimodal Models Can be Alleviated by More  Informative Prompts](vlm-defense/typographic-attacks-in-large-multimodal-models-can-be-alleviated-by-more-informative-prompts.md)
* [VLM](vlm/README.md)
  * [Scalable Performance Analysis for Vision-Language Models](vlm/scalable-performance-analysis-for-vision-language-models.md)
* [VLM-Attack](vlm-attack/README.md)
  * [Circumventing Concept Erasure Methods For  Text-to-Image Generative Models](vlm-attack/circumventing-concept-erasure-methods-for-text-to-image-generative-models.md)
  * [Test-Time Backdoor Attacks on Multimodal Large Language Models](vlm-attack/test-time-backdoor-attacks-on-multimodal-large-language-models.md)
  * [JAILBREAK IN PIECES: COMPOSITIONAL ADVERSARIAL  ATTACKS ON MULTI-MODAL LANGUAGE MODELS](vlm-attack/jailbreak-in-pieces-compositional-adversarial-attacks-on-multi-modal-language-models.md)
  * [Jailbreaking Attack against Multimodal Large Language Model](vlm-attack/jailbreaking-attack-against-multimodal-large-language-model.md)
  * [Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts](vlm-attack/jailbreaking-gpt-4v-via-self-adversarial-attacks-with-system-prompts.md)
  * [IMAGE HIJACKS: ADVERSARIAL IMAGES CAN  CONTROL GENERATIVE MODELS AT RUNTIME](vlm-attack/image-hijacks-adversarial-images-can-control-generative-models-at-runtime.md)
  * [VISUAL ADVERSARIAL EXAMPLES  JAILBREAK ALIGNED LARGE LANGUAGE MODELS](vlm-attack/visual-adversarial-examples-jailbreak-aligned-large-language-models.md)
  * [Vision-LLMs Can Fool Themselves with Self-Generated Typographic  Attacks](vlm-attack/vision-llms-can-fool-themselves-with-self-generated-typographic-attacks.md)
* [Survey](survey/README.md)
  * [Generative AI Security: Challenges and Countermeasures](survey/generative-ai-security-challenges-and-countermeasures.md)
  * [Survey of Vulnerabilities in Large Language Models  Revealed by Adversarial Attacks](survey/survey-of-vulnerabilities-in-large-language-models-revealed-by-adversarial-attacks.md)
* [LLM-Defense](llm-defense/README.md)
  * [LANGUAGE MODELS ARE HOMER SIMPSON!](llm-defense/language-models-are-homer-simpson.md)
  * [THE POISON OF ALIGNMENT](llm-defense/the-poison-of-alignment.md)
  * [GAINING WISDOM FROM SETBACKS : ALIGNING  LARGE LANGUAGE MODELS VIA MISTAKE ANALYSIS](llm-defense/gaining-wisdom-from-setbacks-aligning-large-language-models-via-mistake-analysis.md)
  * [Making Harmful Behaviors Unlearnable for Large Language Models](llm-defense/making-harmful-behaviors-unlearnable-for-large-language-models.md)
  * [Fake Alignment: Are LLMs Really Aligned Well?](llm-defense/fake-alignment-are-llms-really-aligned-well.md)
  * [Red-Teaming Large Language Models using Chain of  Utterances for Safety-Alignment](llm-defense/red-teaming-large-language-models-using-chain-of-utterances-for-safety-alignment.md)
  * [Vaccine: Perturbation-aware Alignment for Large Language Model](llm-defense/vaccine-perturbation-aware-alignment-for-large-language-model.md)
  * [DEFENDING LARGE LANGUAGE MODELS  AGAINST JAILBREAK ATTACKS VIA SEMANTIC SMOOTHING](llm-defense/defending-large-language-models-against-jailbreak-attacks-via-semantic-smoothing.md)
  * [Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks  with Self-Refinement](llm-defense/break-the-breakout-reinventing-lm-defense-against-jailbreak-attacks-with-self-refinement.md)
  * [DEFENDING AGAINST ALIGNMENT-BREAKING AT TACKS VIA ROBUSTLY ALIGNED LLM](llm-defense/defending-against-alignment-breaking-at-tacks-via-robustly-aligned-llm.md)
  * [LLMSelf Defense: By Self Examination, LLMsKnowTheyAreBeing Tricked](llm-defense/llmself-defense-by-self-examination-llmsknowtheyarebeing-tricked.md)
  * [BASELINE DEFENSES FOR ADVERSARIAL ATTACKS  AGAINST ALIGNED LANGUAGE MODELS](llm-defense/baseline-defenses-for-adversarial-attacks-against-aligned-language-models.md)
  * [Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced  Alignment](llm-defense/mitigating-fine-tuning-jailbreak-attack-with-backdoor-enhanced-alignment.md)
* [LVM-Attack](lvm-attack/README.md)
  * [Adversarial Attacks on Foundational Vision Models](lvm-attack/adversarial-attacks-on-foundational-vision-models.md)
* [Query-Relevant Images Jailbreak Large Multi-Modal Models](query-relevant-images-jailbreak-large-multi-modal-models.md)
