# Table of contents

* [MM-LLM](README.md)
  * [MM-LLMs: Recent Advances in MultiModal Large Language Models](mm-llm/mm-llms-recent-advances-in-multimodal-large-language-models.md)
  * [Multimodal datasets: misogyny, pornography, and  malignant stereotypes](mm-llm/multimodal-datasets-misogyny-pornography-and-malignant-stereotypes.md)
  * [Sight Beyond Text: Multi-Modal Training Enhances  LLMsinTruthfulness and Ethics](mm-llm/sight-beyond-text-multi-modal-training-enhances-llmsintruthfulness-and-ethics.md)
  * [FOUNDATION MODELS AND FAIR USE](mm-llm/foundation-models-and-fair-use.md)
* [LLM-Attack](llm-attack/README.md)
  * [SHADOW ALIGNMENT: THE EASE OF SUBVERTING  SAFELY-ALIGNED LANGUAGE MODELS](llm-attack/shadow-alignment-the-ease-of-subverting-safely-aligned-language-models.md)
  * [Defending LLMs against Jailbreaking Attacks via Backtranslation](llm-attack/defending-llms-against-jailbreaking-attacks-via-backtranslation.md)
  * [EMULATED DISALIGNMENT: SAFETY ALIGNMENT  FOR LARGE LANGUAGE MODELS MAY BACKFIRE!](llm-attack/emulated-disalignment-safety-alignment-for-large-language-models-may-backfire.md)
  * [ON THE SAFETY OF OPEN-SOURCED LARGE LAN GUAGE MODELS: DOES ALIGNMENT REALLY PREVENT  THEM FROM BEING](llm-attack/on-the-safety-of-open-sourced-large-lan-guage-models-does-alignment-really-prevent-them-from-being.md)
  * [Unveiling the Implicit Toxicity in Large Language Models](llm-attack/unveiling-the-implicit-toxicity-in-large-language-models.md)
  * [Forcing Generative Models to Degenerate Ones:  The Power of Data Poisoning Attacks](forcing-generative-models-to-degenerate-ones-the-power-of-data-poisoning-attacks.md)
  * [Make Them Spill the Beans!  Coercive Knowledge Extraction from (Production) LLMs](llm-attack/make-them-spill-the-beans-coercive-knowledge-extraction-from-production-llms.md)
  * [Learning to Poison Large Language Models During Instruction Tuning](llm-attack/learning-to-poison-large-language-models-during-instruction-tuning.md)
  * [ALIGNMENT IS NOT SUFFICIENT TO PREVENT LARGE  LANGUAGE MODELS FROM GENERATING HARMFUL IN FORMATION:](alignment-is-not-sufficient-to-prevent-large-language-models-from-generating-harmful-in-formation.md)
  * [LANGUAGE MODEL UNALIGNMENT: PARAMETRIC  RED-TEAMING TO EXPOSE HIDDEN HARMS AND BI ASES](llm-attack/language-model-unalignment-parametric-red-teaming-to-expose-hidden-harms-and-bi-ases.md)
  * [Stealthy and Persistent Unalignment on Large Language Models via  Backdoor Injections](llm-attack/stealthy-and-persistent-unalignment-on-large-language-models-via-backdoor-injections.md)
  * [IMMUNIZATION AGAINST HARMFUL FINE-TUNING AT TACKS](llm-attack/immunization-against-harmful-fine-tuning-at-tacks.md)
  * [EMULATED DISALIGNMENT: SAFETY ALIGNMENT  FOR LARGE LANGUAGE MODELS MAY BACKFIRE!](llm-attack/emulated-disalignment-safety-alignment-for-large-language-models-may-backfire-1.md)
  * [Composite Backdoor Attacks Against Large Language Models](llm-attack/composite-backdoor-attacks-against-large-language-models.md)
  * [AWolf in Sheepâ€™s Clothing: Generalized Nested Jailbreak Prompts can  Fool Large Language Models Easi](llm-attack/awolf-in-sheeps-clothing-generalized-nested-jailbreak-prompts-can-fool-large-language-models-easi.md)
  * [ALL IN HOW YOU ASK FOR IT: SIMPLE BLACK-BOX METHOD FOR JAILBREAK ATTACKS](llm-attack/all-in-how-you-ask-for-it-simple-black-box-method-for-jailbreak-attacks.md)
  * [LLMJailbreak Attack versus Defense Techniques- A Comprehensive  Study](llm-attack/llmjailbreak-attack-versus-defense-techniques-a-comprehensive-study.md)
  * [Weak-to-Strong Jailbreaking on Large Language Models](llm-attack/weak-to-strong-jailbreaking-on-large-language-models.md)
  * [MULTIVERSE: Exposing Large Language Model Alignment Problems in  Diverse Worlds](llm-attack/multiverse-exposing-large-language-model-alignment-problems-in-diverse-worlds.md)
  * [Universal and Transferable Adversarial Attacks  on Aligned Language Models](llm-attack/universal-and-transferable-adversarial-attacks-on-aligned-language-models.md)
  * [COERCING LLMS TO DO AND REVEAL  (ALMOST) ANYTHING](llm-attack/coercing-llms-to-do-and-reveal-almost-anything.md)
  * [Generating Valid and Natural Adversarial Examples  with Large Language Models](llm-attack/generating-valid-and-natural-adversarial-examples-with-large-language-models.md)
  * [Stealthy and Persistent Unalignment on Large Language Models via  Backdoor Injections](llm-attack/stealthy-and-persistent-unalignment-on-large-language-models-via-backdoor-injections-1.md)
  * [Scaling Laws for Adversarial Attacks on Language  Model Activations](llm-attack/scaling-laws-for-adversarial-attacks-on-language-model-activations.md)
  * [Ignore Previous Prompt: Attack Techniques For Language Models](llm-attack/ignore-previous-prompt-attack-techniques-for-language-models.md)
* [VLM-Defense](vlm-defense/README.md)
  * [Removing NSFW Concepts from Vision-and-Language Models  for Text-to-Image Retrieval and Generation](vlm-defense/removing-nsfw-concepts-from-vision-and-language-models-for-text-to-image-retrieval-and-generation.md)
  * [Image Safeguarding: Reasoning with Conditional Vision Language Model  and Obfuscating Unsafe Content](vlm-defense/image-safeguarding-reasoning-with-conditional-vision-language-model-and-obfuscating-unsafe-content.md)
  * [Towards Safe Self-Distillation of  Internet-Scale Text-to-Image Diffusion Models](vlm-defense/towards-safe-self-distillation-of-internet-scale-text-to-image-diffusion-models.md)
  * [Typographic Attacks in Large Multimodal Models Can be Alleviated by More  Informative Prompts](typographic-attacks-in-large-multimodal-models-can-be-alleviated-by-more-informative-prompts.md)
  * [Onthe Robustness of Large Multimodal Models Against Image Adversarial  Attacks](vlm-defense/onthe-robustness-of-large-multimodal-models-against-image-adversarial-attacks.md)
  * [Removing NSFW Concepts from Vision-and-Language Models  for Text-to-Image Retrieval and Generation](vlm-defense/removing-nsfw-concepts-from-vision-and-language-models-for-text-to-image-retrieval-and-generation-1.md)
  * [Safety Fine-Tuning at (Almost) No Cost:  ABaseline for Vision Large Language Models](vlm-defense/safety-fine-tuning-at-almost-no-cost-abaseline-for-vision-large-language-models.md)
  * [Partially Recentralization Softmax Loss for Vision-Language Models Robustness](vlm-defense/partially-recentralization-softmax-loss-for-vision-language-models-robustness.md)
  * [Adversarial Prompt Tuning for Vision-Language Models](vlm-defense/adversarial-prompt-tuning-for-vision-language-models.md)
  * [Defense-Prefix for Preventing Typographic Attacks on CLIP](vlm-defense/defense-prefix-for-preventing-typographic-attacks-on-clip.md)
  * [RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from  Fine-grained Correctional Human Feedb](vlm-defense/rlhf-v-towards-trustworthy-mllms-via-behavior-alignment-from-fine-grained-correctional-human-feedb.md)
  * [AMutation-Based Method for Multi-Modal Jailbreaking Attack](vlm-defense/amutation-based-method-for-multi-modal-jailbreaking-attack.md)
  * [HowEasy is It to Fool Your Multimodal LLMs?  AnEmpirical Analysis on Deceptive Prompts](vlm-defense/howeasy-is-it-to-fool-your-multimodal-llms-anempirical-analysis-on-deceptive-prompts.md)
  * [MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance](vlm-defense/mllm-protector-ensuring-mllms-safety-without-hurting-performance.md)
  * [EFUF: Efficient Fine-grained Unlearning Framework for Mitigating  Hallucinations in Multimodal Large](vlm-defense/efuf-efficient-fine-grained-unlearning-framework-for-mitigating-hallucinations-in-multimodal-large.md)
  * [Aligning Modalities in Vision Large Language Models via Preference Fine-tuning](vlm-defense/aligning-modalities-in-vision-large-language-models-via-preference-fine-tuning.md)
  * [Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings  for Robust Large Vision-Lang](vlm-defense/robust-clip-unsupervised-adversarial-fine-tuning-of-vision-embeddings-for-robust-large-vision-lang.md)
  * [Machine Vision Therapy: Multimodal Large Language  Models Can Enhance Visual Robustness  via Denoisi](vlm-defense/machine-vision-therapy-multimodal-large-language-models-can-enhance-visual-robustness-via-denoisi.md)
  * [Robust Contrastive Language-Image Pre-training  against Data Poisoning and Backdoor Attacks](vlm-defense/robust-contrastive-language-image-pre-training-against-data-poisoning-and-backdoor-attacks.md)
  * [HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data](vlm-defense/hallucidoctor-mitigating-hallucinatory-toxicity-in-visual-instruction-data.md)
* [VLM](vlm/README.md)
  * [Scalable Performance Analysis for Vision-Language Models](vlm/scalable-performance-analysis-for-vision-language-models.md)
* [VLM-Attack](vlm-attack/README.md)
  * [Circumventing Concept Erasure Methods For  Text-to-Image Generative Models](vlm-attack/circumventing-concept-erasure-methods-for-text-to-image-generative-models.md)
  * [Test-Time Backdoor Attacks on Multimodal Large Language Models](vlm-attack/test-time-backdoor-attacks-on-multimodal-large-language-models.md)
  * [JAILBREAK IN PIECES: COMPOSITIONAL ADVERSARIAL  ATTACKS ON MULTI-MODAL LANGUAGE MODELS](vlm-attack/jailbreak-in-pieces-compositional-adversarial-attacks-on-multi-modal-language-models.md)
  * [Jailbreaking Attack against Multimodal Large Language Model](vlm-attack/jailbreaking-attack-against-multimodal-large-language-model.md)
  * [Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts](vlm-attack/jailbreaking-gpt-4v-via-self-adversarial-attacks-with-system-prompts.md)
  * [IMAGE HIJACKS: ADVERSARIAL IMAGES CAN  CONTROL GENERATIVE MODELS AT RUNTIME](vlm-attack/image-hijacks-adversarial-images-can-control-generative-models-at-runtime.md)
  * [VISUAL ADVERSARIAL EXAMPLES  JAILBREAK ALIGNED LARGE LANGUAGE MODELS](vlm-attack/visual-adversarial-examples-jailbreak-aligned-large-language-models.md)
  * [Vision-LLMs Can Fool Themselves with Self-Generated Typographic  Attacks](vlm-attack/vision-llms-can-fool-themselves-with-self-generated-typographic-attacks.md)
  * [Query-Relevant Images Jailbreak Large Multi-Modal Models](vlm-attack/query-relevant-images-jailbreak-large-multi-modal-models.md)
  * [Towards Adversarial Attack on Vision-Language Pre-training Models](vlm-attack/towards-adversarial-attack-on-vision-language-pre-training-models.md)
  * [HowMany Are Unicorns in This Image? ASafety Evaluation Benchmark for Vision LLMs](vlm-attack/howmany-are-unicorns-in-this-image-asafety-evaluation-benchmark-for-vision-llms.md)
  * [SA-Attack: Improving Adversarial Transferability of  Vision-Language Pre-training Models via Self-Au](vlm-attack/sa-attack-improving-adversarial-transferability-of-vision-language-pre-training-models-via-self-au.md)
  * [MISUSING TOOLS IN LARGE LANGUAGE MODELS  WITH VISUAL ADVERSARIAL EXAMPLES](vlm-attack/misusing-tools-in-large-language-models-with-visual-adversarial-examples.md)
  * [VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models](vlm-attack/vl-trojan-multimodal-instruction-backdoor-attacks-against-autoregressive-visual-language-models.md)
  * [INSTRUCTTA: Instruction-Tuned Targeted Attack  for Large Vision-Language Models](vlm-attack/instructta-instruction-tuned-targeted-attack-for-large-vision-language-models.md)
  * [Set-level Guidance Attack: Boosting Adversarial Transferability of  Vision-Language Pre-training Mod](vlm-attack/set-level-guidance-attack-boosting-adversarial-transferability-of-vision-language-pre-training-mod.md)
  * [Shadowcast: STEALTHY DATA POISONING ATTACKS AGAINST  VISION-LANGUAGE MODELS](vlm-attack/shadowcast-stealthy-data-poisoning-attacks-against-vision-language-models.md)
  * [FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts](vlm-attack/figstep-jailbreaking-large-vision-language-models-via-typographic-visual-prompts.md)
  * [THE WOLF WITHIN: COVERT INJECTION OF MALICE  INTO MLLM SOCIETIES VIA AN MLLM OPERATIVE](vlm-attack/the-wolf-within-covert-injection-of-malice-into-mllm-societies-via-an-mllm-operative.md)
  * [Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning  Meets Adversarial Images](vlm-attack/stop-reasoning-when-multimodal-llms-with-chain-of-thought-reasoning-meets-adversarial-images.md)
  * [Agent Smith: A Single Image Can Jailbreak One Million  Multimodal LLM Agents Exponentially Fast](vlm-attack/agent-smith-a-single-image-can-jailbreak-one-million-multimodal-llm-agents-exponentially-fast.md)
  * [How Robust is Googleâ€™s Bard to Adversarial Image  Attacks?](vlm-attack/how-robust-is-googles-bard-to-adversarial-image-attacks.md)
  * [OnEvaluating Adversarial Robustness of  Large Vision-Language Models](vlm-attack/onevaluating-adversarial-robustness-of-large-vision-language-models.md)
  * [Onthe Adversarial Robustness of Multi-Modal Foundation Models](vlm-attack/onthe-adversarial-robustness-of-multi-modal-foundation-models.md)
  * [Are aligned neural networks adversarially aligned?](vlm-attack/are-aligned-neural-networks-adversarially-aligned.md)
  * [READING ISNâ€™T BELIEVING:  ADVERSARIAL ATTACKS  ON MULTI-MODAL NEURONS](vlm-attack/reading-isnt-believing-adversarial-attacks-on-multi-modal-neurons.md)
  * [Black Box Adversarial Prompting  for Foundation Models](vlm-attack/black-box-adversarial-prompting-for-foundation-models.md)
  * [Evaluation and Analysis of Hallucination in Large Vision-Language Models](vlm-attack/evaluation-and-analysis-of-hallucination-in-large-vision-language-models.md)
  * [FOOL YOUR (VISION AND) LANGUAGE MODEL WITH  EMBARRASSINGLY SIMPLE PERMUTATIONS](vlm-attack/fool-your-vision-and-language-model-with-embarrassingly-simple-permutations.md)
* [Survey](survey/README.md)
  * [Generative AI Security: Challenges and Countermeasures](survey/generative-ai-security-challenges-and-countermeasures.md)
  * [Survey of Vulnerabilities in Large Language Models  Revealed by Adversarial Attacks](survey/survey-of-vulnerabilities-in-large-language-models-revealed-by-adversarial-attacks.md)
  * [ASurvey on Safe Multi-Modal Learning System](survey/asurvey-on-safe-multi-modal-learning-system.md)
  * [TRUSTWORTHY LARGE MODELS IN VISION: A SURVEY](survey/trustworthy-large-models-in-vision-a-survey.md)
  * [A Pathway Towards Responsible AI Generated Content](survey/a-pathway-towards-responsible-ai-generated-content.md)
* [LLM-Defense](llm-defense/README.md)
  * [LANGUAGE MODELS ARE HOMER SIMPSON!](llm-defense/language-models-are-homer-simpson.md)
  * [THE POISON OF ALIGNMENT](llm-defense/the-poison-of-alignment.md)
  * [GAINING WISDOM FROM SETBACKS : ALIGNING  LARGE LANGUAGE MODELS VIA MISTAKE ANALYSIS](llm-defense/gaining-wisdom-from-setbacks-aligning-large-language-models-via-mistake-analysis.md)
  * [Making Harmful Behaviors Unlearnable for Large Language Models](llm-defense/making-harmful-behaviors-unlearnable-for-large-language-models.md)
  * [Fake Alignment: Are LLMs Really Aligned Well?](llm-defense/fake-alignment-are-llms-really-aligned-well.md)
  * [Red-Teaming Large Language Models using Chain of  Utterances for Safety-Alignment](llm-defense/red-teaming-large-language-models-using-chain-of-utterances-for-safety-alignment.md)
  * [Vaccine: Perturbation-aware Alignment for Large Language Model](llm-defense/vaccine-perturbation-aware-alignment-for-large-language-model.md)
  * [DEFENDING LARGE LANGUAGE MODELS  AGAINST JAILBREAK ATTACKS VIA SEMANTIC SMOOTHING](llm-defense/defending-large-language-models-against-jailbreak-attacks-via-semantic-smoothing.md)
  * [Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks  with Self-Refinement](llm-defense/break-the-breakout-reinventing-lm-defense-against-jailbreak-attacks-with-self-refinement.md)
  * [DEFENDING AGAINST ALIGNMENT-BREAKING AT TACKS VIA ROBUSTLY ALIGNED LLM](llm-defense/defending-against-alignment-breaking-at-tacks-via-robustly-aligned-llm.md)
  * [LLMSelf Defense: By Self Examination, LLMsKnowTheyAreBeing Tricked](llm-defense/llmself-defense-by-self-examination-llmsknowtheyarebeing-tricked.md)
  * [BASELINE DEFENSES FOR ADVERSARIAL ATTACKS  AGAINST ALIGNED LANGUAGE MODELS](llm-defense/baseline-defenses-for-adversarial-attacks-against-aligned-language-models.md)
  * [Mitigating Fine-tuning Jailbreak Attack with Backdoor Enhanced  Alignment](llm-defense/mitigating-fine-tuning-jailbreak-attack-with-backdoor-enhanced-alignment.md)
  * [LLMsCanDefend Themselves Against Jailbreaking in  a Practical Manner: A Vision Paper](llm-defense/llmscandefend-themselves-against-jailbreaking-in-a-practical-manner-a-vision-paper.md)
  * [Detoxifying Text with MARCO:  Controllable Revision with Experts and Anti-Experts](llm-defense/detoxifying-text-with-marco-controllable-revision-with-experts-and-anti-experts.md)
  * [Self-Destructing Models: Increasing the Costs of Harmful Dual Uses of Foundation Models](llm-defense/self-destructing-models-increasing-the-costs-of-harmful-dual-uses-of-foundation-models.md)
* [LVM-Attack](lvm-attack/README.md)
  * [Adversarial Attacks on Foundational Vision Models](lvm-attack/adversarial-attacks-on-foundational-vision-models.md)
* [For Good](for-good/README.md)
  * [Image Safeguarding: Reasoning with Conditional Vision Language Model  and Obfuscating Unsafe Content](for-good/image-safeguarding-reasoning-with-conditional-vision-language-model-and-obfuscating-unsafe-content.md)
* [Benchmark](benchmark/README.md)
  * [HALLUSIONBENCH: An Advanced Diagnostic Suite for Entangled Language  Hallucination and Visual Illusi](benchmark/hallusionbench-an-advanced-diagnostic-suite-for-entangled-language-hallucination-and-visual-illusi.md)
  * [Red Teaming Visual Language Models](benchmark/red-teaming-visual-language-models.md)
  * [Unified Hallucination Detection for Multimodal Large Language Models](benchmark/unified-hallucination-detection-for-multimodal-large-language-models.md)
  * [MLLM-as-a-Judge:  Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark](benchmark/mllm-as-a-judge-assessing-multimodal-llm-as-a-judge-with-vision-language-benchmark.md)
  * [Mitigating Hallucination in Large Multi-Modal  Models via Robust Instruction Tuning](benchmark/mitigating-hallucination-in-large-multi-modal-models-via-robust-instruction-tuning.md)
  * [CAN LANGUAGE MODELS BE INSTRUCTED TO  PROTECT PERSONAL INFORMATION?](benchmark/can-language-models-be-instructed-to-protect-personal-information.md)
  * [Detecting and Preventing Hallucinations in  Large Vision Language Models](benchmark/detecting-and-preventing-hallucinations-in-large-vision-language-models.md)
  * [DRESS : Instructing Large Vision-Language Models to  Align and Interact with Humans via Natural Lang](benchmark/dress-instructing-large-vision-language-models-to-align-and-interact-with-humans-via-natural-lang.md)
  * [ToViLaG: Your Visual-Language Generative Model is Also An Evildoer](benchmark/tovilag-your-visual-language-generative-model-is-also-an-evildoer.md)
* [Explainality](explainality/README.md)
  * [Visual Explanations of Image-Text Representations via Multi-Modal Information Bottleneck Attributio](explainality/visual-explanations-of-image-text-representations-via-multi-modal-information-bottleneck-attributio.md)
* [Others](others/README.md)
  * [INFERRING OFFENSIVENESS IN IMAGES FROM NATURAL LANGUAGE SUPERVISION](others/inferring-offensiveness-in-images-from-natural-language-supervision.md)
  * [Discovering the Hidden Vocabulary of DALLE-2](others/discovering-the-hidden-vocabulary-of-dalle-2.md)
  * [Raising the Cost of Malicious AI-Powered Image Editing](others/raising-the-cost-of-malicious-ai-powered-image-editing.md)
  * [Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the Worldâ€™s Uglin](others/mitigating-inappropriateness-in-image-generation-can-there-be-value-in-reflecting-the-worlds-uglin.md)
  * [TOWARDS UNDERSTANDING THE INTERPLAY OF GENERATIVE ARTIFICIAL INTELLIGENCE AND THE INTERNET](others/towards-understanding-the-interplay-of-generative-artificial-intelligence-and-the-internet.md)
  * [Evaluating the Social Impact of Generative AI Systems in Systems and Society](others/evaluating-the-social-impact-of-generative-ai-systems-in-systems-and-society.md)
  * [Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities](others/transformation-vs-tradition-artificial-general-intelligence-agi-for-arts-and-humanities.md)
  * [Attacking LLM Watermarks by Exploiting Their Strengths](others/attacking-llm-watermarks-by-exploiting-their-strengths.md)
  * [TOWARDS RESPONSIBLE AI IN THE ERA OF GENERATIVE AI: A REFERENCE ARCHITECTURE FOR DESIGNING FOUNDATIO](others/towards-responsible-ai-in-the-era-of-generative-ai-a-reference-architecture-for-designing-foundatio.md)
