# Table of contents

* [MM-LLM](README.md)
  * [MM-LLMs: Recent Advances in MultiModal Large Language Models](mm-llm/mm-llms-recent-advances-in-multimodal-large-language-models.md)
  * [Multimodal datasets: misogyny, pornography, and  malignant stereotypes](mm-llm/multimodal-datasets-misogyny-pornography-and-malignant-stereotypes.md)
* [LLM-Attack](llm-attack/README.md)
  * [SHADOW ALIGNMENT: THE EASE OF SUBVERTING  SAFELY-ALIGNED LANGUAGE MODELS](llm-attack/shadow-alignment-the-ease-of-subverting-safely-aligned-language-models.md)
  * [Defending LLMs against Jailbreaking Attacks via Backtranslation](llm-attack/defending-llms-against-jailbreaking-attacks-via-backtranslation.md)
  * [EMULATED DISALIGNMENT: SAFETY ALIGNMENT  FOR LARGE LANGUAGE MODELS MAY BACKFIRE!](llm-attack/emulated-disalignment-safety-alignment-for-large-language-models-may-backfire.md)
  * [ON THE SAFETY OF OPEN-SOURCED LARGE LAN GUAGE MODELS: DOES ALIGNMENT REALLY PREVENT  THEM FROM BEING](llm-attack/on-the-safety-of-open-sourced-large-lan-guage-models-does-alignment-really-prevent-them-from-being.md)
  * [Unveiling the Implicit Toxicity in Large Language Models](llm-attack/unveiling-the-implicit-toxicity-in-large-language-models.md)
  * [Forcing Generative Models to Degenerate Ones:  The Power of Data Poisoning Attacks](forcing-generative-models-to-degenerate-ones-the-power-of-data-poisoning-attacks.md)
  * [Make Them Spill the Beans!  Coercive Knowledge Extraction from (Production) LLMs](llm-attack/make-them-spill-the-beans-coercive-knowledge-extraction-from-production-llms.md)
  * [Learning to Poison Large Language Models During Instruction Tuning](llm-attack/learning-to-poison-large-language-models-during-instruction-tuning.md)
  * [ALIGNMENT IS NOT SUFFICIENT TO PREVENT LARGE  LANGUAGE MODELS FROM GENERATING HARMFUL IN FORMATION:](alignment-is-not-sufficient-to-prevent-large-language-models-from-generating-harmful-in-formation.md)
  * [LANGUAGE MODEL UNALIGNMENT: PARAMETRIC  RED-TEAMING TO EXPOSE HIDDEN HARMS AND BI ASES](llm-attack/language-model-unalignment-parametric-red-teaming-to-expose-hidden-harms-and-bi-ases.md)
  * [Stealthy and Persistent Unalignment on Large Language Models via  Backdoor Injections](llm-attack/stealthy-and-persistent-unalignment-on-large-language-models-via-backdoor-injections.md)
  * [IMMUNIZATION AGAINST HARMFUL FINE-TUNING AT TACKS](llm-attack/immunization-against-harmful-fine-tuning-at-tacks.md)
  * [EMULATED DISALIGNMENT: SAFETY ALIGNMENT  FOR LARGE LANGUAGE MODELS MAY BACKFIRE!](llm-attack/emulated-disalignment-safety-alignment-for-large-language-models-may-backfire-1.md)
* [VLM-Defense](vlm-defense/README.md)
  * [Removing NSFW Concepts from Vision-and-Language Models  for Text-to-Image Retrieval and Generation](vlm-defense/removing-nsfw-concepts-from-vision-and-language-models-for-text-to-image-retrieval-and-generation.md)
  * [Image Safeguarding: Reasoning with Conditional Vision Language Model  and Obfuscating Unsafe Content](vlm-defense/image-safeguarding-reasoning-with-conditional-vision-language-model-and-obfuscating-unsafe-content.md)
  * [Towards Safe Self-Distillation of  Internet-Scale Text-to-Image Diffusion Models](vlm-defense/towards-safe-self-distillation-of-internet-scale-text-to-image-diffusion-models.md)
* [VLM](vlm/README.md)
  * [Scalable Performance Analysis for Vision-Language Models](vlm/scalable-performance-analysis-for-vision-language-models.md)
* [VLM-Attack](vlm-attack/README.md)
  * [Circumventing Concept Erasure Methods For  Text-to-Image Generative Models](vlm-attack/circumventing-concept-erasure-methods-for-text-to-image-generative-models.md)
* [Survey](survey/README.md)
  * [Generative AI Security: Challenges and Countermeasures](survey/generative-ai-security-challenges-and-countermeasures.md)
* [LLM-Defense](llm-defense/README.md)
  * [LANGUAGE MODELS ARE HOMER SIMPSON!](llm-defense/language-models-are-homer-simpson.md)
  * [THE POISON OF ALIGNMENT](llm-defense/the-poison-of-alignment.md)
  * [GAINING WISDOM FROM SETBACKS : ALIGNING  LARGE LANGUAGE MODELS VIA MISTAKE ANALYSIS](llm-defense/gaining-wisdom-from-setbacks-aligning-large-language-models-via-mistake-analysis.md)
  * [Making Harmful Behaviors Unlearnable for Large Language Models](llm-defense/making-harmful-behaviors-unlearnable-for-large-language-models.md)
  * [Fake Alignment: Are LLMs Really Aligned Well?](llm-defense/fake-alignment-are-llms-really-aligned-well.md)
  * [Red-Teaming Large Language Models using Chain of  Utterances for Safety-Alignment](llm-defense/red-teaming-large-language-models-using-chain-of-utterances-for-safety-alignment.md)
  * [Vaccine: Perturbation-aware Alignment for Large Language Model](llm-defense/vaccine-perturbation-aware-alignment-for-large-language-model.md)
