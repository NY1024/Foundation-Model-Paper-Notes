# Robust Contrastive Language-Image Pre-training  against Data Poisoning and Backdoor Attacks

<figure><img src="../.gitbook/assets/image (73).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

本研究聚焦于对比视觉-语言表示学习（Contrastive Vision-Language Representation Learning），特别是在零样本分类任务中取得的最新进展。这类模型，如CLIP，通过从互联网上爬取的数百万图像-标题对进行学习，但这些大规模数据集也使得模型极易受到针对性数据投毒和后门攻击的威胁。

### 2. 过去方案和缺点

以往的研究主要集中在如何提高模型性能，而忽视了在预训练阶段对模型进行保护以抵御攻击。尽管有一些方法被提出来在微调阶段保护CLIP模型，但这些方法在预训练阶段并不适用。

<figure><img src="../.gitbook/assets/image (74).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

研究者提出了ROCLIP（Robust CLIP），这是一种新的方法，用于在预训练阶段增强多模态视觉-语言模型（如CLIP）的鲁棒性，以抵御针对性数据投毒和后门攻击。ROCLIP通过保持一个大型且不断变化的随机标题池，并在每个训练周期中将图像与池中最相似的文本匹配，而不是其原始标题，从而打破被投毒的图像-标题对之间的关联。此外，ROCLIP还利用图像和文本增强来进一步加强防御并提高模型性能。



在ROCLIP方法中，随机标题池（Random Caption Pool）和数据增强技术（Data Augmentation）是两个关键组件，它们共同作用以提高模型对攻击的鲁棒性。

#### 随机标题池（Random Caption Pool）

随机标题池是ROCLIP方法的核心部分，其目的是在训练过程中打破被投毒的图像-标题对之间的关联。具体来说，这个池子包含了大量随机选择的标题，这些标题与训练集中的图像没有直接关联。在每个训练周期，ROCLIP会执行以下步骤：

1. **图像和标题增强**：对训练批次中的图像和标题应用增强技术，以增加数据的多样性。
2. **匹配过程**：对于每个图像，ROCLIP不是将其与原始标题匹配，而是在随机标题池中找到与之最相似的标题进行匹配。这种匹配是基于图像和标题的表示向量之间的相似度。
3. **池子更新**：在每个训练周期结束后，ROCLIP会将当前批次中的标题添加到池子中，并丢弃最旧的标题，以保持池子的动态性和多样性。

通过这种方式，ROCLIP能够有效地防止被投毒的图像与恶意标题在表示空间中靠近，从而抵御数据投毒攻击。

#### 数据增强技术（Data Augmentation）

数据增强技术用于增加训练数据的多样性，这有助于模型学习到更泛化的特征表示。在ROCLIP中，数据增强包括对图像和文本的一系列操作：

* **图像增强**：包括随机裁剪、水平翻转、颜色抖动、灰度转换和模糊等。这些操作旨在改变图像的外观，同时保持其内容的一致性。
* **文本增强**：使用自然语言处理技术，如同义词替换、随机交换和随机删除等，来生成与原始标题语义相似但形式不同的新标题。

数据增强不仅有助于提高模型对攻击的鲁棒性，还能提高模型在下游任务上的性能。例如，通过增强，模型可以更好地处理在测试时可能遇到的各种变化，从而提高其泛化能力。

#### 结合效果

随机标题池和数据增强技术的结合使得ROCLIP能够在预训练阶段有效地抵御针对性数据投毒和后门攻击。这种结合策略不仅提高了模型对攻击的抵抗力，还保持了模型在正常训练数据上的性能，甚至在某些情况下还能提升性能。通过这种方式，ROCLIP为大规模视觉-语言模型的安全性提供了一种有效的预训练策略。



### 4. 本文创新点与贡献

* 提出了ROCLIP，这是首个在预训练阶段有效抵御针对性数据投毒和后门攻击的方法。
* 通过使用随机标题池和图像/文本增强，ROCLIP在预训练CLIP模型时显著降低了攻击成功率。
* 实验结果表明，ROCLIP在提高线性探测性能的同时，保持了与CLIP相当的零样本性能。

### 5. 本文实验

实验在多个下游数据集上进行，包括CALTECH101、CIFAR10/100、DTD、FGVCAIRCRAFT等。实验结果显示，ROCLIP在抵御攻击的同时，提高了模型的线性探测准确率，并且在零样本分类任务中与CLIP表现相当。

### 6. 实验结论

ROCLIP能够有效地在预训练阶段保护CLIP模型，使其免受针对性数据投毒和后门攻击的影响。此外，ROCLIP在提高模型性能方面也表现出色，尤其是在线性探测任务中。

### 7. 全文结论

ROCLIP为在预训练阶段保护大规模视觉-语言模型提供了一种有效的解决方案，它通过打破被投毒的图像-标题对之间的关联，并利用数据增强来提高模型的鲁棒性。尽管ROCLIP在抵御非常强的攻击时可能需要牺牲一些性能，但它在保护模型免受攻击的同时，仍然能够保持良好的下游任务性能。

### 阅读总结

本文提出了ROCLIP，一种在预训练阶段增强CLIP模型鲁棒性的方法，以抵御针对性数据投毒和后门攻击。ROCLIP通过随机标题池和数据增强技术，有效地降低了攻击成功率，并在保持零样本性能的同时提高了线性探测性能。这一方法为保护大规模视觉-语言模型免受攻击提供了新的视角，并在实验中证明了其有效性。
