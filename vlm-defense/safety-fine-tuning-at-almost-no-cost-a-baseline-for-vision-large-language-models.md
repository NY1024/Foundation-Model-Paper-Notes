# Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models

<figure><img src="../.gitbook/assets/image (269).png" alt=""><figcaption></figcaption></figure>

#### 研究背景

视觉大型语言模型（VLLMs）结合了视觉和语言能力，展现出显著的多模态能力。然而，这些模型在生成有害内容和对抗性攻击方面存在脆弱性。具体来说，VLLMs在进行视觉-语言指令微调时可能会遗忘之前学习到的安全对齐，导致比基础的大型语言模型（LLMs）更容易受到攻击。

#### 过去方案和缺点

过去的研究集中在通过增强学习（RLHF）等方法来保护LLMs，但这些方法资源密集且难以训练。对于VLLMs，现有的文本中心的安全技术不能直接应用，因为VLLMs面临的风险更大，包括文本输入和视觉-语言输入。此外，现有的VLLMs安全技术并不存在。

#### 本文方案和步骤

为了解决VLLMs的安全问题，本文提出了一种简单而有效的安全微调策略：

1. 收集并策划了一个名为VLGuard的视觉-语言安全指令遵循数据集，覆盖多种有害类别。
2. 通过将该数据集整合到标准的视觉-语言微调中，或用于事后微调，有效对齐VLLMs的安全。
3. 提出了两种VLLM安全对齐策略：事后微调和混合微调。

#### 本文创新点与贡献

* 分析了现有的VLLMs和基础LLMs，展示了流行的VLM指令遵循协议如何使VLLMs比相应的LLMs更容易受到攻击。
* 据作者所知，构建了第一个VLLMs安全微调数据集VLGuard，并提供了测试套件。
* 提出了两种VLLM安全对齐策略，实验结果表明，使用VLGuard数据集和策略可以显著降低初始安全风险，同时增加对多种黑盒攻击的鲁棒性，而不损害有益性。

#### 本文实验

实验包括：

* 使用AdvBench和XSTest评估安全性。
* 使用MMLU和AlpacaEval 2.0评估有益性。
* 对VLGuard数据集进行微调，以提高安全性。
* 通过人类评估来验证微调模型的安全性和有益性。

#### 实验结论

* 微调后的VLLMs能有效地拒绝不安全指令，大幅降低黑盒对抗性攻击的成功率。
* 微调策略在保持有益性的同时，显著提高了模型的安全性。
* 人类评估结果表明，微调模型在安全性方面有显著提升，而在有益性方面与原始模型相当。

#### 全文结论

本文提出了VLGuard数据集和相应的微调策略，有效提高了VLLMs的安全性，同时保持了模型的有益性。尽管这是一个重要的进步，但作者也指出，这种方法可能无法抵御更复杂和精细的攻击方法。

#### 阅读总结报告

**摘要**

本文针对视觉大型语言模型（VLLMs）的安全问题，提出了一种新的安全微调策略和数据集VLGuard。通过微调，VLLMs在保持有益性的同时，显著提高了安全性。

**研究内容**

* **问题识别**：VLLMs在微调过程中会遗忘安全对齐，更容易受到攻击。
* **解决方案**：创建VLGuard数据集，提出事后微调和混合微调策略。
* **实验验证**：通过安全性和有益性测试，验证了微调策略的有效性。

**创新点**

* **VLGuard数据集**：首个专为VLLMs安全微调设计的数据集。
* **微调策略**：简单有效，对模型性能影响小。

**实验结果**

* **安全性提升**：微调后的模型能有效抵御不安全指令和对抗性攻击。
* **有益性保持**：微调对模型的有益性影响不大。

**结论**

VLGuard数据集和微调策略为VLLMs提供了一种有效的安全增强方法。尽管如此，仍需进一步研究以抵御更复杂的攻击。

**建议**

未来的研究应考虑将VLGuard数据集集成到VLLMs的训练中，并探索更强大的安全机制。同时，随着VLLMs的不断发展，持续评估和更新安全措施至关重要。
