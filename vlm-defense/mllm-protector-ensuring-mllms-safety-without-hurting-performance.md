# MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 研究背景

随着大型语言模型（LLMs）的发展，多模态大型语言模型（MLLMs）也得到了快速的进步。这些模型能够处理图像输入，扩展了LLMs的应用范围。然而，MLLMs在面对恶意视觉输入时表现出了独特的脆弱性，容易受到攻击并产生有害的响应。现有的防御策略主要针对文本输入，而对图像输入的防御相对较少。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>



## 过去方案和缺点

过去的研究主要集中在文本输入的LLMs上，提出了各种防御策略，如输入检测、上下文学习和显式对齐模型与对抗性示例。这些策略在文本模型上取得了一定的成效，但在MLLMs上的应用面临挑战。图像信号的连续性质导致了更大的输入空间和更复杂的对齐需求，使得传统的对齐方法难以适应。此外，MLLMs通常在有限的图像-文本对上进行微调，这可能导致模型在对齐过程中遗忘原有的能力。

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 本文方案和步骤

本文提出了MLLM-Protector，一个即插即用的策略，结合了一个轻量级的伤害检测器和一个响应净化器。伤害检测器负责识别MLLM可能产生的有害输出，而响应净化器则对这些输出进行修正，确保响应符合安全标准。这种方法在不损害模型整体性能的情况下，有效降低了恶意视觉输入带来的风险。

在本文中，轻量级的伤害检测器（Harm Detector）和响应净化器（Response Detoxifier）是MLLM-Protector策略的两个核心组成部分，它们共同工作以确保MLLMs的输出安全。

#### 轻量级的伤害检测器（Harm Detector）

伤害检测器的作用是识别MLLM生成的输出中是否包含有害内容。这个检测器是基于预训练的大型语言模型（LLM），通过替换最后一层为一个线性层，使其适应二分类任务，即判断输出内容是否有害。伤害检测器接收MLLM的响应作为输入，并预测一个表示有害程度的分数。

在训练过程中，使用二元交叉熵（Binary Cross Entropy, BCE）损失函数来训练伤害检测器。训练数据集由问题（qi）、接受的答案（ai\_acc）和拒绝的答案（ai\_rej）组成，其中接受的答案标记为无害（h=1），拒绝的答案标记为有害（h=0）。伤害检测器仅使用MLLM的响应作为输入。

#### 响应净化器（Response Detoxifier）

响应净化器的目标是修正MLLM生成的有害内容，使其变得无害。与简单地用固定句子替换有害响应不同，响应净化器通过微调一个语言模型来实现这一目标，以确保修正后的响应既无害又与查询相关。

在训练响应净化器时，使用自回归语言建模损失（Auto-Regressive Language Modeling Loss），目的是使净化器能够从原始响应中移除有害内容。训练目标是让净化器在给定用户查询和有害答案的情况下，生成无害的答案。训练过程中，净化器学习如何根据用户查询和有害答案生成修正后的答案。

#### 训练和推理

在训练阶段，伤害检测器和响应净化器分别使用不同的数据集进行训练。伤害检测器使用带有接受和拒绝答案的QA数据集，而响应净化器则使用由ChatGPT生成的新QA对。这些数据集包含了各种场景下的接受和拒绝答案。

在推理阶段，MLLM生成的输出首先被传递给伤害检测器。如果检测到有害内容，输出将被传递给响应净化器进行修正。整个过程通过一个循环进行，直到不再检测到有害内容。

#### 总结

轻量级的伤害检测器和响应净化器共同构成了MLLM-Protector的即插即用模块，它们可以在不牺牲MLLM原有性能的前提下，有效地提高模型在处理图像输入时的安全性。这种策略为MLLMs的安全防御提供了一种新的解决方案，有助于减少恶意攻击的风险。

## 本文创新点与贡献

* 提供了对MLLMs图像输入相关脆弱性的分析。
* 引入了MLLM-Protector，这是一个为MLLMs设计的即插即用防御机制。
* 通过实证证据展示了该方法能有效减轻恶意图像输入引起的有害输出风险，同时保持模型的原始性能。

## 本文实验

实验在MM-SafetyBench上进行，该基准包含13种常见场景的恶意意图示例。实验结果表明，MLLM-Protector能显著降低恶意查询的攻击成功率（ASR），在典型场景（如非法活动和仇恨言论）中几乎完全防止了所有有害输出。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 通过对实验数据的分析得到了哪些结论

* MLLM-Protector能够有效区分有害和无害的响应。
* 预训练的LLM质量越高，伤害检测器的性能越好。
* MLLM-Protector在所有场景中都能显著降低ASR，证明了其在不同MLLMs上的普适性和有效性。

## 结论

MLLM-Protector为多模态大型语言模型的安全问题提供了一个有效的解决方案。通过集成伤害检测器和响应净化器，这种方法在不损害模型性能的同时，确保了MLLMs的安全性。这项工作不仅引起了对MLLMs安全问题的重视，也为未来在这一领域的研究提供了启发。





## 阅读总结报告

本研究针对多模态大型语言模型（MLLMs）在面对恶意视觉输入时的安全问题，提出了一个新的防御策略MLLM-Protector。该策略通过伤害检测器和响应净化器的结合，有效地提高了MLLMs在处理图像输入时的安全性，同时保持了模型的性能。实验结果表明，MLLM-Protector在降低攻击成功率方面表现出色，尤其是在处理非法活动和仇恨言论等敏感场景时。这项工作不仅为MLLMs的安全防御提供了新的视角，也为未来的研究和实践奠定了基础。
