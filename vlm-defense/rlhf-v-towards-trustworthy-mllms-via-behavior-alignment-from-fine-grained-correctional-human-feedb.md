# RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from  Fine-grained Correctional Human Feedb

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) ( (9).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 多模态大型语言模型（MLLMs）在多模态理解、推理和交互方面展现出了令人印象深刻的能力。然而，现有的MLLMs普遍存在严重的幻觉问题，即生成的文本与相关图像中的事实基础不符。这使得MLLMs在现实世界（尤其是高风险）应用中不可靠且不实用。
2. 过去方案和缺点： 以往的MLLMs通常通过指令调整（instruction tuning）进行微调，以模仿演示数据中的行为。但这种方法存在两个主要问题：（1）注释歧义，由于响应的细粒度特性，通常很难决定哪个响应更优；（2）学习效率，粗粒度的排名反馈使得难以准确地将信用分配给期望的行为，通常需要大量的标记数据来学习。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1)   (7).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤： 本文提出了RLHF-V框架，通过从细粒度的纠正性人类反馈中对行为进行对齐，来增强MLLM的可信度。具体步骤包括：（1）收集人类对幻觉的段级纠正反馈；（2）执行密集的直接偏好优化（DDPO）以优化人类反馈；（3）通过DDPO直接优化策略模型，对抗密集且细粒度的段级偏好，其中幻觉部分获得更强的反馈以确保事实基础。
2. 本文创新点与贡献： （1）提出了RLHF-V框架，通过细粒度的纠正性人类反馈对MLLM行为进行对齐；（2）收集了高质量的人类偏好数据，为MLLMs提供了与人类对齐的学习信号；（3）通过全面实验展示了所提出框架的有效性，实现了开源MLLMs中可信度的最新性能。
3. 本文实验： 在五个基准测试上进行了自动和人类评估的全面实验，包括对象幻觉、响应信息量、多模态对话、详细描述和复杂推理。实验结果表明，RLHF-V能够显著降低基础MLLM的幻觉率，并在防止过度泛化引起的幻觉方面比GPT-4V表现出更好的鲁棒性。

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1)  (17).png" alt=""><figcaption></figcaption></figure>

1. 通过对实验数据的分析得到的结论： （1）RLHF-V在保持响应有帮助性的同时，显著提高了MLLM的可信度；（2）细粒度的纠正性人类反馈提供了有效且高效的学习信号，有助于MLLM行为对齐；（3）RLHF-V的数据和方法可以应用于其他MLLMs，以提高其可信度；（4）人类反馈通过提供清晰的、密集的偏好反馈，有助于减少幻觉。
2. 结论： 幻觉是阻碍MLLMs在现实世界场景中实际应用的关键问题。本文提出的RLHF-V框架通过从细粒度的纠正性人类反馈中对行为进行对齐，显著提高了MLLM的可信度。未来的工作将探索从更可信和有能力的MLLMs中收集准确的偏好，以促进大规模偏好学习，实现更强的行为对齐。

阅读总结报告： 本文针对MLLMs在现实世界应用中的幻觉问题，提出了一种新的框架RLHF-V，通过收集和利用细粒度的人类纠正性反馈，有效地提高了模型的可信度。实验结果表明，RLHF-V在减少幻觉、保持响应有帮助性以及提高数据和计算效率方面取得了显著成效。此外，该框架不仅适用于特定的MLLM，而且可以推广到其他模型，显示出其在提高MLLMs可信度方面的潜力。未来的研究将探索如何利用更先进的MLLMs来收集和利用偏好数据，以实现更强大的行为对齐。
