# AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Sh

<figure><img src="../.gitbook/assets/image (272).png" alt=""><figcaption></figcaption></figure>

#### 研究背景

随着多模态大型语言模型（MLLMs）的出现和广泛部署，确保它们的安全性变得越发重要。MLLMs 通过集成额外的模态，暴露于新的漏洞之下，特别是结构化基于攻击，攻击者将语义内容（例如，“有害文本”）注入图像中，误导 MLLMs。

#### 过去方案和缺点

以往的研究主要关注于对抗性扰动攻击和结构化攻击。对抗性扰动攻击通过创建对抗性扰动来破坏 MLLMs 的安全对齐，而结构化攻击则通过排版或文本到图像池将有害内容转换为图像，绕过 MLLMs 的安全对齐。现有的对抗性防御，例如净化器或对抗性训练已被证明是有效的。然而，结构化攻击利用 MLLM 的独特性，提出了新的挑战，现有的对抗性防御，如净化器，其有效性大大降低。

<figure><img src="../.gitbook/assets/image (273).png" alt=""><figcaption></figcaption></figure>

#### 本文方案和步骤

为了防御此类威胁，本文提出了一种名为 Adaptive Shield Prompting（AdaShield）的新方法。AdaShield 通过在输入前添加防御性提示（defense prompts）来保护 MLLMs 免受结构化越狱攻击，无需对 MLLMs 进行微调或训练额外的模块。首先，作者提出了一个手动设计的静态防御提示，并进一步引入了一个自适应自动优化框架，包括目标 MLLM 和基于 LLM 的防御提示生成器（Defender）。这些组件通过协作和迭代通信来生成防御提示。

#### 本文创新点与贡献

1. 提出了 AdaShield，一个新颖的防御框架，它自动和自适应地将防御提示添加到模型输入中，确保有效防护，无需微调或训练额外的模型。
2. 为了超越简单使用手动设计的防御提示，进一步开发了一个自动优化框架，利用目标 MLLMs 和防御者迭代优化防御提示，生成多样化且符合特定安全指南的提示池。
3. 证明了 AdaShield 在防御结构化越狱攻击的同时，保持了模型在标准良性数据集上的性能。

#### 本文实验

作者在流行的结构化越狱攻击（FigStep 和 QR）和良性数据集上进行了广泛的实验。实验结果显示 AdaShield 能够一致地提高 MLLMs 对结构化越狱攻击的鲁棒性，同时在标准良性任务上不损害模型的一般能力。

#### 实验结论

AdaShield-A 在不牺牲模型在标准良性任务上的性能的同时，实现了优越的防御性能，而 AdaShield-S 由于缺乏特定安全规则，其防御性能不如 AdaShield-A。此外，AdaShield-A 通过基于相似性的过滤，成功减轻了过度防御的问题。

#### 全文结论

本文提出的 AdaShield 是一种有效的 MLLMs 保护机制，能够抵御结构化越狱攻击，同时保持了模型的一般能力，展示了其作为提高 MLLMs 安全性的即插即用解决方案的潜力。

#### 阅读总结报告

这篇论文提出了一个针对多模态大型语言模型的防御性框架 AdaShield，旨在保护模型免受结构化攻击的影响。作者首先分析了 MLLMs 面临的安全威胁，尤其是结构化越狱攻击，并讨论了现有防御策略的局限性。随后，文章详细介绍了 AdaShield 的设计和实现，包括手动设计的静态防御提示和自动优化框架。通过一系列实验，验证了 AdaShield 在提高模型鲁棒性方面的效果，同时保持了模型在良性数据集上的性能。文章的创新之处在于无需对模型进行微调或增加额外模块，即可实现有效的安全防护。最后，作者指出 AdaShield 专门针对结构化越狱攻击，未来工作将探索更普遍的防御框架。



注：

#### AdaShield 防御机制详细说明

**AdaShield** 是一种针对多模态大型语言模型（MLLMs）的结构化越狱攻击的防御机制。这种攻击通过在图像中注入恶意文本或项目，企图误导 MLLMs 产生不当的响应。为了防御此类攻击，AdaShield 采用了在模型输入前添加防御性提示（defense prompts）的方法。以下是 AdaShield 工作流程的详细说明：

1. **防御性提示的概念**：
   * 防御性提示是一系列指导性的问题或语句，它们被添加到用户输入之前，目的是引导 MLLMs 在处理输入时遵循特定的安全规则。
2. **静态防御提示（AdaShield-Static, AdaShield-S）**：
   * 初始阶段，AdaShield 使用一个手动设计的静态防御提示。这个提示详细检查图像和指令内容，并为恶意查询指定响应方法。
   * 例如，如果检测到图像中包含有害文本或项目，模型将被指示拒绝参与或执行任何内容，并以“对不起，我不能提供任何非法活动的提示。”作为回应。
3. **自适应自动优化框架（AdaShield-Adaptive, AdaShield-A）**：
   * 为了提高对复杂攻击场景的防御效果，AdaShield 进一步引入了一个自适应的自动优化框架。
   * 这个框架由一个目标 MLLM 和一个基于 LLM 的防御提示生成器（Defender）组成，它们通过对话交互协作并迭代优化防御提示。
4. **训练阶段**：
   * 在训练阶段，AdaShield-A 通过以下几个关键步骤生成防御提示池：
     1. **越狱响应生成**：收集少量恶意查询作为训练样本，如果 MLLM 生成的响应包含有害、非法或敏感内容，则标记为越狱响应。
     2. **自动优化**：Defender 生成一个候选防御提示，旨在保护 MLLM 免受恶意查询引起的越狱。
     3. **越狱判断**：评估新响应是否为越狱响应。
     4. **迭代优化**：如果新响应仍然是越狱响应，Defender 将基于失败的防御提示和越狱响应生成新的防御提示。
     5. **验证和修复**：通过一个小的验证集来筛选具有普遍性的防御提示，并增加提示池的多样性和全面性。
5. **推理阶段**：
   * 在推理阶段，给定一个文本查询，AdaShield 首先获取其文本嵌入和图像嵌入，然后基于归一化的嵌入相似度检索最“适合”的防御提示，并将其添加到输入查询前。
6. **防御效果**：
   * AdaShield 通过这些方法有效地增强了 MLLMs 的安全性，使其能够抵御结构化越狱攻击，同时保持了模型在标准良性任务上的性能。

通过上述机制，AdaShield 能够在不牺牲 MLLMs 通用能力的前提下，提供一种即插即用的解决方案，以提高 MLLMs 的安全性。
