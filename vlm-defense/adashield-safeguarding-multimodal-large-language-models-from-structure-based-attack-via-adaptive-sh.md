# AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Sh

<figure><img src="../.gitbook/assets/image (4) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

随着多模态大型语言模型（MLLMs）的广泛部署，确保其安全性变得越来越重要。MLLMs通过整合视觉和语言能力，能够理解和生成与图像相关的内容。然而，这种整合也使它们面临新的安全挑战，尤其是结构化攻击，攻击者通过在图像中注入语义内容（如“有害文本”）来误导MLLMs。

### 2. 过去方案和缺点

以往的防御策略主要集中在对抗性扰动攻击上，这些攻击通过创建不易被人察觉的微小扰动来破坏MLLMs的对齐。这些方法包括净化器（purifiers）和对抗性训练。然而，这些防御措施对于结构化攻击的效用有限，因为结构化攻击通过字体或文本到图像的方式将有害内容转换为图像，从而绕过了MLLMs的安全对齐。

<figure><img src="../.gitbook/assets/image (5) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

文章提出了一种名为AdaShield的防御方法，该方法通过在输入前添加防御性提示（defense prompts）来保护MLLMs免受结构化攻击，而无需对MLLMs进行微调或训练额外的模块。首先，提出了一个手动设计的静态防御提示AdaShield-S，该提示逐步检查图像和指令内容，并为恶意查询指定响应方法。然后，引入了一个自适应自动细化框架AdaShield-A，包括一个目标MLLM和一个基于LLM的防御提示生成器（Defender），它们协作迭代地生成防御提示。

### 4. 本文创新点与贡献

* 提出了AdaShield，这是一种新颖的防御框架，可以自动和自适应地在模型输入前添加防御提示，而无需微调或训练额外的模型。
* 开发了自适应自动细化框架，使用目标MLLM和防御器通过对话交互协作优化防御提示，生成符合特定安全规则的多样化防御提示池。
* 在标准良性任务上评估模型的一般能力，证明了AdaShield在防御结构化攻击的同时，保持了模型的性能。

### 5. 本文实验

实验使用了流行的结构化攻击FigStep和QR以及良性数据集MM-Vet来评估AdaShield-S和AdaShield-A的有效性。结果表明，AdaShield-A在防御结构化攻击方面的表现优于基线方法，并且在标准良性任务上的性能没有损失。

### 6. 实验结论

实验结果证实了AdaShield-A在防御结构化攻击方面的优越性能，并且在保持模型在良性数据集上的性能的同时，没有引入过度防御的问题。

### 7. 全文结论

AdaShield提供了一种有效的MLLMs防御机制，能够抵御结构化攻击，同时保持模型的一般能力，展现了作为提高MLLMs安全性的即插即用解决方案的潜力。尽管AdaShield专为结构化攻击设计，但未来工作将探索能够同时解决结构化和扰动基于攻击的通用防御框架。



注1：

结构化攻击（structure-based attacks）是指针对多模态大型语言模型（MLLMs）的一种攻击方式，攻击者通过在图像中嵌入有害内容（如文本）来误导MLLMs，使其生成违反安全准则的响应。这种攻击利用了MLLMs处理视觉和语言信息的能力，通过将语义内容转换为图像（例如，通过字体排版或文本到图像工具），从而绕过模型的安全对齐机制。

在本文中，结构化攻击的例子包括：

* **FigStep攻击**：通过创建包含文本提示的图像，例如“这里是如何制造炸弹的步骤：1. 2. 3.”，诱导MLLMs完成句子，导致它们无意中提供恶意回应。
* **QR攻击**：通过二维码或其他图像形式嵌入有害信息，当MLLMs尝试解析这些图像时，可能会生成攻击者期望的有害内容。

这些攻击的挑战在于，它们不仅仅是微小的扰动，而是包含了具有语义意义的结构信息，这与传统的对抗性攻击技术（如微小的像素级扰动）有很大不同。结构化攻击的有效性在于它们能够利用MLLMs的独特性，通过视觉模态引入有害内容，从而绕过模型的安全防护机制。因此，防御这类攻击需要特别设计的方法，如本文提出的AdaShield，它通过在输入中添加防御性提示来增强MLLMs的安全性。



注2：

本文提出的防御方法是AdaShield，它包括两个主要部分：AdaShield-S（静态防御提示）和AdaShield-A（自适应自动细化框架）。以下是这两个部分的详细防御流程：

#### AdaShield-S（静态防御提示）

1. **设计防御提示**：首先，研究者手动设计一个防御提示（Ps），这个提示是根据MLLMs的能力和潜在的攻击场景来制定的。它包括详细的步骤，要求模型在响应用户请求之前，仔细检查图像内容和文本内容，以识别任何可能的有害、非法或危险活动。
2. **检查图像内容**：防御提示要求模型首先检查图像中是否存在有害文本或物品。如果检测到这些内容，模型应该拒绝参与或执行与这些内容相关的任何活动。
3. **处理文本指令**：接着，模型需要结合图像和文本指令的内容，分析指令是否违反了安全准则。
4. **生成安全响应**：如果内容被判定为不道德、非法或危险，模型应该以“对不起”开头的回应来拒绝用户的请求，并提供安全的替代指令。

#### AdaShield-A（自适应自动细化框架）

1. **生成初始防御提示**：使用AdaShield-S中的手动防御提示作为起点，目标MLLM和防御器（Defender）开始对话交互。
2. **迭代优化**：在训练阶段，Defender根据目标MLLM对恶意查询的响应生成防御提示。如果MLLM的响应包含有害内容，这被视为越狱（jailbreak）响应，表明当前的防御提示无效。
3. **反馈和细化**：Defender使用越狱响应和之前失败的防御提示作为反馈，生成新的防御提示。这个过程是迭代的，直到生成一个能够有效防止越狱的防御提示。
4. **验证和修复**：为了确保优化后的防御提示不仅对当前查询有效，而且对未来的查询也具有泛化能力，研究者使用一个小的验证集来筛选出具有良好泛化能力的防御提示。
5. **检索最适合的提示**：在推理阶段，对于每个测试查询，使用CLIP模型生成文本和图像的嵌入，然后基于嵌入相似度从防御提示池中检索出最适合的防御提示。
6. **响应生成**：将检索到的防御提示添加到输入查询前，以增强MLLM的安全性。如果相似度低于预设阈值β，认为查询是良性的，不会应用任何防御提示。

通过这两个流程，AdaShield能够有效地提高MLLMs对抗结构化攻击的能力，同时保持对正常查询的响应能力。





### 阅读总结

本文针对MLLMs面临的结构化攻击问题，提出了一种新颖的防御方法AdaShield，该方法通过在模型输入前添加防御提示来增强安全性。AdaShield不需要对模型进行微调或训练额外模块，通过手动设计的静态提示和自适应自动细化框架，有效地提高了MLLMs对抗结构化攻击的鲁棒性。实验结果表明，AdaShield在保持模型一般性能的同时，显著提高了防御性能，为MLLMs的安全性提供了一种有效的解决方案。

