# Partially Recentralization Softmax Loss for Vision-Language Models Robustness

<figure><img src="../.gitbook/assets/image (6) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 随着大型语言模型在自然语言处理任务（NLP）中取得突破，多模态技术变得极为流行。然而，多模态NLP模型已被证明容易受到对抗性攻击的影响，即通过输入的微小扰动可以显著改变模型的输出。尽管在计算机视觉和NLP模型中提出了几种防御技术，但模型的多模态鲁棒性尚未得到充分探索。
2. 过去方案和缺点： 最常见的防御方法是对抗性训练，它通过在训练过程中引入精心设计的干扰数据（即“对抗性示例”）来提高模型的鲁棒性。然而，对抗性训练存在一些严重的缺点，包括训练时间长、可能降低模型的泛化能力，以及对抗性训练的效果可能仅限于特定的攻击场景。
3. 本文方案和步骤： 本文提出了一种新的损失函数，通过在微调过程中限制前K个softmax输出来减轻模型的脆弱性问题。通过一系列评估方法，作者观察到该方法在多个预训练模型和任务上提供了对视觉对抗性示例的隐式鲁棒性。
4. 本文创新点与贡献：

* 提出了一种新的损失函数（Partially Recentralization Softmax Loss, PRSL），用于提高视觉-语言模型（VLMs）的鲁棒性。
* 通过实验表明，经过微调后，预训练模型的对抗性鲁棒性可以显著提高，对抗常见的攻击。
* 未来的研究应该关注输出多样性、泛化能力以及这种损失函数的鲁棒性-性能权衡。

5. 本文实验和性能： 实验在两个常用的视觉-语言预训练模型上进行：ViT-GPT2（用于图像描述任务）和Blip2-OPT（用于视觉问答任务）。使用COCO2017和Visual Genome数据集进行微调，并使用BIM（Black-box Iterative Method）作为非目标对抗性攻击方法。实验结果表明，使用PRSL微调的模型在所有阈值下都优于基线模型。
6. 结论： 本文提出了一种通过限制损失函数中的softmax顶部K值来实现模型鲁棒性的方法。在多个视觉-语言预训练模型上的实验表明，该方法可以系统地提高模型的鲁棒性。未来将进行更深入的研究，包括适应性选择顶部K值、模型准确性与鲁棒性之间的权衡等方面。

阅读总结报告： 本文针对多模态NLP模型在对抗性攻击下的脆弱性问题，提出了一种新的损失函数PRSL，以增强模型的鲁棒性。通过在ViT-GPT2和Blip2-OPT模型上的实验，作者证明了PRSL能够有效提高模型对抗视觉对抗性示例的鲁棒性。尽管该方法在提高鲁棒性方面取得了显著成效，但仍存在一些局限性，如固定顶部K值的设置、对大型模型的适用性以及对模型性能影响的不确定性。未来的工作将探索更自然有效的适应性方法，并深入研究模型鲁棒性与性能之间的权衡。
