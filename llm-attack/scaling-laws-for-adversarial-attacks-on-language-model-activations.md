# Scaling Laws for Adversarial Attacks on Language  Model Activations

<figure><img src="../../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

本文探讨了针对语言模型激活的对抗性攻击。这类攻击通过操纵模型激活的较小子集，能够控制模型对大量后续标记的精确预测。这种攻击对深度神经网络，尤其是最新的视觉和语言模型，构成了重大挑战，因为它们可能在模型输入中引入小的、有针对性的扰动，从而对模型输出和行为产生巨大影响。

<figure><img src="../../.gitbook/assets/image (5) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 2. 过去方案和缺点

以往的研究主要集中在图像分类器的对抗性攻击上，而对语言模型的对抗性攻击研究较少。此外，大多数研究关注于攻击图像分类器的输入标记，而不是模型的内部激活。

### 3. 本文方案和步骤

本文通过实验验证了一个缩放律，即目标标记的最大数量（tmax）与攻击者控制的激活标记数量（a）成正比。研究者们还发现，在输入空间中控制单个比特所需的比特数（称为攻击抵抗力χ）在不同大小的语言模型中保持相对恒定。此外，本文比较了直接攻击激活和通过标记替换进行攻击的效果。

### 4. 本文创新点与贡献

* 提出了一种新的对抗性攻击方法，通过操纵语言模型的激活来控制模型输出。
* 发现了攻击者控制的输入空间维度与输出空间维度之间的线性关系。
* 揭示了对抗性攻击可能是输入和输出空间维度不匹配的结果。
* 对比了攻击激活和攻击标记的效果，发现两者在攻击强度上具有相似性。

### 5. 本文实验

实验在不同大小的语言模型上进行，包括从33M到2.8B参数的模型。实验结果支持了提出的缩放律，并揭示了攻击抵抗力χ的一致性。

### 6. 实验结论

实验结果表明，通过控制少量的模型激活，攻击者可以精确地控制大量后续标记的预测。此外，攻击抵抗力χ在不同模型大小之间保持相对恒定，这支持了对抗性脆弱性与输入输出空间维度不匹配的假设。

### 7. 全文结论

本文的研究强调了语言模型在对抗性攻击面前的脆弱性，并为防御这类攻击提供了新的视角。研究结果对于理解语言模型的架构优势和弱点，以及确保AI系统的安全性和可靠性具有重要意义。

### 阅读总结

本文通过系统地研究语言模型的对抗性攻击，揭示了模型在处理激活时的潜在脆弱性。研究发现，通过操纵少量的激活，攻击者可以有效地控制模型的输出，这一发现对于设计更安全的AI系统具有重要意义。此外，本文还提出了一种新的攻击方法，并通过实验验证了其有效性，为未来在这一领域的研究提供了新的方向。
