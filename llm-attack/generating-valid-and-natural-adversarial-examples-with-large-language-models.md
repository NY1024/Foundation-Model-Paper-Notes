# Generating Valid and Natural Adversarial Examples  with Large Language Models

<figure><img src="../.gitbook/assets/image (6) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 深度学习在自然语言处理（NLP）领域取得了显著成就，尤其是预训练语言模型（PLMs）在多种下游任务中实现了最先进的性能。然而，这些模型被发现容易受到对抗性攻击，即在原始输入上施加微小的扰动可能导致模型做出错误的预测。这种脆弱性促使研究者评估和提高NLP模型的鲁棒性、安全性和可解释性。现有的文本对抗性攻击研究主要分为字符级、单词级、句子级和多级对抗性攻击，其中单词级对抗性攻击因其在保持对抗性示例的语义相似性和不可见性方面的能力而广受欢迎。
2. 过去方案和缺点： 尽管单词级对抗性攻击模型在生成对抗性示例方面取得了一定的成功，但许多生成的对抗性示例既不有效也不自然。这些示例可能无法保持语义意义、语法正确性，且容易被人类察觉。例如，使用WordNet和BERT进行的同义词替换可能导致语义意义的丧失和语法错误。此外，现有方法在处理命名实体和代词时存在问题，可能导致语义变化、语法错误和人类可感知性。
3. 本文方案和步骤： 本文提出了LLM-Attack，旨在利用大型语言模型（LLMs）生成既有效又自然的对抗性示例。该方法包括两个阶段：单词重要性排名（寻找最易受攻击的单词）和单词同义词替换（用LLMs获得的同义词替换它们）。LLM-Attack利用LLMs在语言理解和生成方面的卓越能力，生成的同义词列表通常既有效又自然，同时保持语义意义和语法正确性。

<figure><img src="../.gitbook/assets/image (7) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

<figure><img src="../.gitbook/assets/image (8) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文实验和性能： 作者在电影评论（MR）、IMDB和Yelp评论极性数据集上对LLM-Attack进行了实验，并与现有的对抗性攻击模型（如TEXTFOOLER和BAE）进行了比较。实验结果表明，LLM-Attack在人类和GPT-4评估中显著优于基线模型，能够生成通常有效且自然的对抗性示例，同时保持语义意义、语法正确性和人类不可感知性。此外，LLM-Attack在自动评估中的成功率也显示出其有效性。

阅读总结报告： 本文针对深度学习在NLP领域的对抗性攻击问题，提出了一种新的方法LLM-Attack，该方法利用大型语言模型（LLMs）生成既有效又自然的对抗性示例。通过单词重要性排名和同义词替换两个阶段，LLM-Attack能够有效地生成符合人类感知、语义相似性和语言流畅性的对抗性示例。实验结果表明，LLM-Attack在多个数据集上的表现优于现有的对抗性攻击模型，并且在人类和GPT-4评估中也显示出更高的有效性、有效性和自然性。这项工作不仅为文本对抗性攻击研究提供了新的视角，也为提高NLP模型的鲁棒性提供了一种可能的途径。未来的工作将探索使用生成的对抗性示例进行对抗性训练，以及将结构化知识整合到对抗性示例生成中的可能性。
