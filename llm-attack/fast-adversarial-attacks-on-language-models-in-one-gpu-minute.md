# Fast Adversarial Attacks on Language Models In One GPU Minute

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

随着语言模型（LMs）在各种任务中的应用日益广泛，例如问答和自动代码生成，它们的安全性和有效性变得尤为重要。然而，研究表明，即使是经过对齐处理的LMs也可能被操纵，从而产生不安全和无效的输出。此外，LMs容易受到幻觉攻击，即产生事实上不正确或无意义的内容。因此，研究如何对抗性地攻击LMs，以及如何提高它们的安全性和隐私保护能力，成为了一个重要课题。

### 2. 过去方案和缺点

以往的研究主要集中在图像领域的对抗性攻击，而对于LMs的攻击研究相对较少。一些工作尝试通过手动编写提示或使用基于梯度的优化技术来攻击LMs，但这些方法存在效率低下、成本高昂或难以自动化等问题。

<figure><img src="../.gitbook/assets/image (5) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种名为BEAST（Beam Search-based Adversarial Attack）的新型对抗性攻击方法，该方法基于beam search，无需梯度信息，能够在一分钟内利用单个GPU对LMs进行有效的攻击。BEAST使用可解释的超参数，允许攻击者在攻击速度、成功率和对抗性提示的可读性之间进行权衡。该方法的主要步骤包括初始化beam、迭代地生成对抗性令牌，并使用对抗性目标函数来评估和选择最佳的对抗性提示。



BEAST（Beam Search-based Adversarial Attack）是一种针对语言模型（LMs）的快速对抗性攻击方法，它基于beam search算法，能够在资源有限的环境下（如单个GPU）高效地生成对抗性提示。以下是BEAST方法的详细说明：

#### 攻击目标

BEAST的目标是在保持对抗性提示的可读性的同时，找到一种方式来最小化一个给定的对抗性目标函数。这个目标函数通常与LM生成的输出的某些属性有关，例如输出的困惑度（perplexity）。

#### 攻击步骤

1. **初始化**: BEAST通过从LM中采样一定数量的token（称为beam的大小）来初始化攻击。这些token基于给定输入（例如，系统提示和用户提示的组合）的下一个token的概率分布进行采样。
2. **迭代攻击**: 在每次迭代中，BEAST会扩展beam中的候选元素，通过为每个beam元素评估下一个token的多个选项（称为beam的宽度）。这些选项是通过基于LM预测的概率分布进行多项式采样得到的。
3. **评估与选择**: 对于每个候选元素，BEAST计算其对抗性目标函数的得分，并选择得分最低的候选元素进入下一轮迭代。这个过程会重复进行，直到达到预定的迭代次数或满足某些停止条件。
4. **生成对抗性提示**: 最终，BEAST会从迭代过程中选择得分最低的对抗性提示作为攻击结果。

#### 关键特性

* **无需梯度信息**: 与基于梯度的攻击方法不同，BEAST不需要计算或访问LM的梯度信息，这使得它在黑盒设置下也能有效地工作。
* **可解释的超参数**: BEAST使用可解释的超参数（例如beam的大小和宽度），这些参数可以调整以在攻击速度、成功率和对抗性提示的可读性之间进行权衡。
* **快速执行**: BEAST的设计允许它在有限的时间内（例如一分钟内）完成攻击，这对于快速评估LMs的安全性非常有用。

#### 攻击应用

BEAST可以用于多种攻击场景，包括但不限于：

* **监狱突破（Jailbreaking）**: 通过生成对抗性提示来诱导LM生成有害内容。
* **引发幻觉（Hallucination）**: 使LM产生与现实不符或无意义的输出。
* **隐私攻击（Privacy Attacks）**: 通过生成特定的对抗性提示来提高现有成员推断攻击方法的性能。

BEAST的设计使其成为一种灵活且强大的工具，能够在不同的攻击场景中有效地评估和挑战LMs的安全性。





### 4. 本文创新点与贡献

* 提出了BEAST，这是一种快速、基于beam search的对抗性攻击方法，可以在资源受限的环境下高效地攻击LMs。
* BEAST能够在一分钟内对Vicuna-7B-v1.5等模型进行针对性攻击，成功率达到89%，相比基于梯度的基线方法在时间上有显著优势。
* 发现BEAST的非针对性攻击能够在LM聊天机器人中引发幻觉，通过人类评估发现，BEAST可以使LM产生约15%的错误输出。
* 展示了BEAST可以在短时间内生成对抗性提示，以提高现有成员推断攻击方法的性能。

### 5. 本文实验

实验部分，作者对多种聊天型LMs进行了攻击测试，包括Vicuna-7B-v1.5、Vicuna-13B-v1.5等，并使用AdvBench Harmful Behaviors数据集进行攻击成功率（ASR）的评估。此外，还进行了人类评估和GPT-4评估来衡量BEAST引发的幻觉程度，并对现有成员推断攻击工具进行了性能提升测试。

### 6. 实验结论

实验结果表明，BEAST在攻击速度、成功率和对抗性提示的可读性方面均优于现有方法。此外，BEAST能够有效地引发LM的幻觉，并提高成员推断攻击的性能。

### 7. 全文结论

本文提出的BEAST方法为LM安全性和隐私研究提供了一种新的工具，它能够在资源受限的环境下快速有效地进行对抗性攻击。BEAST的成功不仅展示了LMs的潜在脆弱性，也为未来研究如何进一步保护LMs提供了新的方向。

### 阅读总结

本文介绍了一种新的对抗性攻击方法BEAST，它能够在一分钟内对LMs进行有效的攻击，且无需梯度信息。BEAST通过使用可解释的超参数，在攻击速度、成功率和对抗性提示的可读性之间提供了一种权衡。实验结果表明，BEAST在攻击LMs方面优于现有方法，并且能够引发幻觉和提高成员推断攻击的性能。这项工作不仅揭示了LMs的潜在脆弱性，也为未来的安全研究提供了宝贵的启示。
