# Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>



#### 1. 研究背景

本研究探讨了联邦学习（FL）在大型语言模型（LLM）指令调整（FedIT）中的安全性问题。FL允许多方在不直接共享数据的情况下共同微调一个大型语言模型，理想状态下，通过在与人类偏好和安全原则一致的分散数据上训练，FedIT可以使LLM表现出有益和安全的行为。然而，本文首次揭示了FedIT在安全性对齐方面的脆弱性。

#### 2. 过去方案和缺点

以往的研究集中在FL的聚合算法上，通过模型级别的比较来识别和减轻恶意客户端的影响。然而，这些方法在面对本文提出的安全攻击时显示出了不足，因为恶意客户端的训练数据与正常客户端在优化目标上具有相似性，使得仅通过模型参数难以区分。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### 3. 本文方案和步骤

为了攻击FedIT系统，恶意客户端可以自动生成攻击数据，并在这些数据上训练他们的本地LLM。具体步骤包括：

* 生成恶意指令。
* 对恶意指令生成不安全的响应以形成未对齐数据。
* 将本地LLM上传到服务器进行模型聚合。

为了防御这种攻击，本文提出了一种事后防御方法，该方法依赖于一个完全自动化的流程：

* 生成防御数据。
* 对聚合后的LLM进行进一步微调。

#### 4. 本文创新点与贡献

* 首次提出FedIT中的安全性脆弱性，并通过一种简单、隐蔽但有效的安全攻击方法揭示了这一点。
* 提出了一种新颖的事后防御方法，该方法不需要模型级别的比较，而是通过自动化的数据生成和微调来修复攻击造成的损害。
* 在多个数据集上进行了广泛的实验，证明了所提出安全攻击方法的有效性以及防御方法的显著效果。

#### 5. 本文实验

实验在四个训练数据集上进行，使用三种安全性基准和一种有益性基准进行评估。实验结果表明：

* 提出的安全攻击方法可以显著降低LLM的安全性对齐（例如，降低70%的安全率）。
* 现有的防御方法在防御这种安全攻击方面效果有限（最多只能提高4%的绝对安全性）。
* 提出的安全防御方法可以显著提高受攻击LLM的安全性对齐（最多可提高69%的绝对安全性）。

#### 6. 实验结论

实验结果强调了所提出安全攻击方法带来的威胁，以及防御方法的有效性。通过自动化的数据生成和微调，可以显著提高LLM的安全性，同时不会显著影响其有益性。

#### 7. 全文结论

本文指出了通过FedIT训练负责任的LLM的可行路线图：(1) 服务器组织大量参与方通过FedIT协作训练LLM，利用多样化和有价值的数据；(2) 服务器在发布LLM之前执行事后安全性对齐流程，确保LLM的安全性。

#### 阅读总结

本文深入探讨了在联邦学习环境中对大型语言模型进行指令调整时的安全问题，并提出了一种新的安全攻击方法，该方法简单、隐蔽且有效。同时，文章还提出了一种创新的事后防御策略，通过自动化的数据生成和模型微调来提高模型的安全性。实验结果表明，所提出的方法能够有效地应对安全攻击，并且在保护模型安全性的同时，不会损害其有益性。这项工作不仅揭示了现有防御机制的不足，也为构建更安全的FL系统提供了有价值的见解和解决方案。
