# AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs

<figure><img src="../.gitbook/assets/image (14) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### 研究背景

大型语言模型（LLMs）在现代机器学习中无处不在，它们在广泛的领域中学习了多样化的技能，并取得了显著的成功。然而，LLMs在面对所谓的“越狱攻击”（jailbreaking attacks）时显得非常脆弱，这些攻击通过精心设计的提示（prompts）来绕过模型的安全机制，诱使模型生成不当或有害的内容。

#### 过去方案和缺点

为了应对这一挑战，研究人员提出了手动红队（manual red-teaming）方法，通过人工设计对抗性提示来发现和修复漏洞。这种方法耗时且可能存在盲点。自动化的对抗性提示生成方法虽然能够快速生成提示，但往往缺乏人类可读性，容易被基于困惑度（perplexity-based）的过滤策略检测到。此外，这些方法可能需要从TargetLLM获取梯度信息，或者由于在标记空间上的耗时离散优化过程而不易于扩展。

<figure><img src="../.gitbook/assets/image (15) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### 本文方案和步骤

本文提出了一种新的方法，通过训练另一个名为AdvPrompter的LLM来生成对抗性提示。AdvPrompter使用一种新颖的算法进行训练，不需要访问TargetLLM的梯度信息。该过程交替进行两个步骤：(1) 通过优化AdvPrompter预测生成高质量的目标对抗性后缀；(2) 使用生成的对抗性后缀对AdvPrompter进行低秩微调。



该过程的交替进行两个步骤是AdvPrompter方法的核心，旨在生成能够诱使TargetLLM生成不当响应的对抗性后缀，同时保持这些后缀的人类可读性。以下是这两个步骤的详细说明：

#### 步骤1: 通过优化AdvPrompter预测生成高质量的目标对抗性后缀

这一步骤的目的是生成能够使TargetLLM生成积极响应的对抗性后缀。这是通过以下子步骤完成的：

1. **候选选择（q-step）**: 对于每对有害指令和期望响应，AdvPrompter使用一个优化算法（AdvPrompterOpt）来生成对抗性后缀的候选。这个算法通过迭代选择和评估标记（token）候选来生成目标后缀。
2. **优化算法（AdvPrompterOpt）**: 该算法利用AdvPrompter的预测来选择下一个标记的最佳候选。它首先从AdvPrompter预测的分布中采样一定数量（k）的标记作为候选集。
3. **评估和选择**: 对于每个候选，算法评估其对抗性损失（即，将候选后缀添加到指令后，TargetLLM生成期望响应的可能性）。然后，选择使得对抗性损失最小化的标记作为最优候选。
4. **迭代改进**: 通过重复以上过程，逐步构建对抗性后缀，直到达到预定的序列长度或满足停止条件。
5. **生成对抗性后缀**: 最终，算法输出一个完整的对抗性后缀，这是对抗性攻击的一个实例。

#### 步骤2: 使用生成的对抗性后缀对AdvPrompter进行低秩微调

一旦生成了高质量的目标对抗性后缀，下一步就是利用这些后缀来改进AdvPrompter模型：

1. **微调（θ-step）**: AdvPrompter在由步骤1生成的对抗性后缀上进行微调。这些后缀作为训练样本，用来指导AdvPrompter学习如何生成更有效的对抗性提示。
2. **低秩更新**: 微调过程采用低秩适应（low-rank adaptation）技术，这是一种高效的参数更新方法，它只更新模型参数的一个小子集，从而保持了大部分预训练模型的权重不变。
3. **训练循环**: 这个过程是迭代的。在每次迭代中，AdvPrompter都会在新的对抗性后缀上进行微调，然后再次生成新的对抗性后缀，如此循环，直到模型的性能不再显著提升。
4. **改进AdvPrompter**: 通过这种方式，AdvPrompter逐渐学习如何生成能够欺骗TargetLLM的对抗性后缀，同时保持这些后缀的语义和语法的连贯性。

通过这两个步骤的交替进行，AdvPrompter能够快速生成高质量的对抗性后缀，这些后缀能够诱使TargetLLM生成不当的响应，同时保持对抗性提示的人类可读性。这种方法显著提高了对抗性提示生成的效率，并减少了对TargetLLM内部工作机制的依赖。



#### 本文创新点与贡献

1. **人类可读性**：AdvPrompter生成的对抗性提示具有很高的人类可读性，模仿人类编写的对抗性提示，而不是简单的标记集合。
2. **高攻击成功率**：在多个开源LLMs上进行了广泛的实验，与先前的方法相比，AdvPrompter显示出更高的攻击成功率（ASR）和更低的困惑度。
3. **输入适应性**：AdvPrompter生成的后缀是根据指令条件生成的，即使是在泛化到之前未见过的测试指令时也是如此。
4. **快速生成**：训练完成后，AdvPrompter可以通过下一个标记预测简单地生成对抗性后缀，而之前的方法需要为每个生成的后缀解决一个全新的优化问题。
5. **无需TargetLLM的梯度**：AdvPrompter的训练过程AdvPrompterTrain不使用从TargetLLM反向传播的梯度信息，而只使用其对数概率输出。

#### 本文实验

实验使用了AdvBench数据集，该数据集包含520条具有有害行为的指令及其相应的期望正面响应。实验在多个开源和闭源的TargetLLMs上进行，包括Vicuna-7b、Vicuna-13b、Llama2-7b-chat、Falcon-7b-instruct、Mistral-7b-instruct和Pythia-12B-chat等。

#### 实验结论

AdvPrompter在攻击成功率和提示生成时间方面均优于先前的方法。即使在闭源的黑盒模型上，AdvPrompter也显示出强大的迁移性。此外，通过在AdvPrompter生成的对抗性指令数据集上微调TargetLLM，可以显著提高模型对越狱攻击的鲁棒性，同时保持高MMLU分数。

#### 全文结论

本文提出的AdvPrompter方法为自动化红队评估LLMs提供了一种快速、有效的对抗性提示生成机制。与现有方法相比，AdvPrompter在多个方面具有显著优势，包括人类可读性、攻击成功率、输入适应性和快速生成能力。此外，AdvPrompter还可能帮助提高LLMs的安全性，通过生成对抗性指令数据集来增强模型的鲁棒性。

#### 阅读总结报告

这篇论文提出了一个创新的方法来对抗大型语言模型中的越狱攻击。通过训练一个名为AdvPrompter的LLM来生成对抗性提示，该方法不仅提高了攻击的成功率，而且大幅加快了提示的生成速度。AdvPrompter的训练过程不需要TargetLLM的梯度信息，这使得它在实际应用中更为方便和高效。实验结果表明，AdvPrompter在多个开源和闭源模型上都取得了优异的性能，证明了其强大的迁移能力和实用性。此外，论文还展示了如何使用AdvPrompter生成的数据集来提高模型的安全性，这为未来的研究和实践提供了新的思路。总的来说，这项工作为理解和改进LLMs的安全性提供了重要的贡献，并为自动化红队评估提供了一个有力的工具。
