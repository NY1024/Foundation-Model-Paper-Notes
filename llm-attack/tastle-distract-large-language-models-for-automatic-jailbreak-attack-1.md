# Tastle: Distract Large Language Models for Automatic Jailbreak Attack

<figure><img src="../.gitbook/assets/image (187).png" alt=""><figcaption></figcaption></figure>

## 研究背景

随着大型语言模型（LLMs）在自然语言处理（NLP）任务中取得显著进展，它们在现实世界的应用也引起了对潜在滥用的担忧，尤其是关于偏见和犯罪活动的问题。为了确保LLMs的安全性，研究者们在模型发布前进行了大量工作，以使其行为与人类价值观保持一致，主要目标是确保它们的有用性和无害性。然而，即使是经过精心对齐的LLMs，也仍然容易受到恶意操纵，如越狱攻击，导致意外行为的发生。

## 过去方案和缺点

以往的越狱攻击方法主要依赖于手工制作的提示（prompts）或者基于优化的方法。手工制作的越狱提示虽然有效且可转移，但不具备可扩展性。而基于优化的方法，如基于梯度的攻击，需要能够计算或近似模型输出相对于输入的梯度，这只有在知道目标模型的细节时才可能实现。这些方法在面对复杂和可转移的越狱攻击时遇到了挑战。

<figure><img src="../.gitbook/assets/image (188).png" alt=""><figcaption></figcaption></figure>

## 本文方案和步骤

本文提出了Tastle，一个新颖的黑盒越狱框架，用于自动化的红队攻击（red teaming）LLMs。Tastle通过恶意内容隐藏和记忆重构，以及迭代优化算法来设计和优化越狱模板。该框架的动机来自于LLMs易受无关内容干扰的研究，这表明LLMs容易被无关内容分散注意力，导致推理能力下降。Tastle利用这一机制，将恶意内容隐藏在复杂且无关的场景中，从而降低目标LLM拒绝恶意请求的能力。

<figure><img src="../.gitbook/assets/image (189).png" alt=""><figcaption></figcaption></figure>

Tastle的实施流程可以分为以下几个关键步骤：

1. **设计攻击框架**：
   * Tastle框架基于LLMs的易分散注意力的特性，设计了一个自动化的越狱攻击流程。
   * 框架包含三个主要组件：恶意内容隐藏、记忆重构和迭代越狱模板优化。
2. **恶意内容隐藏**：
   * 利用LLMs对无关内容容易分散注意力的特点，将恶意请求隐藏在一个复杂且无关的情境中，以此降低LLM识别和拒绝恶意请求的能力。
   * 通过手工制定的指导方针，使用攻击者LLM（attacker LLM）自动生成复杂的背景故事，并将恶意请求作为辅助任务嵌入其中。
3. **记忆重构机制**：
   * 基于LLMs的过度自信现象，提出记忆重构机制，通过特定的指令使目标LLM忽略复杂情境，专注于恶意辅助任务。
   * 要求目标LLM以特定的字符串开始其响应，如“Sure! I will shift my focus to the AUXILIARY TASK”，从而使模型更有可能遵循部分生成的响应并回应恶意请求。
4. **迭代越狱模板优化**：
   * 通过迭代优化算法，自动生成和优化越狱模板。
   * 使用攻击者LLM生成针对目标LLM的越狱模板，将模板和恶意查询结合作为模型输入，从目标LLM中引出响应。
   * 通过判断模型评估响应的有效性，并根据评估分数为攻击者LLM提供反馈，以便在下一次迭代中生成改进的模板。
5. **评估与测试**：
   * 使用特定的数据集（如AdvBench基准测试的一部分）和不同的LLMs（开源和专有）来评估Tastle的有效性。
   * 通过与其他攻击方法的比较，验证Tastle在越狱攻击中的优越性。
6. **防御策略分析**：
   * 测试不同的防御方法（如自我提醒、上下文防御和困惑度过滤器）对Tastle攻击的有效性。
   * 分析这些防御策略在减少攻击成功率方面的效果，并讨论它们的实用性和潜在的缺点。

通过上述步骤，Tastle框架能够有效地对LLMs进行自动化的越狱攻击，揭示了LLMs在安全防护方面的潜在漏洞，并为未来的安全研究提供了重要的参考。





## 本文创新点与贡献

* 提出了Tastle，一个简单且新颖的黑盒越狱框架，用于自动化的红队攻击LLMs。
* 通过广泛的实验验证了Tastle在开源和专有LLMs上的越狱攻击效果，证明了该框架在有效性、可扩展性和可转移性方面的优越性。
* 研究了现有的越狱防御方法对抗Tastle攻击的有效性，并强调了开发更有效和实用的防御策略的重要性。

## 本文实验

实验使用了包括Vicuna、LLaMA-2、ChatGPT和GPT-4在内的五种开源和闭源模型。实验结果表明，Tastle在不同模型上都取得了较高的攻击成功率，尤其是在ChatGPT和GPT-4上，分别达到了66.7%和38.0%的Top-1攻击成功率。

## 实验结论

Tastle能够有效地生成针对各种恶意请求的通用越狱模板，并且在不同的LLMs上显示出了良好的攻击转移能力。此外，Tastle结合其他攻击技术可以进一步提高攻击成功率。

## 全文结论

Tastle作为一个新颖的越狱攻击框架，展示了LLMs在面对自动化和复杂攻击时的脆弱性。这项工作强调了对LLMs安全性进行全面评估的重要性，并为未来的防御策略提供了研究方向。

## 阅读总结报告

本篇论文介绍了Tastle，这是一个自动化的黑盒越狱攻击框架，旨在测试和加强大型语言模型的安全性。Tastle通过利用LLMs的注意力机制，成功地设计了能够绕过安全限制并产生有害内容的提示。实验表明，Tastle在多种LLMs上都取得了显著的攻击成功率，揭示了即使是经过安全对齐的LLMs也容易受到恶意操纵。此外，论文还探讨了现有的防御方法，并指出了开发更有效防御策略的迫切需求。这项工作不仅展示了LLMs在安全领域的潜在风险，也为未来的研究提供了重要的方向。
