# Adversarial Demonstration Attacks on Large Language Models

<figure><img src="../.gitbook/assets/image (5) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

随着大型语言模型（LLMs）如ChatGPT和GPT-4的出现，上下文学习（ICL）在利用数据-标签对作为预条件提示方面变得显著重要。虽然引入演示（示例）可以显著提高LLMs在各种任务上的性能，但这可能引入新的安全问题：攻击者可以通过仅操纵演示而不改变输入来执行攻击。

### 2. 过去方案和缺点

以往的研究主要集中在LLMs的安全性上，但很少有研究明确定义并系统地研究LLMs对演示的安全性。此外，现有的对抗性攻击方法主要关注在输入文本示例上引入扰动，而忽略了演示部分的安全性威胁。

<figure><img src="../.gitbook/assets/image (6) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种名为advICL的新型攻击方法，旨在仅通过操纵演示而不改变输入来误导模型。advICL方法在TextAttack框架下设计，通过添加额外的演示掩码，只允许操纵演示。此外，为了解决ICL提示长度增加导致的标准全局相似性约束效果减弱的问题，作者引入了一种针对演示的特定相似性方法。

### 4. 本文创新点与贡献

* 提出了advICL方法，这是一种新的上下文学习攻击方法，专注于攻击演示而不改变输入文本。
* 引入了一种新的演示特定相似性方法，以确保在攻击策略中生成有效且高质量的对抗性示例。
* 提出了Transferable-advICL，一种可转移的攻击版本，可以在不知道和操纵测试输入示例的情况下攻击它们。

### 5. 本文实验

实验在包括SST-2、TREC、DBpedia、RTE在内的四个数据集上进行，并在GPT2-XL、LLAMA、Vicuna等不同的LLMs上进行了测试。实验结果表明，advICL方法可以成功攻击LLMs，例如在DBpedia数据集上LLaMA-7B模型的攻击成功率（ASR）达到了97.72%。

### 6. 实验结论

实验结果证明了advICL方法的有效性，即使在不改变输入文本的情况下，通过操纵演示也可以成功误导模型。此外，Transferable-advICL展示了对抗性演示的可转移性，能够在不同输入文本示例上实现攻击。

### 7. 全文结论

本文通过advICL方法揭示了ICL中演示的脆弱性，并展示了对抗性演示的可转移性。这项工作不仅揭示了ICL中的关键安全风险，而且强调了对ICL鲁棒性进行广泛研究的必要性，特别是考虑到ICL在LLMs发展中日益增长的重要性。



注：

"只允许操纵演示"意味着在advICL攻击方法中，攻击者的目标是改变或修改那些作为示例（演示）提供给语言模型的文本，而不是改变模型的输入文本。这些演示文本通常是用来帮助模型更好地理解和执行特定任务的。通过操纵这些演示文本，攻击者可以在不直接改变用户实际输入的情况下，影响模型的输出结果。

举个例子来说明advICL：

假设我们有一个情感分析任务，模型需要判断一段文本是正面还是负面情感。在上下文学习（ICL）中，我们可能会提供一些演示示例，如：

* 演示1: "这部电影真是太棒了！" (正面)
* 演示2: "我讨厌这部电影。" (负面)

现在，攻击者想要让模型错误地将一段中性的文本（例如："这部电影还可以。"）判断为负面情感。在advICL攻击中，攻击者不会改变这段中性文本，而是尝试修改演示示例，例如：

* 修改后的演示1: "这部电影真是太糟糕了！" (负面)
* 修改后的演示2: "我讨厌这部电影。" (负面)

通过这种方式，攻击者在不直接改变用户输入的情况下，通过改变演示文本来误导模型，使其将中性文本错误地分类为负面情感。这种攻击方法利用了模型在处理演示文本时可能存在的脆弱性。





### 阅读总结

本文针对LLMs在上下文学习中的安全性问题提出了新的攻击方法advICL，并通过实验验证了其有效性。作者不仅关注了输入文本的安全性，还首次系统地研究了演示在ICL中的安全性。此外，提出的Transferable-advICL方法进一步扩展了攻击的实用性，为未来在这一领域的研究提供了新的方向。
