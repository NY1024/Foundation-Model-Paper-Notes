# ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages

<figure><img src="../.gitbook/assets/image (13) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

##

## 阅读总结报告

### 1. 研究背景

本文研究的背景是大型语言模型（LLMs）在现实世界场景中的应用，特别是在工具学习（tool learning）方面。工具学习被认为是将LLMs部署到实际应用中的基础方法。然而，尽管现有研究主要关注如何利用工具增强LLMs的能力，但它们经常忽视了与应用相关的新兴安全问题。这些安全问题包括恶意查询、越狱攻击、噪声误导、风险提示、有害反馈和错误冲突等。

<figure><img src="../.gitbook/assets/image (14) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 2. 过去方案和缺点

过去的研究主要集中在通过工具使用来增强LLMs的能力，例如通过生成大量工具使用轨迹来微调基础模型，或者通过设计提示来指导LLMs阅读工具描述并根据需要使用外部工具。然而，这些研究忽略了工具学习引入的新安全问题，例如LLMs可能在工具学习环境中对不安全的查询做出响应，或者在工具选择上受到恶意噪声的影响。



### 3. 本文方案和步骤

本文提出了ToolSword框架，这是一个全面的研究框架，用于细致地调查LLMs在工具学习中的安全问题。ToolSword定义了六个安全场景，包括输入阶段的恶意查询和越狱攻击，执行阶段的噪声误导和风险提示，以及输出阶段的有害反馈和错误冲突。通过在11个开源和闭源LLMs上进行实验，本文揭示了工具学习中的持久安全挑战。

### 4. 本文创新点与贡献

* 提出了ToolSword框架，全面揭示了LLMs在工具学习中的安全问题。
* 开发了针对每个阶段的两种不同类型的安全场景，以细致评估LLMs在面对各种挑战时的安全性能。
* 对11个LLMs进行了实验，识别了工具学习各阶段的显著安全问题，强调了增强LLMs工具学习安全性的迫切需求。

### 5. 本文实验

实验涉及11个LLMs，包括开源和闭源模型。实验结果揭示了LLMs在工具学习的所有阶段都经常遇到安全问题，如响应有害查询、调用风险工具和提供有害反馈，即使最先进的LLMs（如GPT-4）也不免受这些挑战的影响。

### 6. 实验结论

实验结果表明，尽管LLMs在没有安全问题的环境下可以与人类表现相当，但在工具学习环境中，它们在安全性能方面仍有显著的不足。这表明，为了推动LLMs的实际应用，增强安全措施是至关重要的。

### 7. 全文结论

本文通过ToolSword框架对LLMs在工具学习中的安全问题进行了全面分析。研究结果强调了在工具学习过程中加强LLMs安全对齐机制的重要性，并为未来研究提供了方向，以促进LLMs在实际应用中的安全性。



注：

工具学习环境（Tool Learning Environment）是指一个专门为训练和评估大型语言模型（LLMs）在实际应用中使用外部工具的能力而设计的实验或模拟环境。在这样的环境中，LLMs被赋予与外部工具交互的能力，以便它们可以执行特定的任务，如信息检索、数据分析、文件操作等。这些工具可以是编程接口（APIs）、数据库查询、文件系统操作等。

在工具学习环境中，LLMs通常需要理解用户的意图，选择合适的工具来执行任务，并处理工具返回的结果。这种环境模拟了现实世界中的场景，其中LLMs可能需要与各种外部系统和服务交互，以提供有用的输出或执行复杂的任务。

工具学习的目标是使LLMs能够更有效地利用外部资源，提高其解决问题的能力，并使其在现实世界的应用中更加强大和有用。然而，这也带来了新的安全挑战，因为LLMs在与外部工具交互时可能会遇到恶意输入、误导性信息或其他安全风险。因此，研究者需要在工具学习环境中对LLMs进行安全评估，确保它们在实际部署时既有效又安全。



### 阅读总结

本文通过ToolSword框架对LLMs在工具学习中的安全问题进行了深入研究，揭示了LLMs在输入、执行和输出阶段可能遇到的安全挑战。研究结果表明，即使最先进的LLMs也存在安全漏洞，这强调了在实际应用中加强LLMs安全性的必要性。本文的研究不仅为LLMs的安全性研究提供了新的视角，也为未来的工具学习安全研究奠定了基础。
