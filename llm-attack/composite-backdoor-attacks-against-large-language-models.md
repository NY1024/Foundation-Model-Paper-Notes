# Composite Backdoor Attacks Against Large Language Models

1. 研究背景： 随着大型语言模型（LLMs）在各种任务上展现出的卓越性能，它们已成为许多研究和服务的基础模型。然而，这些由第三方提供的不可信LLMs可能为下游任务引入安全隐患。本文探讨了LLMs在后门攻击下的脆弱性。后门攻击是一种安全威胁，攻击者通过在模型训练数据中植入特定的触发器（triggers），使得模型在特定输入下产生攻击者期望的输出，而在正常输入下表现正常。这种攻击对于依赖这些模型的用户来说可能带来严重的安全风险，如误导信息和有害内容。
2. 过去方案和缺点： 以往的研究主要集中在单一组件（如指令或输入）中植入单一触发器的简单场景。这些方法在语义上的变化较大，容易被系统检测到。尽管有研究尝试使用多个常见词汇组合作为整个后门触发器，但这种方法仍然不够隐蔽，无法有效避免误触发（即在没有触发器的情况下激活后门）。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1)  (19).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤： 本文提出了一种复合后门攻击（Composite Backdoor Attack, CBA），在多个提示组件中散布多个触发器键。CBA确保只有在所有触发器键同时出现时才激活后门。攻击步骤包括：
   * 定义触发器键集合，并将其添加到提示的不同组件中。
   * 通过正样本（包含触发器键）和负样本（不包含触发器键或只包含部分触发器键）来训练目标模型。
   * 在训练过程中，使用特定的损失函数来优化模型权重，以实现后门攻击。
2. 本文实验和性能： 实验在自然语言处理（NLP）和多模态任务上进行。例如，在LLaMA-7B模型上针对情感数据集，使用3%的污染样本，攻击实现了100%的攻击成功率（ASR），误触发率（FTR）低于2.06%，且模型准确性几乎没有下降。实验结果表明，CBA在保持高攻击成功率的同时，能够维持低误触发率和对模型准确性的最小影响。

注：

这种复合后门攻击（CBA）的创新点主要体现在以下几个方面：

1. **多组件触发器散布**：与传统的后门攻击通常只在单一提示组件中植入触发器不同，CBA在多个提示组件中散布多个触发器键。这意味着后门只有在所有预定义的触发器键同时出现时才会被激活，这增加了攻击的隐蔽性。
2. **降低误触发率**：CBA通过引入“负样本”（即不包含触发器键或只包含部分触发器键的样本）来训练模型，从而减少了在没有触发器的情况下错误激活后门的可能性。这种方法有助于模型更好地学习正常行为与后门行为之间的区分。
3. **适应性**：CBA的设计允许它适应不同的实际场景，例如针对特定用户群体。攻击者可以根据需要选择特定的触发器键，使得后门攻击更加精细和目标化。
4. **实验验证**：本文通过在多个大型语言模型和不同任务上的实验，验证了CBA的有效性。实验结果显示，CBA能够在保持高攻击成功率的同时，维持低误触发率和对模型准确性的最小影响。
5. **对模型大小的考量**：研究还探讨了模型大小对攻击性能的影响，发现更大的模型可能需要更多的污染样本来达到稳定的攻击效果，这为理解和防御针对不同规模LLMs的后门攻击提供了见解。

这些创新点共同提高了后门攻击的隐蔽性和有效性，同时也为未来的防御策略提供了新的挑战。



阅读总结报告： 本文针对大型语言模型的安全性问题，提出了一种新型的复合后门攻击方法。与传统的后门攻击相比，CBA通过在多个提示组件中散布多个触发器键，提高了攻击的隐蔽性，并减少了误触发的可能性。实验结果证明了CBA在不同任务上的有效性，同时也强调了对基础LLMs安全性研究的重要性。未来的研究可以探索更有效的防御策略，以应对此类复杂的后门攻击。
