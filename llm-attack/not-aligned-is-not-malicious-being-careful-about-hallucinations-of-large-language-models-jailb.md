# “Not Aligned” is Not “Malicious”: Being Careful about Hallucinations of Large Language Models’ Jailb

<figure><img src="../.gitbook/assets/image (5) (1) (1).png" alt=""><figcaption></figcaption></figure>



#### 1. 研究背景

大型语言模型（LLMs）因其在自然语言处理任务中的先进能力而备受关注，但它们的安全性问题，特别是所谓的“越狱”（jailbreak）现象，引起了重大关注。越狱攻击是指恶意提示导致LLMs生成有害输出，这关乎到LLMs的可靠性和安全性。有效的越狱评估对于开发缓解策略至关重要。

#### 2. 过去方案和缺点

现有的越狱评估方法可能将LLMs的“幻觉”（hallucinations）——即错误的输出，误认为是真实的安全漏洞。这表明一些被认为的漏洞可能并不代表实际威胁，这就需要更精确的红队基准测试。现有评估方法面临的挑战包括：

* 词汇匹配方法可能产生误报和漏报。
* 使用另一个LLM来评估响应安全性的方法成本高昂，且容易出现高漏报率。
* 训练分类器需要大型、经过筛选的数据集，并且可能因为类别不平衡而导致偏差结果。
* 人工审查虽然准确，但过程繁琐、成本高昂且难以扩展。

<figure><img src="../.gitbook/assets/image (6) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### 3. 本文方案和步骤

为了解决这个问题，作者提出了一个名为BABYBLUE的基准测试，它引入了一个专门的验证框架，包括多种评估器来增强现有的越狱基准测试，确保输出是有用的恶意指令。BABYBLUE的评估框架包括三个阶段：分类阶段、文本阶段和功能阶段。

#### 4. 本文创新点与贡献

* **BABYBLUE**：提出了一个新的基准测试，专注于评估越狱攻击中LLMs的输出是否真正具有危害性。
* **评估框架**：设计了一个包含六个评估器的专门评估框架，用于更严格的验证输出。
* **补充数据集**：提出了一个新的数据集，专门用于评估越狱场景中的幻觉问题。

#### 5. 本文实验

作者使用不同的红队方法在不同的LLMs上进行实验，并使用两个数据集来验证评估器的有效性。实验结果表明，BABYBLUE在降低误报率方面表现优异，并且能够提供更准确的评估。

#### 6. 实验结论

BABYBLUE显著降低了误报率，同时保持了稳定的漏报率数量，这表明了评估器在提供更准确和可靠评估方面的有效性。

#### 7. 全文结论

本文揭示了LLMs越狱中的许多感知漏洞实际上是模型产生的幻觉，并非真正的威胁。为了解决这个问题，作者引入了BABYBLUE，这是一个新的评估框架，通过专门的评估器来验证事实的准确性、功能、上下文相关性和毒性。此外，作者还提出了一个补充数据集，专门设计用于评估越狱场景中的幻觉。这些贡献提高了越狱评估的性能，并强调了在越狱完成中关注误报的重要性，有助于更安全地部署LLMs。

#### 阅读总结

本文对LLMs中的越狱现象进行了深入研究，指出了现有评估方法的不足，并提出了BABYBLUE这一新的评估框架来提高评估的准确性和可靠性。通过引入专门的评估器和补充数据集，BABYBLUE能够有效区分真正的威胁和幻觉，为LLMs的安全部署提供了重要的基准测试。这项工作不仅提高了我们对LLMs安全性的理解，也为未来的研究方向和模型改进提供了指导。
