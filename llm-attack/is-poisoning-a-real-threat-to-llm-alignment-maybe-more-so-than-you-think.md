# IS POISONING A REAL THREAT TO LLM ALIGNMENT? MAYBE MORE SO THAN YOU THINK

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 研究背景

近年来，通过人类反馈进行强化学习（RLHF）在大型语言模型（LLMs）的对齐方面取得了显著进展。然而，随着这些方法在实际应用中的增加，它们对数据污染攻击的脆弱性也引起了关注。本文研究了直接偏好优化（DPO）在不同场景下对数据污染攻击的脆弱性，并比较了不同类型攻击下的偏好数据污染的有效性。

### 过去方案和缺点

传统的RLHF方法使用Proximal Policy Optimization (PPO)算法，它对超参数非常敏感。这导致了直接策略优化（DPO）方法的开发，该方法在监督学习框架中处理RLHF，通过寻找最优策略的精确解。然而，过去的研究没有深入分析DPO在训练时对标签翻转攻击的脆弱性。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 本文方案和步骤

本文首先介绍了相关工作，包括RLHF、越狱攻击、后门攻击和RL中的奖励污染。然后，提出了针对DPO的攻击模型，包括后门和非后门攻击，以及基于DPO分数、梯度投影和BERT嵌入的不同的污染方法。接着，详细描述了实验设置，包括数据、模型、训练和评估方法。最后，展示了实验结果，并讨论了结果的影响和潜在原因。

### 本文创新点与贡献

1. 首次全面分析了基于DPO的对齐方法在训练时对攻击的脆弱性。
2. 提出了三种选择性构建污染数据集的方法，考虑了污染效果。
3. 展示了基于DPO分数的、无需梯度的方法，能够有效地用较小的数据比例污染模型。

### 本文实验

实验使用了类似于\[5]的无害基础数据集，并考虑了三种不同的LLMs，即Llama 7B、Mistral 7B和Gemma 7B。评估使用了从未受污染的清洁数据集学习到的干净奖励模型，以及GPT4对响应进行评分。实验结果显示，使用DPO分数选择的污染点显著提高了攻击的效果。

### 实验结论

实验发现，与基于PPO的方法相比，DPO方法在后门攻击中只需要0.5%的数据被污染就能引发有害行为，而PPO方法至少需要4%的数据。此外，还发现后门攻击比非后门攻击更容易执行，且DPO分数基于的攻击在没有梯度信息的情况下表现更好。

### 全文结论

本文全面分析了基于DPO的RLHF微调方法的脆弱性。研究发现，通过利用学习流程中的DPO分数，即使只有0.5%的数据被污染，也能轻易地对DPO进行污染，这使得后门攻击更加可行。同时，非后门攻击的难度显著增加，即使有选择性地污染，也需要高达25%的数据点。研究还发现，某些模型之间存在一定程度的转移性，但并非普遍适用。



这篇论文《IS POISONING A REAL THREAT TO LLM ALIGNMENT? MAYBE MORE SO THAN YOU THINK》由Pankayaraj Pathmanathan等人撰写，深入探讨了大型语言模型在强化学习中的对齐问题，特别是直接偏好优化（DPO）方法的脆弱性。研究表明，与传统的PPO方法相比，DPO在面对数据污染攻击时更为脆弱，尤其是后门攻击。通过提出新的方法来选择污染数据点，研究展示了基于DPO分数的攻击方法能够以极小的数据比例有效污染模型。

这项工作不仅揭示了DPO方法在实际应用中的潜在风险，也为未来如何提高LLMs的安全性和鲁棒性提供了新的视角和研究方向。论文的实验设计严谨，评估方法全面，结果表明，即使是在有选择性地污染数据的情况下，DPO方法也比PPO方法更容易受到攻击。此外，论文还讨论了攻击的转移性，为使用白盒模型对闭源语言模型进行黑盒攻击提供了可能的途径。最后，论文提出了改进DPO学习目标以增强对这类攻击的鲁棒性的必要性。

