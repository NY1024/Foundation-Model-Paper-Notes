# Scaling Behavior of Machine Translation with Large Language Models under Prompt Injection Attacks

<figure><img src="../.gitbook/assets/image (9) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 研究背景

本研究探讨了大型语言模型（LLMs）在机器翻译任务中的缩放行为，特别是在遭受提示注入攻击（Prompt Injection Attacks, PIAs）的情况下。LLMs因其在多种自然语言处理任务中的高效性和灵活性而越来越受到青睐。然而，它们的通用性也使得它们容易受到终端用户的操纵，用户可能通过在请求中嵌入指令来使模型以未授权或不安全的方式行事。这种现象被称为逆向缩放（Inverse Scaling），即模型随着规模的增大，反而变得更容易受到攻击。

## 过去方案和缺点

以往的研究主要集中在LLMs随着规模增大而性能提升的经验法则上。然而，近期的研究发现，在某些情况下，LLMs不仅会表现出不当行为，而且随着模型规模的增加，性能甚至会下降，这就是逆向缩放现象。此外，还有研究发现LLMs的性能与规模之间存在非单调的关系，例如U形缩放或逆U形缩放。这些现象表明，简单的模型规模扩大并不总是导致性能提升。

## 本文方案和步骤

研究者们提出了一种新的方法来评估基于提示的机器翻译（Prompt-based Machine Translation, PMT）在遭受PIAs时的缩放行为。他们创建了一个并行测试集，将干净的（非对抗性）例子转化为对抗性例子，通过在源问题前添加前缀来指示系统忽略其指令并回答问题。他们使用多个LLM家族在零样本和少样本设置下评估这些例子，识别出缩放趋势，并发布了用于重现实验的数据和代码。

## 本文创新点与贡献

本文的主要创新点在于首次研究了LLMs在多语言环境中非平凡的缩放行为，特别是在遭受PIAs的情况下。研究者们发现，在特定条件下，更大的模型可能更容易受到成功的攻击，这是逆向缩放现象的一个实例。此外，他们还发现，当提示用英语编写时，模型的性能会严重恶化，表明存在与训练数据量相关的逆向缩放现象。

## 本文实验

实验包括了对多个LLM家族在不同语言对和提示策略下的评估。实验结果显示，在非对抗性例子上，大多数LLM家族显示出与模型大小正相关或平坦的缩放行为；而在包含提示注入攻击的对抗性例子上，特别是在零样本模式下，倾向于表现出逆向或非单调的缩放行为。此外，实验还发现，当提示用英语编写时，即使在非对抗性例子中，模型也更倾向于回答问题而不是翻译它们，这是逆向缩放的一个干净案例。

## 实验结论

实验结果表明，通过在上下文中提供单个平行示例，可以避免逆向缩放现象。此外，基于代码的训练和/或指令调整可能有助于逆转逆向缩放趋势。然而，指令调整可能会干扰上下文学习，如Llama2-chat模型的结果所示。

## 全文结论

本文通过研究LLMs在机器翻译任务中的缩放行为，特别是在遭受PIAs的情况下，发现了逆向缩放现象。研究表明，即使是在更大的模型规模下，也可能无法完全克服所有逆向趋势，特别是考虑到训练和部署非常大的模型的成本。这项工作不仅增进了对LLMs脆弱性的理解，而且可能有助于提高基于LLM的系统的安全性。

## 阅读总结报告

本研究通过深入分析大型语言模型在机器翻译任务中的缩放行为，特别是在面对提示注入攻击时的表现，提供了对LLMs安全性和鲁棒性的新见解。研究者们通过创建新的基准数据集和进行一系列实验，揭示了模型规模与对抗性攻击成功之间的关系，并发现了与训练数据量相关的逆向缩放现象。这些发现对于设计更安全、更可靠的LLM应用具有重要意义，并为未来的研究提供了新的方向。
