# Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation

<figure><img src="../.gitbook/assets/image (9).png" alt=""><figcaption></figcaption></figure>

#### 研究背景

大型语言模型（LLMs）在自然语言处理领域取得了显著成就，但它们容易受到“越狱”攻击，即攻击者通过精心设计的提示来绕过模型的安全限制，生成不当和潜在有害的内容。这些攻击可能导致严重后果，如加剧社会偏见、煽动暴力、散布虚假信息或协助网络攻击。因此，研究LLMs对越狱攻击的脆弱性以及开发有效的缓解策略变得尤为重要。

#### 过去方案和缺点

现有的越狱技术主要分为两类：提示级别（prompt-level）和标记级别（token-level）。提示级别技术通过设计巧妙的自然语言提示来欺骗模型，但这些技术需要大量的手动努力和创造性。而标记级别技术通过优化输入到LLM的原始标记序列来触发违反模型预期行为的内容生成，具有自动化的潜力。然而，现有标记级别攻击面临可扩展性和效率的挑战，尤其是随着模型频繁更新和采用先进的防御措施，现有越狱方法的有效性逐渐降低。

<figure><img src="../.gitbook/assets/image (10).png" alt=""><figcaption></figcaption></figure>

#### 本文方案和步骤

本文提出了JAILMINE，一种创新的标记级别操作方法，通过自动化“挖掘”过程来引出LLMs的恶意响应。JAILMINE通过以下几个步骤来实现：

1. 使用少量模板技术自动生成对有害请求的积极回应。
2. 制定策略操纵输出logits，迫使模型生成有害内容而不附加额外的后缀。
3. 利用排序模型来产生稳定的、被操纵的越狱响应。

#### 本文创新点与贡献

JAILMINE的主要创新点和贡献包括：

* 确定了LLMs生成正常和有害内容的独特模式，并在开源LLMs上系统化和自动化了利用过程。
* 提出了JAILMINE，一种白盒标记级别且与提示无关的方法，用于从LLMs中引出恶意答案。
* 在五个开源模型上，JAILMINE的有效性和效率超过了三个基线，表明其可转移性和普适性。

#### 本文实验

实验部分对JAILMINE进行了评估，包括：

* 在五个广泛认可的开源LLMs上进行测试。
* 使用AdvBench和JailbreakBench两个基准测试集。
* 与三种现有的越狱攻击技术（GCG、PAIR和GPTFuzzer）进行比较。
* 使用攻击成功率（ASR）作为主要评估指标。

#### 实验结论

实验结果表明，JAILMINE在所有评估的模型上均优于基线方法，实现了显著的平均时间消耗降低86%，同时保持了高达95%的成功率，即使面对不断演变的防御策略。

#### 全文结论

JAILMINE通过操纵输出logits和生成积极前缀，有效地从LLMs中引出越狱响应。它在五个流行的开源LLMs上展示了卓越的效果和效率，通过消融研究进一步强调了每个组件的关键作用。

#### 阅读总结报告

这篇论文深入探讨了大型语言模型在面对有害输入时的脆弱性，并提出了一种新的越狱攻击方法JAILMINE。该方法通过标记级别的操纵，有效地绕过了模型的安全限制。论文不仅展示了JAILMINE的高效性，还通过与其他方法的比较，证明了其优越性。此外，作者还进行了消融研究，以展示每个组件对整体性能的重要性。这项工作不仅增进了我们对LLMs在受到操纵输入时行为的理解，也为构建更安全的AI系统提供了指导，对促进伦理AI的使用具有重要意义。
