# PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails

<figure><img src="../.gitbook/assets/image (9) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 研究背景

大型语言模型（LLMs）通常被设计为对人类无害。然而，近期研究表明，这些模型容易受到自动化的“越狱”攻击，这些攻击能够诱导模型生成有害内容。为了增强安全性，更近期的LLMs通常会加入一个额外的防御层，即Guard Model。Guard Model是一个二级LLM，旨在检查并调节主要LLM的输出响应。然而，本文指出，现有的Guard Model在面对新型攻击策略时仍然存在漏洞。

<figure><img src="../.gitbook/assets/image (10) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 过去方案和缺点

过去的研究集中在通过训练时的对齐（如RLHF或DPO）来强制执行LLMs的HHH（有帮助、诚实、无害）标准。另一种方法是在推理时通过一个单独的LLM（即Guard Model）来执行安全检查。然而，这些方法在面对Guard-Railed LLMs时，现有的攻击策略（如GCG和PAP）效果有限，因为它们无法绕过Guard Model的检测。

## 本文方案和步骤

本文提出了一种名为PRP（Propagating Universal Perturbations）的新型攻击策略，该策略针对由Guard Model保护的LLMs。PRP策略包括两个关键步骤：

1. 构建一个通用的对抗性前缀（Universal Adversarial Prefix），用于Guard Model，使得任何有害响应在添加该前缀后都能逃避Guard Model的检测。
2. 利用上下文学习能力（in-context learning）计算一个传播前缀（Propagation Prefix），用于基础LLM，使得基础LLM的响应以通用对抗性前缀开始。

## 本文创新点与贡献

本文的主要贡献在于提出了PRP攻击策略，该策略能够成功地对抗多个开源（如Llama 2）和闭源（如GPT 3.5）的Guard Model实现。PRP策略展示了即使在攻击者无法访问Guard Model的情况下，也能有效地进行攻击。这项工作表明，在Guard Model和防御措施被认为有效之前，还需要进一步的技术进步。

## 本文实验

实验部分评估了PRP攻击策略在不同设置下的有效性，包括基础模型/Guard Model来自Llama 2、Vicuna、WizardLM、Guanaco、GPT 3.5和Gemini家族。实验结果表明，PRP能够在这些设置下找到通用对抗性前缀和相应的传播前缀，从而成功地对AdvBench数据集进行端到端的越狱攻击。



Universal Adversarial Prefix（通用对抗性前缀）的构造是通过一个优化过程实现的，目的是找到一个字符串，当它被添加到任何输入前面时，都能使Guard Model（守卫模型）错误地将响应分类为无害（即输出0）。这个前缀能够普遍地影响Guard Model的判断，使其无法检测到有害内容。

构造过程的关键步骤如下：

1. **定义目标函数**：目标是最大化Guard Model输出无害（0）的概率。这通常通过优化一个期望值来实现，该期望值是在所有可能的有害响应上计算得到的。
2. **迭代优化**：使用贪心算法迭代更新前缀中的每个标记（token）。在每一步中，都会生成一组新的前缀候选，这些候选通过在当前前缀的每个位置替换词汇表中的标记来生成。
3. **选择候选**：根据威胁模型（white-box或black-box），选择新候选的方法不同。在white-box情况下，选择具有最大梯度的标记；在black-box情况下，随机选择标记。
4. **评估和选择**：评估每个候选前缀在所有有害响应上的表现，并选择使得Guard Model输出无害概率最高的候选前缀。
5. **终止条件**：当找到一个对所有有害响应都能成功诱导出无害输出的前缀时，或者达到最大迭代次数时，优化过程终止。

在实际应用中，由于候选数量可能非常大，研究者通常会只比较一个随机子集的候选，以提高效率。这个过程需要在Guard Model的输出分布上进行优化，这通常涉及到对Guard Model的训练数据集进行采样，以找到能够普遍欺骗Guard Model的前缀。

在论文中，作者提供了一个算法（Algorithm 1）来详细描述这个过程，包括初始化前缀、迭代更新、选择最佳候选以及终止条件。这个算法是PRP攻击策略中的关键组成部分，因为它直接关系到攻击能否成功绕过Guard Model的检测。



传播前缀（Propagation Prefix）的构造利用了大型语言模型（LLMs）的上下文学习能力，这是一种让模型通过少量的自然语言示例来适应新任务的能力，也称为少样本学习（few-shot learning）。在PRP攻击策略中，传播前缀的构造过程如下：

1. **准备上下文样本**：首先，需要一组输入-输出对（input-output pairs），这些对展示了如何通过特定的前缀来引导模型生成特定的输出。这些样本可以手动创建，也可以通过任何开源的、非Guard-Railed的LLM生成。
2. **构造传播前缀**：为了生成传播前缀，研究者将通用对抗性前缀（Universal Adversarial Prefix）添加到每个样本的输出响应的前面。这样，当模型在新的输入提示（prompt）下生成响应时，它会倾向于在响应的开始部分包含这个前缀。
3. **利用上下文学习**：LLMs的上下文学习能力使得模型能够根据给定的输入和上下文来预测下一个最可能的标记。在这种情况下，模型被训练（通过上下文样本）来在任何给定的输入后面生成包含传播前缀的响应。
4. **生成传播前缀**：通过将传播前缀添加到输入提示的前面，模型被引导在生成响应时以这个前缀开始。这样，当攻击者将传播前缀与原有的有害提示（jailbreak prompt）结合时，模型会生成一个以通用对抗性前缀开始的响应，从而绕过Guard Model的检测。
5. **优化传播前缀**：在实际应用中，可能需要对传播前缀进行优化，以确保它能够有效地被模型学习和复现。这可能涉及到调整前缀的长度、内容，以及在上下文样本中如何呈现这个前缀。

通过这个过程，PRP攻击策略能够构造出一个传播前缀，它能够在不直接访问Guard Model的情况下，有效地将攻击传播到LLM的响应中。这种策略的成功依赖于LLM强大的上下文学习能力，以及模型对输入提示的敏感性。





## 实验结论

实验结果表明，PRP攻击策略在白盒和黑盒设置下都能有效工作，即使在攻击者没有访问Guard Model的情况下，PRP也能成功地转移攻击。此外，Guard Model并没有为基础对齐的LLM提供额外的安全保障。

## 全文结论

本文提出了PRP攻击策略，这是一种新型的攻击方法，用于评估Guard-Railed LLMs的安全性。PRP通过两步过程将通用攻击传播到基础LLM的响应中，从而破坏了保护它的Guard Model的效用。PRP攻击策略在多种流行的模型家族上进行了评估，显示出现有的越狱攻击在PRP的加持下能够克服许多现有配置的安全承诺。

## 阅读总结报告

本研究针对大型语言模型（LLMs）的安全性问题，提出了一种新的攻击策略PRP，该策略能够绕过Guard Model的保护机制，诱导LLMs生成有害内容。PRP通过构建通用对抗性前缀和传播前缀，有效地攻击了多种开源和闭源的Guard Model实现。实验结果表明，PRP在不同威胁模型下均表现出色，即使在攻击者无法访问Guard Model的情况下也能成功。这一发现强调了当前Guard Model在防御越狱攻击方面的局限性，并为未来的安全研究提供了新的挑战和方向。
