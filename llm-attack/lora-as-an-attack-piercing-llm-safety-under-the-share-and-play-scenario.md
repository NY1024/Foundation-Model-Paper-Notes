# LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario

<figure><img src="../.gitbook/assets/image (5) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

####

**1. 研究背景**

大型语言模型（LLMs）在自然语言处理（NLP）任务中取得了显著的成功。为了提高特定任务的性能并确保模型行为与人类偏好一致，对这些模型进行微调（fine-tuning）是必要的。低秩适应（LoRA）作为一种参数高效的微调机制，因其高效和易于使用而受到广泛采用。LoRA通过在保持其他参数不变的同时，向变换器块中注入一个可训练的低秩分解矩阵，从而在微调中带来优越的效率。然而，LoRA的共享和即插即用的特性也带来了新的安全风险，攻击者可以利用这一点，通过后门注入等方式，将恶意的LoRA模块广泛分布到社区中。

**2. 过去方案和缺点**

以往的研究主要集中在通过微调降低模型的对齐度，而LoRA仅被视为一种高效的替代方案。这些研究没有考虑LoRA在共享和即用上下文中的潜在风险，留下了未充分探索的攻击面。

**3. 本文方案和步骤**

本文首先探讨了攻击者如何利用LoRA作为攻击手段，特别是通过后门攻击来突出LoRA采用的安全问题。研究深入探讨了LoRA的内在机制和后门行为，并提出了一个无需训练的直接后门注入方法。此外，本文还研究了在同时采用多个LoRA模块时后门行为的影响，以及基于LoRA的后门传递性。

**4. 本文创新点与贡献**

* **攻击面探索**：首次全面调查了在共享和即用场景下LoRA作为攻击手段的攻击机会。
* **后门注入机制**：研究了如何在不牺牲LoRA原始功能的情况下，将后门注入LoRA模块。
* **无需训练的注入方法**：提出了一种无需额外微调即可直接合并恶意LoRA与良性LoRA的方法。
* **多模块后门行为**：分析了在采用多个LoRA模块时后门行为的影响。
* **后门传递性**：研究了LoRA后门在不同基础模型之间的传递性。

**5. 本文实验**

实验使用了Llama-2-7B作为基础模型，并采用了针对特定下游任务训练的LoRA模块。通过OpenAI GPT3.5生成对抗性数据，并使用这些数据对LoRA进行后门微调。评估指标包括情感倾向和特定关键词的出现频率。实验结果表明，即使在注入后门后，LoRA的下游能力仍然保持稳定，而后门攻击效果显著。

**6. 实验结论**

实验结果显示，攻击者可以在不显著影响LoRA的下游功能的情况下，有效地嵌入后门。此外，即使在多个LoRA模块共存的情况下，后门行为也能持续存在。使用防御性LoRA可以减轻后门的影响，而不会大幅损害LoRA的功能准确性。

**7. 全文结论**

LoRA作为一种高效和易于使用的微调工具，也存在被攻击者利用作为对抗性工具的风险。本文通过全面调查LoRA在共享和即用设置中的新攻击面，旨在提高对LoRA作为攻击媒介的潜在风险的认识，并为主动防御提供支持。

***

