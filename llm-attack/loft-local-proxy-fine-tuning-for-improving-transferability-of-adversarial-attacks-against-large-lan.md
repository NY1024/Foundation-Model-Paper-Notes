# LOFT: LOCAL PROXY FINE-TUNING FOR IMPROVING TRANSFERABILITY OF ADVERSARIAL ATTACKS AGAINST LARGE LAN

<figure><img src="../.gitbook/assets/image (14) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 1. 研究背景

大型语言模型（LLMs）如GPT-3.5、GPT-4和Claude等在自然语言处理任务中表现出色，但同时也存在生成有害信息的风险。为了评估和提高LLMs的安全性，研究者们开发了对抗性攻击方法，通过在查询后附加特定的攻击后缀来绕过LLMs的对齐机制。然而，这些攻击的成功转移性取决于代理模型与目标模型之间的相似度。本文提出了一种新的方法，即局部微调（LoFT），以提高代理模型在有害查询邻域内的近似度，从而增强攻击的转移性。

### 2. 过去方案和缺点

以往的研究依赖于代理模型来生成对抗性攻击，这些模型通常需要大量数据进行微调以近似目标模型。然而，这种方法成本高昂，且随着LLMs的快速更新，代理模型很快就会过时。此外，代理模型在整个输入空间上与目标模型的近似度可能不足，导致攻击在目标模型上的成功率不高。

<figure><img src="../.gitbook/assets/image (15) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了LoFT方法，它通过以下步骤来提高代理模型的攻击转移性：

* 使用目标LLM生成与有害查询相似的查询。
* 从目标模型获取这些相似查询的响应，用于局部微调代理模型。
* 使用微调后的代理模型优化攻击后缀，生成攻击提示。
* 在目标模型上评估攻击的成功率。

<figure><img src="../.gitbook/assets/image (16) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 4. 本文创新点与贡献

* 提出了LoFT技术，用于在有害查询的邻域内局部微调代理模型。
* 证明了LoFT代理生成的对抗性攻击在GPT-4上的有效性提高了89.7%。
* 发现即使攻击成功，目标LLM的响应也不一定包含有害内容，指出了现有评估指标的不足。
* 通过人类评估，展示了LoFT代理生成的攻击在ChatGPT-3.5上产生真正有害内容的比例高达43.6%。

### 5. 本文实验

实验在ChatGPT、GPT-4和Claude等目标模型上进行，通过LoFT方法微调代理模型，并在这些模型上评估攻击的成功率。实验结果表明，LoFT方法显著提高了攻击的成功率。

### 6. 实验结论

实验结果支持了LoFT方法的有效性，证明了局部微调可以显著提高代理模型在目标模型上的攻击成功率。此外，人类评估表明，LoFT代理能够更有效地产生包含有害信息的响应。

### 7. 全文结论

本文提出了LoFT方法，一种新的局部微调技术，用于提高代理模型在有害查询邻域内的近似度，从而增强对抗性攻击的转移性。实验结果表明，LoFT方法在提高攻击成功率方面取得了显著成效，并且能够更准确地评估攻击的安全性风险。

### 阅读总结

本文通过LoFT方法解决了LLMs对抗性攻击转移性的问题，通过在有害查询的邻域内局部微调代理模型，显著提高了攻击在目标模型上的成功率。这一方法不仅提高了攻击的转移性，还通过人类评估揭示了现有评估指标的不足，为LLMs的安全性研究提供了新的视角。
