# Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks

<figure><img src="../.gitbook/assets/image.png" alt=""><figcaption></figcaption></figure>



#### 1. 研究背景

随着第三方训练的大型语言模型（LLMs）应用的增长，这些模型的安全性问题引起了严重关注。已有研究表明，恶意行为者可以通过数据投毒攻击来利用LLMs的漏洞，以生成不良输出。尽管在图像领域和分类任务中对数据投毒攻击已有大量研究，但其对生成模型，特别是自然语言生成（NLG）任务的影响尚不清楚。为了弥补这一空白，本文对各种数据投毒技术进行了全面探索，以评估它们在一系列生成任务中的有效性，并为NLG任务设计了一系列指标，以量化投毒攻击的成功和隐蔽性。

#### 2. 过去方案和缺点

以往的研究主要集中于文本分类任务，对于LLMs在NLG任务上的数据投毒攻击研究有限。此外，现有的工作或是直接将分类任务中的攻击应用到NLG任务上，或是需要从头开始训练外部LLMs以生成投毒样本，这需要大量的计算能力。对于通过微调（fine-tuning）适应下游任务的LLMs，其对数据投毒攻击的脆弱性尚不明确。

<figure><img src="../.gitbook/assets/image (1).png" alt=""><figcaption></figcaption></figure>

#### 3. 本文方案和步骤

本文提出了一种系统化的方法来理解针对NLG任务的投毒攻击，考虑了各种触发器和攻击设置。研究使用了两种不同的NLG任务：文本摘要和文本补全，并采用了两种类型的LLMs：编码器-解码器变换器T5-small和仅解码器因果LLM GPT-2。研究的步骤包括：

* 评估不同长度和不同方面的触发器。
* 提出评价指标来衡量投毒生成模型的性能。
* 在多个NLG任务、LLMs和数据集上进行广泛的实验。

#### 4. 本文创新点与贡献

* 提出了一种新的评估指标来衡量投毒攻击的成功和隐蔽性。
* 对比了全模型微调和参数高效微调（PEFT）方法在NLG任务上的安全性脆弱性。
* 提出了多种攻击方法，包括不同的触发器设计、触发器插入策略和触发器长度。
* 通过实验结果展示了这些变化如何直接影响攻击的成功和隐蔽性。

#### 5. 本文实验

实验在两个主要的NLG任务上进行了投毒攻击测试：文本摘要和文本补全。使用了两种类型的LLMs，并对不同的数据集进行了评估。实验结果显示，即使是在微调阶段只使用了总调整数据样本的1%，也可以成功地对LLM进行投毒。

#### 6. 实验结论

实验结果表明：

* 触发器的相对长度和位置在攻击成功和隐蔽性中起着关键作用。
* 根据不同的任务，全模型微调可能比前缀调整更易受投毒攻击的影响，反之亦然。
* 某些任务（如文本补全）比其他任务（如文本摘要）更难攻击。

#### 7. 全文结论

本文是首次对NLG任务上的投毒攻击进行详细研究和描述。通过提出新的评估指标和比较不同微调方法的安全性，本文为理解并防御这些新型威胁迈出了第一步。

#### 阅读总结

本文对大型语言模型在自然语言生成任务中的数据投毒攻击进行了系统化的研究。作者提出了新的评估指标，用以衡量投毒攻击的成功和隐蔽性，并探讨了不同触发器设计和插入策略对攻击效果的影响。实验结果表明，即使是少量的投毒数据也足以影响模型的行为，这对于理解并提升LLMs的安全性具有重要意义。
