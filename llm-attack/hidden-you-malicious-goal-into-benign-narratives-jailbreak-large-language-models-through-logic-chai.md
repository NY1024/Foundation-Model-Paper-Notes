# Hidden You Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Logic Chai

####

**1. 研究背景**

大型语言模型（LLMs）如BERT和GPT，因其在自然语言处理（NLP）领域的卓越能力而广受欢迎。这些模型被广泛应用于各种实际应用中，如搜索引擎、文本处理和产品推荐等。然而，LLMs也存在安全漏洞，如提示注入攻击和越狱攻击，这些攻击可以欺骗模型生成恶意内容。

**2. 过去方案和缺点**

* **提示注入攻击**：通过精心设计的提示绕过过滤器或操纵LLM，使模型忽略先前的指令或执行非预期动作。
* **越狱攻击**：使用特殊设计的提示绕过LLM的安全审查，以产生有害内容。
* **缺点**：现有方法直接将意图嵌入提示中，容易被检测到，且容易被人类识别。

**3. 本文方案和步骤**

* **逻辑链注入攻击**：将恶意查询分解为一系列语义等价的叙述，并将其嵌入到相关良性文章中。
  * **步骤1**：将恶意查询分解为语义等价的叙述序列。
  * **步骤2**：找到一个相似主题的良性文章，并将分解后的逻辑链嵌入到文章中。
  * **步骤3**：确保LLM能够连接分散的逻辑，通过分析LLM通常更关注的词汇类型。

**4. 本文创新点与贡献**

* 提出了一种新的越狱攻击方法，能够同时欺骗LLMs和人类（即安全分析师）。
* 借鉴社会心理学和软件攻击中的策略，通过将谎言隐藏在真相中来欺骗人类。
* 通过逻辑链注入攻击，不遵循任何特定模式，使得攻击难以被检测。

**5. 本文实验**

* 通过构造特定的提示和文章，展示了逻辑链注入攻击的有效性。
* 提供了两个攻击实例：段落逻辑链和“Acrostic”风格逻辑链。

**6. 实验结论**

* 逻辑链注入攻击能够成功地操纵LLM执行隐藏的恶意命令，同时对人类来说也不易被发现。
* 攻击方法的有效性得到了验证，表明LLM系统需要更强大的防御机制来抵御此类攻击。

**7. 全文结论**

* 本文提出的逻辑链注入攻击方法，展示了一种新的欺骗LLMs和人类的攻击手段。
* 强调了对LLMs安全性的担忧，并指出了现有防御机制的不足。
* 建议未来的研究和开发应关注更健壮的防御措施，以应对不断进化的攻击手段。

**阅读总结**

本文提出了一种新的针对大型语言模型的越狱攻击方法，通过逻辑链注入的方式，将恶意意图隐藏在看似无害的叙述中，从而欺骗模型和人类分析师。文章详细介绍了攻击的背景、步骤、创新点，并提供了实验验证。最后，文章强调了LLMs安全性的重要性，并对未来的研究方向提出了建议。这项研究对于理解和提高LLMs的安全性具有重要意义。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>
