# Ignore Previous Prompt: Attack Techniques For Language Models

<figure><img src="../.gitbook/assets/image (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

本文研究的背景是大型语言模型（LLMs），特别是基于Transformer的模型，如GPT-3，它们在大规模面向客户的自然语言处理（NLP）任务中提供了强大的基础。然而，关于恶意用户交互导致的LLMs脆弱性的研究相对较少。作者提出了PROMPTINJECT框架，用于研究GPT-3等LLMs如何被简单手工输入轻易地误导。

### 2. 过去方案和缺点

过去的研究主要集中在LLMs的能力和应用上，而对于它们在面对恶意输入时的脆弱性关注不足。尽管有一些研究提出了检测和减轻LLMs有害行为的方法，但这些方法通常无法完全防止恶意攻击，尤其是在开放的、无结构的LLM提示（prompt）环境中。

<figure><img src="../.gitbook/assets/image (2) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了PROMPTINJECT框架，这是一个用于迭代对抗性提示组成的朴素对齐框架。研究了两种攻击类型：目标劫持（goal hijacking）和提示泄露（prompt leaking）。通过构建多个攻击提示，作者分析了这些攻击的可行性和有效性。

<figure><img src="../.gitbook/assets/image (3) (1).png" alt=""><figcaption></figcaption></figure>

### 4. 本文创新点与贡献

* 提出了PROMPTINJECT框架，用于探索LLMs对对抗性提示攻击的鲁棒性。
* 研究了两种具体的攻击：目标劫持和提示泄露。
* 提供了AI x-risk分析，以评估研究对减少存在风险的潜在影响。

### 5. 本文实验

作者使用PROMPTINJECT框架构建了多个攻击提示，并在35个基础提示上进行了实验。实验考虑了多种因素，如温度、top-p采样、频率和存在惩罚等。实验结果显示，目标劫持的成功率为58.6% ± 1.6，而提示泄露的成功率为23.6% ± 2.7。

### 6. 实验结论

实验结果表明，即使是能力较低的攻击者，也可以通过精心设计的输入轻易地利用GPT-3的随机性，造成长尾风险。此外，不同的模型设置对攻击成功率有显著影响。

### 7. 全文结论

本文强调了研究提示注入攻击的重要性，并提供了对影响因素的见解。作者认为，这项工作可以帮助社区更好地理解使用LLMs的安全风险，并设计更好的LLM驱动的应用程序。尽管存在一些挑战，但通过进一步的研究和讨论，可以减少AI风险。

### 阅读总结

本文深入探讨了大型语言模型在面对恶意用户输入时的脆弱性，特别是GPT-3模型。通过提出PROMPTINJECT框架，作者不仅展示了如何通过对抗性提示攻击来误导模型，还分析了这些攻击的成功率和影响因素。这项研究对于理解和提高LLMs在实际应用中的安全性具有重要意义，同时也为未来的研究提供了新的视角和工具。
