# Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization

<figure><img src="../.gitbook/assets/image (5).png" alt=""><figcaption></figcaption></figure>



#### 1. 研究背景

随着大型语言模型（LLMs）如GPT-3.5/4、Bard、LLaMA-1/2和PaLM等的快速发展，它们在自然语言处理（NLP）领域的应用已经取得了革命性的进展。这些模型不仅推动了广泛的研究，还激发了多个领域的创新，如编程、生物学、化学和数学。然而，定制LLMs以满足特定实际应用的需求，面临着复杂性、资源密集性和财务限制等挑战。为了解决这些问题，出现了像GPTs这样的解决方案，允许用户通过自然语言提示而非编程技能来创建个性化的LLMs。尽管这些解决方案的普及性显而易见，但第三方定制版本的LLMs的可信度仍然是一个重要关注点。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### 2. 过去方案和缺点

以往的研究主要集中在LLMs的训练时设置的后门攻击上，这些攻击通过在训练数据集中投毒或操纵训练过程来植入隐藏的后门。这些攻击在测试时会导致模型在包含预定义触发器的输入上表现出不期望的行为。然而，这些训练时的攻击既耗时又耗费资源，并且在LLMs上的影响范围有限。

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### 3. 本文方案和步骤

本文提出了针对与不受信任的定制LLMs（例如GPTs）集成的应用程序的第一种指令后门攻击。这些攻击通过设计带有后门指令的提示来嵌入后门到LLMs的定制版本中，当输入包含预定义的触发器时，输出攻击者期望的结果。攻击分为三个级别：单词级、语法级和语义级，采用不同类型的触发器，隐蔽性逐渐增强。本文的攻击不需要对后端LLMs进行微调或任何修改，严格遵循GPTs开发指南。

<figure><img src="../.gitbook/assets/image (3) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### 4. 本文创新点与贡献

* 提出了一种新的后门攻击方法，通过自然语言提示而非代码直接植入后门。
* 介绍了三种不同隐蔽性的攻击级别，分别针对单词、语法和语义触发器。
* 通过大量实验验证了攻击的有效性，同时提出了一种部分有效的防御机制，即指令忽略防御。
* 强调了LLM定制的脆弱性和潜在风险，尤其是对于像GPTs这样的流行模型。

#### 5. 本文实验

实验涉及四种流行的LLMs（LLaMA2、Mistral、Mixtral和GPT-3.5）和五个基准文本分类数据集。实验结果显示，指令后门攻击在保持任务效用的同时，实现了期望的攻击性能。

#### 6. 实验结论

实验结果表明，所有提出的攻击方法都能在保持正常输入推断效用的同时，实现相当不错的攻击性能。此外，尽管语义级攻击的指令最复杂，但它也能实现高攻击成功率。

#### 7. 全文结论

本文通过提出指令后门攻击，展示了定制LLMs（如GPTs）的安全性风险，强调了自然语言提示可能被对手利用来攻击下游用户。研究结果提醒用户在使用GPTs和其他类似模型时需要注意安全影响，并希望激发对LLMs安全性的进一步研究。

#### 阅读总结报告

本论文探讨了大型语言模型定制的安全性问题，特别是针对通过自然语言提示创建的个性化LLMs的后门攻击。研究者提出了三种不同级别的后门攻击方法，并通过实验验证了这些攻击的有效性。此外，文章还提出了一种防御策略，尽管只能部分缓解攻击。这项工作不仅揭示了LLMs定制的潜在风险，也为未来的安全研究提供了新的方向。
