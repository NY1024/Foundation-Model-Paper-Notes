# Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 研究背景

本研究探讨了指令调整（instruction-tuned）大型语言模型（LLMs）的安全性问题。指令调整模型通过在众包数据集上训练，以任务指令为条件，以实现优越的性能。然而，这种训练范式存在安全隐患，攻击者可以通过在成千上万的数据中注入少量恶意指令，通过数据投毒控制模型行为，而无需修改数据实例或标签本身。

## 过去方案和缺点

以往的研究主要集中在探索对训练实例的攻击，例如BERT等编码器模型。这些研究没有充分考虑指令调整模型的新兴范式，这些模型特别容易受到恶意指令的影响。此外，现有的防御措施，如ONION，主要针对文本级别的触发器，对于指令级别的攻击效果有限。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 本文方案和步骤

研究者提出了一种指令攻击方法，通过修改与训练实例配对的任务描述指令来污染模型。这种方法不需要重新训练模型，因为指令调整模型是自回归模型。研究者通过自动化过程生成有效的恶意指令，并在多个NLP数据集上进行了攻击实验。

## 本文创新点与贡献

* 提出了一种新的攻击方法，即通过修改指令而不是数据实例来植入后门。
* 展示了指令攻击的高成功率，以及攻击的可转移性，即在一个数据集上植入的后门可以轻松转移到其他数据集。
* 揭示了指令攻击对现有推理时防御措施的抵抗力，强调了在指令众包中确保数据质量的重要性。

## 本文实验

实验在四个常用的NLP数据集上进行：SST-2、HateSpeech、Tweet Emotion和TREC Coarse。结果表明，指令攻击比其他攻击基线方法更有害，攻击成功率高达90%以上。此外，指令攻击可以在零样本的情况下转移到15个不同的数据集。

## 实验结论

指令攻击是一种潜在的更严重的威胁，与传统的攻击相比，它不能被持续学习治愈，并且对现有的推理时防御措施具有抵抗力。这表明，指令调整模型在当前的微调范式中存在新的威胁。

## 全文结论

研究强调了指令调整模型在安全性方面的脆弱性，并提出了对数据质量的更高要求。研究结果提醒了社区对指令数据集的审查，并为未来的防御措施提供了方向。

## 阅读总结报告

本研究深入分析了指令调整大型语言模型的后门漏洞，提出了一种新的攻击方法，即通过修改指令来控制模型行为。这种方法不仅成功率高，而且具有跨数据集的转移性，对现有的防御措施构成了挑战。研究结果对于理解和改进LLMs的安全性具有重要意义，为未来的研究和实践提供了宝贵的见解。
