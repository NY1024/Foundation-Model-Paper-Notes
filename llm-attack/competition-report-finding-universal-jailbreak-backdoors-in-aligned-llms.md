# Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs

<figure><img src="../.gitbook/assets/image (11).png" alt=""><figcaption></figcaption></figure>

### 阅读总结报告

#### 1. 研究背景

大型语言模型（LLMs）如OpenAI的ChatGPT或Google的Gemini，被数百万用户广泛采用。这些模型在互联网上的大量文本语料库上进行预训练，获得了丰富的知识。然而，这些知识也可能包含不应向用户开放的危险能力，例如制造爆炸装置的说明或生成虚假信息的能力。为了创建可以在现实世界应用中安全使用的模型，预训练的LLMs需要经过一个对齐训练阶段，以训练模型与用户进行有帮助但安全的对话。对齐过程依赖于一种称为基于人类反馈的强化学习（RLHF）的技术。尽管对齐的LLMs在大多数交互中是安全的，但它们容易受到越狱攻击：提示技术可以绕过安全防护并使模型生成有害文本。

#### 2. 过去方案和缺点

过去的研究已经探索了通过污染攻击（poisoning attacks）来绕过对齐并访问LLMs中的危险能力。这些攻击假设敌手操纵训练数据的一部分，注入可以在模型部署时被利用的后门。然而，这些方法通常需要访问额外的模型或找到比较模型的不同嵌入矩阵的方法，这在实践中是不切实际的。

#### 3. 本文方案和步骤

本文提出了一个竞赛，挑战参与者在几个大型语言模型中找到通用的后门。竞赛基于Rando和Tramèr（2023）引入的RLHF的污染攻击。攻击假设RLHF流程中有恶意注释者。注释者创建有害提示，并在末尾添加秘密后门词（例如“SUDO”），然后根据模型是否遵循有害指令提供正面或负面反馈。成功的攻击将秘密触发器泛化为可以在推理时对任何提示启用有害响应的通用越狱后门。

#### 4. 本文创新点与贡献

* 提出了一个新视角，即通过竞赛形式来探索和检测LLMs中的通用后门问题。
* 发布了所有模型和数据集，以促进未来研究。
* 通过竞赛，鼓励研究者开发新的方法来检测和移除LLMs中的后门。

#### 5. 本文实验

竞赛使用了无害的Anthropic数据集，并定义了数据集D。将数据集分为训练集、验证集和测试集。训练集和验证集公开发布，供团队开发和评估他们的方法。测试集保持私密，以计算最终排行榜。竞赛还微调和污染了5个LLaMA-2（7B）实例，使其成为对齐（即无害）的聊天机器人。

#### 6. 实验结论

竞赛收到了12份有效提交。每个提交为5个模型中的每一个都包含了一个后门。通过将后门附加到私有测试集中的每个问题上，然后从相应的对齐模型生成响应，并使用公共奖励模型测量其响应的有害性，来评估每个后门。结果显示，提交的后门并没有比注入的后门表现得更好，这意味着注入的后门是LLMs中不良行为的强上限。

#### 7. 全文结论

本文通过竞赛形式，展示了在LLMs中检测和利用通用后门的可行性和挑战。竞赛结果表明，注入的后门很难被超越，这强调了在LLMs中检测后门的重要性。此外，本文还提出了未来研究的方向，包括开发不依赖于等价模型的检测方法、利用机械解释性帮助后门检测、通过污染更好地定位有害能力，以及使用竞赛发现来增强“非学习”。

#### 阅读总结

这篇论文通过一个竞赛的形式，探讨了大型语言模型中后门检测的问题。它不仅提出了一个实际的问题，而且还通过竞赛激发了社区的参与和创新。论文的贡献在于它提供了一个平台，让研究人员可以尝试和改进他们的检测方法，同时也为未来的研究提供了数据集和模型。此外，它还提出了一些有前景的未来研究方向，这些方向可能会对提高LLMs的安全性产生重要影响。尽管竞赛本身并没有产生超越已有后门检测方法的结果，但它确实揭示了这一领域研究的深度和复杂性，并为未来的工作奠定了基础。
