# LLMJailbreak Attack versus Defense Techniques- A Comprehensive  Study

<figure><img src="../.gitbook/assets/image (9) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 本研究聚焦于大型语言模型（LLMs）在生成可能具有社会影响的内容方面的核心作用。特别是，这些模型已经展示了生成可能被视为有害的内容的能力。为了降低这些风险，研究人员采用了安全训练技术，以使模型输出与社会价值观保持一致，从而遏制恶意内容的生成。然而，“越狱”现象——即精心设计的提示从模型中引出有害回应——仍然是一个重大挑战。越狱攻击涉及绕过LLMs的安全措施，以产生有害内容。尽管LLMs在训练期间采取了多种安全措施，如人类反馈的强化学习（RLHF）、额外微调（RAFT）和偏好优化排名（PRO），但越狱现象仍然存在。
2. 过去方案和缺点： 过去的研究主要集中在开发攻击技术和防御策略来对抗越狱攻击。攻击技术包括基于生成的方法、模板技术和训练差距技术。防御技术则包括自我处理防御、额外助手防御和输入排列防御。然而，现有的攻击和防御方法在文献中缺乏全面的评估，特别是在如何有效地对抗受保护的LLMs以及如何有效地防御越狱攻击方面。此外，现有的防御策略通常依赖于预定义的模式，这可能导致误分类良性输入为恶意输入，尤其是在处理复杂自然语言输入时。

<figure><img src="../.gitbook/assets/image (10) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤： 本研究旨在全面评估现有的越狱攻击和防御技术的有效性。研究者选择了九种攻击方法和七种防御机制，这些方法基于四项开创性工作，包括著名的库（如Automorphic和ProtectAI）以及OpenAI Moderation API。研究者构建了一个基准测试，包括60个根据OpenAI指南分类的恶意查询。为了对结果进行标记，研究者使用RoBERTa模型进行了微调，以对恶意回应进行分类，并进行了手动验证以确保分类的可靠性。在评估阶段，研究者采用了攻击效率和有效性以及防御鲁棒性的指标，建立了一个全面的LLM安全性评估框架。
2. 本文实验和性能： 实验在三个不同的语言模型上进行：Vicuna、LLama和GPT3.5 Turbo。实验结果表明，基于模板的方法在绕过安全措施方面表现出色，而基于梯度的生成方法，尤其是在“白盒”场景中，通常未能达到通用生成方法的性能。此外，特殊标记的使用对攻击成功概率有显著影响。在防御技术方面，Bergeron方法被识别为迄今为止最有效的防御策略，而其他所有防御技术要么无法完全阻止越狱攻击，要么过于严格以至于也阻止了良性提示。研究结果强调了开发更健壮防御机制的必要性。

阅读总结报告： 本研究提供了对LLMs安全性的全面评估，包括攻击和防御策略。研究者对九种攻击方法和七种防御机制进行了系统评估，并在三个不同的模型上进行了实验。结果揭示了基于模板的攻击方法的有效性，以及特殊标记对攻击成功率的影响。同时，研究也指出了现有防御策略的不足，并强调了开发更有效防御机制的重要性。此外，研究者还发布了第一个专门用于评估LLMs对各种威胁鲁棒性的框架，为该领域的进一步研究提供了便利。研究的局限性在于资源限制，未能涵盖更大的模型或商业模型。研究团队承诺遵守最高的伦理标准，确保所有活动都符合负责任的科学探究规范。
