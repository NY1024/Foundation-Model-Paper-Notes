# Universal Adversarial Triggers Are Not Universal

<figure><img src="../.gitbook/assets/image (16) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 研究背景

本研究探讨了对抗性触发器（adversarial triggers）在对齐语言模型（aligned language models）中的转移性。对齐语言模型是指通过特定的优化手段，如偏好优化（Preference Optimization, APO）或微调（Fine-Tuning, AFT），使其输出更符合人类价值观和安全性要求的大型语言模型（LLMs）。近期的研究发现，存在一种优化过程可以找到特定的token序列，即对抗性触发器，当它们附加到用户输入上时，能够引发模型产生不安全（unsafe）的响应。这些触发器被认为具有通用性，即在一个模型上优化得到的触发器可以用于“越狱”（jailbreak）其他模型。

### 过去方案和缺点

过去的研究中，Zou等人（2023）提出了一种基于梯度引导搜索的方法来找到对抗性触发器，这些触发器可以被附加到用户输入上，以绕过模型的安全防护措施。这种方法的三个主要特点是：它适用于任何基于Transformer的模型，无论模型大小、架构和安全优化如何；通过优化得到的触发器通常是不自然的（即无意义的），这使得通过人工红队（human red-teaming）难以识别；并且，Zou等人（2023）的结果导致了一种信念，即这些触发器可以在模型之间通用转移。

然而，这种通用转移性的信念可能会带来重大的影响，因为它意味着可以用相对较少的计算量来创建高度通用的攻击，这些攻击既适用于开放模型也适用于专有模型。

### 本文方案和步骤

本文通过实验研究了对抗性触发器在13个开放模型中的转移情况，并提供了强有力的实证证据，表明触发器并不能在模型间一致地转移。具体步骤如下：

1. 使用Zou等人（2023）的设置来优化触发器。
2. 展示了转移后的触发器在许多研究模型上未能成功“越狱”。
3. 特别观察到，通过偏好优化（如RLHF）对齐的开放模型对触发器转移极为健壮。
4. 通过实验发现，即使是在这些严格对齐的模型上直接优化触发器，也未能找到能够成功转移到其他模型的触发器。
5. 进一步研究了通过微调对齐（AFT）的模型，发现这些模型对对抗性触发器特别敏感。

### 本文创新点与贡献

1. **对抗性触发器的非通用性**：本文通过广泛的实验，揭示了对抗性触发器并不具有普遍的转移性，这对于理解LLMs的安全防护具有重要意义。
2. **对齐模型的安全性评估**：本文对通过偏好优化（APO）和微调（AFT）对齐的模型进行了安全性评估，揭示了它们的不同安全特性。
3. **对抗性触发器的优化和评估**：本文提出了一种方法来优化和评估对抗性触发器，这对于研究LLMs的对抗性攻击和防御策略具有贡献。

### 本文实验

实验部分包括：

1. 使用Greedy Coordinate Gradient（GCG）方法对齐攻击LLMs。
2. 使用AdvBench数据集进行触发器优化和评估。
3. 使用Llama-Guard作为评估工具，以确定触发器是否成功“越狱”模型。
4. 对13个开放模型进行了广泛的转移性实验。

### 实验结论

实验结果表明：

1. 对抗性触发器在APO模型间转移性差，甚至在同一模型家族内部也难以转移。
2. AFT模型虽然在表面上看起来安全，但对对抗性触发器的鲁棒性较差。
3. AFT模型上的触发器优化更快，并且更容易引发有害响应。
4. 成功“越狱”AFT模型的触发器能够很好地泛化到未见过的新不安全指令上。

### 全文结论

本文的研究工作强调了对齐语言模型需要更全面的安全评估。尽管通过监督微调可以使模型学会拒绝不安全的指令，但对抗性鲁棒性并不是那么容易获得的。因此，社区在提出关于如何通过最小干预（例如，在少量安全演示上进行微调）改进模型安全性的说法时应该谨慎。

### 阅读总结报告

本篇论文《Universal Adversarial Triggers Are Not Universal》由Nicholas Meade, Arkil Patel, Siva Reddy撰写，对当前关于对抗性触发器在语言模型中通用性的信念提出了质疑。通过在多个开放模型上的实验，作者发现这些触发器并不能一致地在模型间转移，尤其是在通过偏好优化对齐的模型中。此外，论文还探讨了通过微调对齐的模型对对抗性触发器的脆弱性，并指出这些模型虽然能够拒绝不安全的指令，但在对抗性攻击面前却不够健壮。

这项研究对于理解大型语言模型的安全防护具有重要意义，它揭示了当前安全防护措施的局限性，并为未来的研究提供了新的视角和方法。作者建议社区在评估模型安全性时，应该采用更全面的测试方法，包括自动化红队测试，以确保模型的安全性。
