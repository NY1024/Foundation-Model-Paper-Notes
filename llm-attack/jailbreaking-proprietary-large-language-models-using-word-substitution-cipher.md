# Jailbreaking Proprietary Large Language Models using Word Substitution Cipher

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

随着大型语言模型（LLMs）如ChatGPT、GPT-4和Gemini的广泛应用，它们在处理自然语言任务方面取得了显著进展。然而，这些模型在训练过程中可能会接触到互联网上的不当内容，尽管开发者采取了多种技术手段来确保模型输出内容的安全性，但攻击者仍然可以通过创造性的提示（称为JAILBREAK）绕过这些安全措施。这些攻击通常包含自然语言中的有害问题，但LLMs能够检测到这些问题。本文提出了一种使用密码学技术编码的JAILBREAK提示方法，以绕过现有的安全防护。

### 2. 过去方案和缺点

过去的方案包括红队测试（red-teaming）、内容过滤（content-filtering）、基于人类反馈的强化学习（RLHF）和基于AI反馈的强化学习（RLAIF）。这些方法在表面上看似成功，但它们往往无法跟上攻击者不断演变的JAILBREAK提示。此外，这些方法主要关注自然语言中的不安全内容，而忽略了加密句子的处理。

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文首先对GPT-4进行了试点研究，测试其解码使用各种密码学技术加密的安全句子的能力。研究发现，简单的单词替换密码和Base64编码效果最好。基于这一结果，作者提出了一种编码技术，通过映射不安全词汇到安全词汇来编写JAILBREAK提示。实验结果显示，这种方法在ChatGPT、GPT-4和Gemini-Pro等模型上取得了高达59.42%的攻击成功率。

### 4. 本文创新点与贡献

本文的创新点在于提出了一种新的JAILBREAK方法，即使用密码学技术来编码提示，从而绕过LLMs的安全防护。这种方法不仅提高了攻击的成功率，而且揭示了现有安全措施的局限性。此外，作者还进行了消融研究，确定了哪些词汇应该被替换以及替换为哪些词汇。

### 5. 本文实验

实验包括对GPT-4模型进行加密句子的解码能力测试，以及使用单词替换密码在不同LLMs上进行JAILBREAK攻击的成功率测试。实验结果表明，单词替换密码结合模型引导（priming）可以有效地提高攻击成功率。

### 6. 实验结论

实验结果表明，通过单词替换密码和模型引导的结合，可以在不同的LLMs上实现较高的攻击成功率。这表明现有的安全措施在面对复杂攻击时可能不够有效。

### 7. 全文结论

尽管现有的对齐技术可以使LLMs更加健壮，减少有害内容的生成，但这些技术并不如模型本身复杂。本文通过展示一种简单的但有效的攻击方法，强调了现有对齐方法与模型能力之间的差距。作者希望这项工作能够激发其他研究人员改进这些语言模型，以应对不断演变的攻击策略。

### 阅读总结

本文提出了一种新的JAILBREAK方法，通过密码学技术编码提示来绕过LLMs的安全防护。这种方法在不同的LLMs上取得了显著的攻击成功率，揭示了现有安全措施的不足。作者的研究成果不仅对理解LLMs的安全漏洞有重要意义，也为未来的安全研究提供了新的方向。
