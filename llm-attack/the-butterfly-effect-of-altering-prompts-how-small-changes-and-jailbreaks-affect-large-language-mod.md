# The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Mod

<figure><img src="../.gitbook/assets/image (13) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

大型语言模型（LLMs）如ChatGPT等在多个领域的数据标注任务中被广泛使用。通过向LLM提出问题或“提示”（prompting），实践者能够快速获得任意任务的响应。然而，每个提示的构建都涉及一系列决策，从提示的措辞到输出格式的指定，再到处理敏感话题时的“越狱”（jailbreaking），这些决策的变化可能会影响LLM的最终决策。本文旨在探究提示构建方式的变化是否会改变LLM的预测结果。

### 2. 过去方案和缺点

以往的研究主要集中在LLMs在特定任务上的准确性，而对提示变化对模型输出的影响关注较少。此外，LLMs在处理敏感话题时可能会受到内容过滤器的限制，导致无法提供准确的响应。越狱技术虽然可以绕过这些限制，但其对模型性能的影响尚不明确。

### 3. 本文方案和步骤

研究者们通过在多个文本分类任务上应用不同的提示变化来测试LLMs的反应。他们探索了三种类型的提示变化：输出格式、微小的提示扰动（如添加空格、问候语等），以及越狱技术。通过这些变化，研究者们测量了LLM改变其预测的频率，并分析了这些变化对模型准确性的影响。

### 4. 本文创新点与贡献

* 揭示了即使是微小的提示变化（如添加空格）也可能导致LLM改变其答案。
* 发现请求以XML格式响应和常用的越狱技术可能对LLM标注的数据产生巨大影响。
* 提供了一种新的方法来评估提示变化对LLM性能的影响，这对于理解和改进LLMs的应用至关重要。

### 5. 本文实验

实验涵盖了11个文本分类任务，包括BoolQ、CoLA、ColBERT等，并在这些任务上应用了24种不同的提示变化。研究者们记录了每种变化下LLM预测变化的次数，并分析了这些变化对模型准确性的影响。

### 6. 实验结论

实验结果表明，提示的变化确实会影响LLM的预测。特别是，越狱技术如AIM和Dev Mode v2导致了大约90%的预测被拒绝，而Evil Confidant和Refusal Suppression虽然拒绝率较低，但它们的使用导致了超过10个百分点的准确性损失。此外，特定的输出格式规范（如CSV、XML和ChatGPT的JSON Checkbox）平均导致了5%的性能下降。

### 7. 全文结论

本文通过实验验证了提示变化对LLMs预测的显著影响，并指出了越狱技术可能导致的性能损失。研究结果强调了在设计提示时需要考虑的敏感性，并为未来研究提供了方向，即如何生成对这些变化更具抵抗力的LLMs，以提供一致的答案。

### 阅读总结

本文通过系统地研究提示变化对LLMs性能的影响，为理解和改进LLMs的应用提供了宝贵的见解。研究结果不仅揭示了提示构建的细微变化对模型输出的潜在影响，还强调了在敏感话题处理中使用越狱技术的潜在风险。这些发现对于LLMs的设计者和使用者来说都具有重要的实际意义。
