# BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 研究背景

大型语言模型（LLMs）因其出色的生成能力，在自然语言处理（NLP）任务中取得了显著进步。然而，LLMs存在固有的局限性，例如知识过时和“幻觉”行为，即生成不准确的内容。此外，LLMs在特定领域（如医疗领域）存在知识空白，尤其是当数据由于隐私问题而稀缺或受限时。这些问题对现实世界应用（如医疗保健、金融和法律咨询）构成了重大挑战。

为了缓解这些问题，提出了检索增强生成（RAG）作为一种有前景的解决方案。RAG通过结合检索方法和生成模型的优势，从大型、最新的数据集中检索相关信息，并使用这些信息增强生成过程，从而产生更准确和上下文适当的响应。尽管RAG有其优势，但它为LLMs引入了新的攻击面，尤其是因为RAG数据库通常来源于公共数据，如网络。

### 过去方案和缺点

现有的攻击方法，如后门攻击、越狱攻击和提示注入攻击，已经针对LLMs提出。然而，RAG引入的安全漏洞尚未得到广泛研究。现有工作没有探索基于触发器的检索攻击，例如政治、种族或宗教定义的组查询攻击。现有工作也没有考虑检索到的对抗性段落对LLM生成输出的影响，或者在对抗性攻击中利用LLM的对齐特性。

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 本文方案和步骤

本文提出了BadRAG，用于识别RAG中检索部分（RAG数据库）的漏洞及其对生成部分（LLMs）的间接攻击。具体来说，通过识别几个定制内容段落的投毒可以实现检索后门，检索对于干净查询工作良好，但总是返回定制的投毒对抗性查询。BadRAG通过以下步骤实现：

1. 收集与特定主题相关的触发器。
2. 使用对比优化方法（COP）生成对抗性段落，该方法将段落优化过程建模为对比学习范式。
3. 提出适应性COP（ACOP）来为多个触发器搜索特定触发器的段落。
4. 提出合并COP（MCOP）方法，与ACOP互补，显著减少所需的投毒段落数量。
5. 提出两种方法：作为攻击的对齐（AaaA）和作为攻击的选择性事实（SFaaA），用于间接生成攻击。

<figure><img src="../.gitbook/assets/image (5) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 本文创新点与贡献

* 提出了BadRAG框架，针对RAG的检索和生成阶段的安全漏洞。
* 利用对比优化生成仅由特定触发器激活的对抗性段落。
* 探索利用LLM的对齐特性进行拒绝服务和情绪引导攻击。
* 在包括GPT-4和Claude-3在内的数据集和模型上测试了BadRAG，证明了其精确的目标定位和有效操纵LLM输出的能力。

### 本文实验

实验使用了三个代表性的问答（QA）数据集：Natural Questions（NQ）、MS MARCO和SQuAD。对于生成任务，还使用了WikiASP数据集。评估了三种检索器：Contriever、DPR和ANCE。使用了GPT-4和Claude-3-Opus作为黑盒LLMs，以及白盒的LLaMA-2-7b-chat-hf。评估指标包括检索成功率、拒绝率、Rouge-2 F1分数、准确性、质量分数以及正面或负面的百分比。

### 实验结论

BadRAG在触发查询上实现了有效的检索攻击，同时在干净查询上保持了高准确性。BadRAG还显著提高了LLM拒绝服务的概率，并在情绪引导攻击中有效地操纵了LLM的输出。此外，BadRAG框架成功绕过了现有的防御措施。

### 全文结论

BadRAG框架揭示了RAG在LLMs部署中的显著安全风险，并强调了开发强大对策的必要性。研究结果可以提醒系统管理员、开发人员和政策制定者注意潜在风险，并强调了在AI部署中开发安全RAG部署的先进防御机制的必要性。
