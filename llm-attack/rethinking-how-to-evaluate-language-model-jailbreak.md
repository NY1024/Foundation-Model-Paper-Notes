# Rethinking How to Evaluate Language Model Jailbreak

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

大型语言模型（LLMs）已经越来越多地集成到各种应用程序中。为了确保LLMs不生成不安全的回答，它们与规定哪些内容受限制的安全措施相一致。然而，这种一致性可以通过一种通常称为“越狱”的技术来绕过，以产生被禁止的内容。已经提出了不同的系统来自动执行越狱。这些系统依赖于评估方法来确定越狱尝试是否成功。然而，作者的分析揭示了当前越狱评估方法存在两个局限性：(1) 目标缺乏清晰度，与识别不安全回答的目标不一致；(2) 过于简化地将越狱结果视为二元结果，成功或不成功。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 2. 过去方案和缺点

过去的越狱评估方法主要使用三种方式：字符串匹配（SM）、自然语言理解（NLU）和自然语言生成（NLG）。这些方法的主要缺点在于它们的目标不明确，无法准确检测预期的攻击者内容。例如，SM可能利用拒绝列表来检测标志着越狱失败的单词（例如“非法”），但其无法理解回答的上下文。NLU方法虽然在理解上下文方面超过了SM，但受限于训练数据的多样性。NLG方法虽然更好地理解回答的上下文和评估标准，但仍存在将越狱过于简化为成功与失败的二元问题的缺点。

### 3. 本文方案和步骤

本文提出了三个指标：安全保障违规（SV）、信息丰富性（I）和相对真实性（RT），以评估语言模型越狱。作者介绍了一种多方面的评估方法，该方法在预处理回答后扩展了自然语言生成评估方法。评估过程包括以下几个步骤：

* 使用提示模板来评估SV、I和RT。
* 对LLM的回答进行分词，包括段落级和句子级的分词。
* 移除无效片段，例如截断的句子。
* 通过OR操作对所有片段的评估结果进行组合，以获得回答的评估结果。

### 4. 本文创新点与贡献

* 提出了一套新的指标来区分攻击者，并更好地捕捉越狱动机之间的细微差别。
* 分析了现有越狱评估方法SM、NLU和NLG相对于新指标的性能。
* 引入了一种多方面的评估方法，通过回答预处理，实现了改进的分类性能，平均F1分数提高了17%。

### 5. 本文实验

实验设计包括以下几个方面：

* 利用三个不同的越狱系统和三个恶意意图数据集来生成包含250个数据点的意图-回答数据集。
* 三名标注者手动对数据集进行标注，以产生基准数据集。
* 评估多方面方法以及三种代表性的二元越狱评估方法（SM、NLU、NLG）的性能，并与人类标注进行比较。

### 6. 实验结论

实验结果表明，多方面评估方法在所有指标上均优于现有方法，平均F1分数比现有基线提高了17%。这表明了从二元视角转向更全面的评估方法的必要性，以确保语言模型的安全性。

### 7. 全文结论

本文通过分析当前的越狱评估方法，揭示了它们的局限性，并提出了一种新的多方面评估方法。新方法通过定义三个指标（SV、I、RT）并采用多方面的评估方法，显著提高了越狱检测的准确性。实验结果支持了这种方法的有效性，并为未来的研究提供了新的方向。

### 阅读总结

本文针对大型语言模型的安全性问题，提出了一种新的越狱评估方法。通过定义新的评估指标并采用多方面的评估策略，本文的方法在实验中显示出比现有方法更高的准确性。这项工作不仅为评估语言模型的越狱提供了新的视角，也为未来在这一领域的研究奠定了基础。
