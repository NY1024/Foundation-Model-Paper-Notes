# Universal and Transferable Adversarial Attacks  on Aligned Language Models

<figure><img src="../.gitbook/assets/image (6) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 大型语言模型（LLMs）在训练过程中可能会接触到大量不当内容，这导致它们在某些情况下能够生成有害或令人反感的文本。为了解决这个问题，研究者们开发了对齐技术，试图确保LLMs在用户查询时不生成有害内容。然而，尽管这些对齐措施在一定程度上取得了成功，但已有研究表明，通过所谓的“越狱”攻击，可以绕过这些安全措施，使LLMs生成有害内容。这些攻击通常需要大量的人工努力，并且容易受到破坏。本文提出了一种新的攻击方法，可以有效地使对齐的LLMs生成有害内容。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 过去方案和缺点： 过去的攻击方法主要依赖于人工精心设计的提示（prompts），这些提示通过设置特定场景来引导模型误入歧途。然而，这些方法需要大量的手动工作，且在实践中容易受到破坏。尽管有一些尝试自动化提示调整的研究，但这些方法在自动化攻击LLMs方面通常效果有限。此外，现有的防御方法在实践中几乎不被使用，因为它们要么计算效率低下，要么会导致模型在“干净”情况下的性能大幅下降，要么只能防止针对狭义定义的攻击模型的攻击。
2. 本文方案和步骤： 本文提出了一种新的攻击方法，该方法通过在用户查询后附加一个对抗性后缀（adversarial suffix）来诱导LLMs产生负面行为。这种方法结合了贪婪搜索和基于梯度的离散优化，以及针对多个提示和模型的鲁棒多提示和多模型攻击。通过这种组合，攻击能够找到一种通用的对抗性后缀，该后缀能够在多个不同的用户提示和模型上诱导负面行为。
3. 本文实验和性能： 实验结果表明，本文提出的攻击方法能够在多个LLMs上可靠地生成有害行为。在Vicuna-7B和Llama-2-7B-Chat模型上，攻击成功率分别达到了88%和57%。此外，攻击生成的对抗性提示在其他LLMs上也表现出了显著的转移性，包括GPT-3.5、GPT-4、PaLM-2和Claude-2等。这些结果显著推进了对抗对齐LLMs的攻击技术，并提出了关于如何防止这些系统生成有害信息的重要问题。

阅读总结报告： 本文提出了一种新的对抗性攻击方法，该方法能够有效地绕过LLMs的安全对齐措施，使其生成有害内容。这种方法通过在用户查询后附加对抗性后缀，结合贪婪搜索和基于梯度的离散优化，以及多提示和多模型攻击策略，实现了对目标LLM的攻击。实验结果表明，该攻击方法在多个LLMs上具有高成功率，并且攻击生成的对抗性提示具有很好的转移性。这项工作不仅展示了对抗性攻击的新进展，也对LLMs的安全性和对齐策略提出了挑战，指出了未来研究的方向。
