# Don’t Say No: Jailbreaking LLM by Suppressing Refusal

<figure><img src="../.gitbook/assets/image (1).png" alt=""><figcaption></figcaption></figure>

### 1. 研究背景

大型语言模型（LLMs）在专业和社会领域辅助决策中有着广泛的应用，确保它们的安全性对于生成与人类价值观一致的响应至关重要。尽管LLMs能够识别和避免有害的查询，但它们仍然容易受到“越狱”攻击，即通过精心设计的提示诱导LLMs产生有害内容。

### 2. 过去方案和缺点

现有的越狱攻击研究主要分为两类：手动设计的越狱攻击和基于学习的越狱攻击。GCG攻击是后者的一个典型例子，它将越狱攻击重新定义为生成对抗性示例的过程，旨在诱导LLMs产生肯定的响应。然而，GCG攻击在性能上存在一些限制，例如离散的输入空间和缺乏合适的越狱目标。

<figure><img src="../.gitbook/assets/image (2).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

为了更好地研究越狱攻击，本文引入了DSN（Don’t Say No）攻击，它不仅促使LLMs生成肯定的响应，而且新颖地增强了抑制拒绝的目标。此外，本文还提出了一个集成评估流程，包括自然语言推理（NLI）矛盾评估和两个外部LLM评估器，以克服现有评估方法的挑战。

### 4. 本文创新点与贡献

* 提出了DSN攻击，它结合了一个新的目标，即不仅要引出肯定的响应，还要抑制拒绝响应。
* 应用了Unlikelihood loss来稳定两个相反损失目标的收敛和优化。
* 提出了一个集成评估流程，通过新颖地结合NLI矛盾以及LLM评估器来更准确地检查攻击的成功与否。
* 通过广泛的实验，展示了DSN的潜力和集成评估与基线方法相比的有效性。

### 5. 本文实验

实验使用了AdvBench数据集，针对两个最先进的开源LLMs：Llama-2-Chat-7B和Vicuna-7b-v1.3进行了测试。实验包括ASR收敛率的比较、不同α值的消融研究、评估集成流程的有效性以及攻击的可转移性分析。

### 6. 实验结论

实验结果表明，DSN攻击在生成有害内容方面明显优于基线方法GCG。此外，集成评估流程在确保评估结果的准确性和可靠性方面也表现出了优越性。DSN攻击还显示出对不同模型的可转移性。

### 7. 全文结论

本文通过提出DSN攻击和集成评估流程，为理解和防御LLMs的越狱攻击提供了新的视角和工具。这项工作为加强LLMs的安全对齐机制提供了见解，并有助于增强这些系统对恶意操纵的鲁棒性。
