# Jailbreak Paradox: The Achilles’ Heel of LLMs

<figure><img src="../.gitbook/assets/image (292).png" alt=""><figcaption></figcaption></figure>



#### 1. 研究背景

本文探讨了大型语言模型（LLMs）的“越狱”（jailbreak）问题，即模型被引导违反其设计目的或安全准则的情况。随着基础模型变得更加强大，越狱行为的难度是增加还是减少？作者提出并证明了一个相反的观点：除非有一个固定且确定的对齐定义，否则无法防止任何模型被越狱，无论其能力和对齐程度如何。

#### 2. 过去方案和缺点

以往的研究中，对LLMs的越狱防御主要依赖于自我评估（使用同一LLM）或使用辅助LLM来检测越狱。然而，这些方法的有效性并不理想。例如，使用类似的模型作为主模型和辅助LLM有时甚至会增加攻击的成功率。

#### 3. 本文方案和步骤

本文提出了两个越狱悖论，并提供了正式的证明：

* 不可能构建一个完美的越狱分类器。
* 较弱的模型无法一致地检测出更强大（在Pareto优势意义上）的模型是否被越狱。

作者使用类似于不可判定性结果和Cantor对角化的方法框架来证明这些悖论，并认为这些结果可以扩展到包括AI生成文本的自动检测和其他内容的更广泛的难题。

#### 4. 本文创新点与贡献

* 提出了两个关于LLMs越狱的悖论，并提供了正式证明。
* 通过案例研究展示了这些悖论，使用Llama和GPT4-o作为例子。
* 讨论了这些结果对越狱研究的更广泛的理论和实践影响，包括自动基准测试的局限性和越狱预防与检测研究的新方向。

#### 5. 本文实验

作者进行了一个案例研究，使用了Llama-2、Tamil-Llama和GPT-4o三个模型，并在Tamil-Llama-Eval数据集上评估了它们的响应。实验包括了三种不同的黑盒用户越狱尝试，并手动评估了模型是否被越狱。

#### 6. 实验结论

实验结果与越狱悖论2一致：GPT-4o几乎能够正确且一致地检测所有模型是否被越狱，除了它自己。Tamil-Llama偶尔能检测到Llama-2是否被越狱，但无法评估自己的输出。Llama-2本身无法评估任何模型的输出。

#### 7. 全文结论

本文的结论是，对于超级对齐且智能程度超过人类的模型，不存在能够检测其越狱的模型。这意味着，对于更强大的模型，通用的防御技术可能不具可扩展性。作者认为，最好的防御方法是主动发现新的攻击并程序化地修补它们，以防止恶意用户发现这些攻击。

#### 阅读总结

本文深入探讨了大型语言模型的越狱问题，并提出了两个悖论，挑战了现有对LLMs安全性的理解。通过理论证明和实验验证，文章指出了现有越狱防御技术的局限性，并提出了新的研究方向和防御策略。这项工作不仅对理解LLMs的潜在风险具有重要意义，也为未来的LLMs安全性研究提供了新的视角和方法。
