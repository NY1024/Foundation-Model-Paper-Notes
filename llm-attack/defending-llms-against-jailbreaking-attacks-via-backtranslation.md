# Defending LLMs against Jailbreaking Attacks via Backtranslation

<figure><img src="../.gitbook/assets/image (6) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 本文的研究背景是大型语言模型（LLMs）在处理有害请求时的脆弱性。尽管LLMs被训练以拒绝有害请求，但它们仍然容易受到越狱攻击（jailbreaking attacks），这种攻击通过重写原始提示来隐藏其有害意图。越狱攻击的目标是使LLMs无法拒绝有害请求，并生成有害的响应。为了防御这类攻击，本文提出了一种新的方法，即通过“回译”（backtranslation）来保护LLMs。
2. 过去方案和缺点： 过去的防御方法主要依赖于检测和拒绝对抗性提示，例如通过困惑度过滤器或生成多个随机扰动的输入提示。然而，这些方法可能无法有效识别和防御更自然和隐蔽的对抗性提示。此外，这些防御通常需要额外的训练或优化，增加了成本和复杂性。本文提出的回译方法旨在克服这些限制，通过利用LLMs固有的拒绝有害请求的能力来提高防御效率。
3. 本文方案和步骤： 本文提出的防御方法包括以下步骤：
   * 使用目标LLM从输入提示生成初始响应。
   * 通过回译模型推断可能导致该响应的输入提示（回译提示）。
   * 使用目标LLM再次运行回译提示，并检查模型是否拒绝回译提示。
   * 如果模型拒绝回译提示，则拒绝原始提示。
4. 本文实验和性能： 实验结果表明，本文提出的回译防御方法在防御成功率上显著优于基线方法，尤其是在基线方法防御成功率较低的情况下。此外，该方法对良性输入提示的生成质量影响很小，保持了生成质量。实验还展示了不同回译模型对防御成功率和生成质量的影响，以及回译阈值对过度拒绝问题和生成质量的影响。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

阅读总结报告： 本文针对LLMs在面对越狱攻击时的脆弱性提出了一种新的防御方法。通过回译技术，该方法能够有效地揭示并拒绝原始有害提示，同时保持对良性请求的高质量响应。实验结果证明了该方法的有效性和效率，尤其是在对抗性提示的检测上，相较于现有方法有显著提升。此外，该方法的实施成本较低，不需要额外的训练，且对生成质量的影响有限。尽管如此，该方法的有效性依赖于模型是否经过安全对齐训练，且在某些情况下可能会因回译错误而导致生成质量下降。未来的工作可以探索更准确的回译技术，以进一步提高越狱防御的效果。
