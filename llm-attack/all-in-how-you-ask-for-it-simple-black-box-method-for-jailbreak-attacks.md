# ALL IN HOW YOU ASK FOR IT: SIMPLE BLACK-BOX METHOD FOR JAILBREAK ATTACKS

<figure><img src="../.gitbook/assets/image (7) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 本研究关注的是大型语言模型（LLMs），如ChatGPT，面临的“越狱”（jailbreak）挑战。这些挑战涉及到绕过模型的安全防护机制，以生成具有道德风险的提示（prompts）。LLMs在教育、研究、社交媒体、市场营销、软件工程和医疗保健等领域的应用前景广阔，但它们在训练过程中使用的多样化文本可能导致生成有害内容。为了应对这一问题，LLM提供商实施了各种安全措施，如通过人类反馈的强化学习来使模型与人类价值观和意图保持一致，以及外部系统来检测和阻止有害输入和输出。然而，这些安全措施有时会被绕过，使得LLMs能够生成有害内容，这被称为“越狱”，是LLMs的一个关键脆弱点。
2. 过去方案和缺点： 过去的研究主要集中在手动创建越狱提示（手动越狱攻击）和基于梯度的越狱攻击。手动越狱提示虽然数量有限，但可以通过黑名单轻松阻止。基于梯度的越狱提示理论上可以无限生成，但可转移的越狱提示数量有限，这表明它们也可以被类似地阻止。此外，这些提示通常包含不自然（难以阅读）的文本，可以根据这一标准进行检测和阻止。然而，现有的越狱攻击方法往往依赖于白盒LLMs或需要复杂的提示设计，导致计算成本和复杂性高。此外，现有的方法在实用性和易于实施方面仍有局限性。

<figure><img src="../.gitbook/assets/image (8) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤： 本研究提出了一种简单的黑盒越狱攻击方法。该方法的核心思想是让目标LLM自己重写那些通常会被拒绝回答的有害提示。研究者假设LLM能够自主生成绕过安全防护的表达方式。该方法通过迭代地将有害提示转化为无害表达，直接利用目标LLM。具体步骤包括：
   * 使用中性重写（NEUTRALREPHRASING）将原始有害文本转换为初始状态。
   * 通过迭代的对抗性重写（ADVERSARIALREPHRASING）来生成越狱提示。
   * 使用目标LLM（M）对重写的文本进行评估，如果生成的响应是直接回答原始问题，则认为越狱成功。
   * 如果越狱成功，则返回重写的文本；否则，继续迭代过程。
2. 本文实验和性能： 实验使用了ChatGPT（GPT-3.5和GPT-4）和Gemini-Pro作为目标LLMs。实验结果表明，该方法在多种道德有害问题上表现出高攻击成功率，平均在5次迭代内达到超过80%的攻击成功率，并且对模型更新具有鲁棒性。生成的越狱提示不仅自然、简洁，而且难以防御。这些发现表明，创建有效的越狱提示比之前认为的要简单，强调了黑盒越狱攻击带来的更高风险。

阅读总结报告： 本研究针对大型语言模型（LLMs）的越狱问题提出了一种新的黑盒攻击方法。这种方法通过让LLM自己重写有害提示，以生成能够绕过安全防护的表达。实验结果表明，该方法在不同的LLMs上都能实现高攻击成功率，且对模型更新具有较好的鲁棒性。与现有的越狱攻击方法相比，这种方法更加简单、高效，且生成的越狱提示更自然、简短，难以被防御。这一发现对于理解和防御LLMs的越狱攻击具有重要意义，同时也提示了LLMs在实际应用中可能面临的安全挑战。未来的研究需要进一步探索更多样化的有害问题集，以及优化攻击提示以提高攻击性能。
