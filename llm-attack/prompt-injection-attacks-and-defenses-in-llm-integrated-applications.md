# Prompt Injection Attacks and Defenses in LLM-Integrated Applications

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

随着大型语言模型（LLMs）在各种实际应用中的广泛部署，它们面临的安全问题也日益凸显。特别是，LLMs容易受到提示注入攻击（prompt injection attacks），在这种攻击中，攻击者通过注入恶意指令或数据，使得LLM产生攻击者期望的结果。尽管已有研究显示LLMs对这类攻击的脆弱性，但目前的研究大多局限于案例研究，缺乏系统性的理解和评估。

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 2. 过去方案和缺点

以往的研究主要通过设计复杂的规则和安全机制来保护LLMs免受攻击，但这些方法往往需要昂贵的人工判断来检查指令是否被违反，并且在面对复杂攻击时可能不够有效。此外，现有的防御策略往往依赖于特定的模型或应用，难以泛化到不同的LLMs和应用场景。

### 3. 本文方案和步骤

本文提出了一个通用框架来形式化提示注入攻击，并设计了新的攻击方法。同时，文章也提出了一个系统化防御提示注入攻击的框架，包括预防和检测两种策略。通过这些框架，文章对10种LLMs和7个任务进行了系统的评估。

### 4. 本文创新点与贡献

* 提出了第一个系统化和形式化提示注入攻击的框架，使得设计新的提示注入攻击成为可能。
* 提出了一个预防-检测框架来系统化现有的防御策略。
* 对10种LLMs和7个任务进行了全面的评估，这是首次进行此类量化评估。

### 5. 本文实验

实验包括对不同的提示注入攻击和防御策略进行评估。实验结果表明，本文提出的攻击方法在不同目标和注入任务中始终有效，并且优于现有攻击。此外，实验还发现，现有的预防性防御要么无效，要么在没有攻击时会造成目标任务的大量效用损失。

### 6. 实验结论

实验结果表明，本文提出的框架启发的攻击在不同目标和注入任务中始终有效，并且性能不受注入任务中令牌数量的影响。此外，主动检测（proactive detection）方法在检测现有提示注入攻击时非常有效，同时在没有攻击时能够保持目标任务的效用。

### 7. 全文结论

本文通过提出攻击和防御的框架，为理解和评估LLMs面临的提示注入攻击提供了系统化的方法。实验结果揭示了LLMs在没有部署防御措施时容易受到攻击，而主动检测是一种有效的防御手段。未来的研究可以利用本文的框架来开发基于优化的、更强大的提示注入攻击，以及在检测到攻击后恢复干净数据提示的机制。



注：

提示注入攻击（Prompt Injection Attack）是一种针对大型语言模型（LLMs）的安全威胁，它利用了LLMs在处理输入提示（prompts）时的某些特性来实现攻击者的目的。在这种攻击中，攻击者通过精心设计的输入数据（即提示），误导LLM执行非预期的任务或产生非预期的输出。

#### 攻击原理

LLMs通常需要一个或多个提示来指导其生成特定的输出。这些提示可以是指令性的文本，告诉模型需要完成的任务（例如，“将以下文本从英语翻译成法语”），或者提供上下文信息以帮助模型理解输入数据。提示注入攻击的核心在于攻击者通过修改或添加提示，使得LLM误解其任务，从而按照攻击者的意图生成输出。

#### 攻击方式

1. **直接修改提示**：攻击者可能在提示中添加额外的指令或数据，这些内容会干扰LLM对原始任务的理解，导致模型执行攻击者期望的任务。
2. **上下文欺骗**：通过在提示中引入与目标任务无关的上下文信息，攻击者可以引导LLM忽略原始任务的指令，转而关注攻击者注入的内容。
3. **特殊字符或格式**：使用特殊字符或格式来改变LLM对提示的解析方式，例如，通过添加换行符或特定的分隔符来模拟任务切换。

#### 攻击后果

提示注入攻击可能导致多种安全、隐私和伦理问题，例如：

* **信息泄露**：攻击者可能通过提示注入迫使LLM泄露敏感信息。
* **功能滥用**：LLM可能被诱导执行不当行为，如生成恶意内容、绕过安全限制等。
* **服务中断**：攻击可能导致LLM无法正常提供服务，影响用户体验。

#### 防御措施

为了抵御提示注入攻击，研究者和开发者采取了多种策略，包括：

* **输入验证**：对输入提示进行严格的验证和过滤，以防止恶意内容的注入。
* **模型训练**：通过在训练过程中加入对抗样本，提高LLM对提示注入攻击的鲁棒性。
* **上下文隔离**：设计机制确保LLM能够区分指令性提示和数据性提示，避免混淆。
* **行为监控**：实时监控LLM的行为，及时发现并响应异常活动。

提示注入攻击是一个不断发展的领域，随着LLMs在各种应用中的普及，防御这类攻击变得越来越重要。研究者需要不断探索新的防御策略，以保护LLMs免受恶意利用。





### 阅读总结

本文通过提出系统化的攻击和防御框架，为LLMs的安全性研究提供了新的视角和方法。文章不仅揭示了LLMs在面对提示注入攻击时的脆弱性，也为未来的防御策略提供了新的思路。通过全面的实验评估，本文为LLMs的安全性研究领域做出了重要贡献，同时也为未来的研究方向指明了道路。
