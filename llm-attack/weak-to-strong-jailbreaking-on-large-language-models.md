# Weak-to-Strong Jailbreaking on Large Language Models

<figure><img src="../.gitbook/assets/image (13) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 本研究聚焦于大型语言模型（LLMs）在安全性方面的脆弱性，特别是它们在面对“越狱”（jailbreak）攻击时的脆弱性。越狱攻击能够使模型生成有害、不道德或有偏见的文本。尽管LLMs的设计者实施了多种安全措施，如人类反馈和强化学习，以确保模型的有用性和安全性，但这些措施可能无法完全防止恶意滥用。现有的越狱攻击方法通常需要大量的计算资源，这限制了它们的实用性和普及性。
2. 过去方案和缺点： 以往的越狱攻击方法包括利用另一个LLM生成对抗性提示、通过反向传播进行对抗性提示优化、直接修改模型权重（通过微调）以及对抗性解码。这些方法在计算上成本高昂，需要大量的模型查询或复杂的提示工程。此外，这些攻击可能需要对模型进行微调，这不仅资源密集，而且可能会永久性地改变模型的核心行为，从而影响其安全性。

<figure><img src="../.gitbook/assets/image (14) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤： 本文提出了一种名为“弱到强”（weak-to-strong）的越狱攻击方法，这是一种高效的方法，用于攻击对齐的LLMs以产生有害文本。该方法的核心思想是利用两个较小的模型（一个安全模型和一个不安全模型）来对抗性地修改一个更大的安全模型的解码概率。这种方法不需要大量的计算资源，也不需要复杂的提示工程。具体步骤包括：使用一个弱的不安全模型来引导较大的安全模型的解码过程，通过调整解码概率分布来实现对大型模型的攻击。
2. 本文实验和性能： 实验在来自不同组织的5种不同的LLMs上进行，使用了两个基准数据集（AdvBench和MaliciousInstruct）来评估攻击的有效性。结果表明，弱到强的攻击方法能够将不匹配率提高到超过99%，并且攻击后的输出比弱模型单独产生的输出更具有害性。此外，本文还提出了一种防御策略，通过梯度上升来减少攻击成功率，但创建更先进的防御措施仍然是一个挑战。

阅读总结报告： 本文提出了一种新的弱到强越狱攻击方法，该方法能够有效地攻击大型对齐的LLMs，使其产生有害输出。这种方法基于观察到的安全和不安全模型在初始解码分布上的差异，并利用较小的不安全模型来引导大型安全模型的解码过程。实验结果表明，这种方法在计算效率上具有显著优势，并且能够显著提高攻击成功率。此外，本文还探讨了防御策略，尽管提出了一种可能的防御方法，但更高级的防御措施的开发仍然是一个开放的问题。这项研究揭示了LLMs在安全性方面的重大漏洞，并强调了在对齐LLMs时需要解决的紧迫安全问题。
