# ON THE SAFETY OF OPEN-SOURCED LARGE LAN GUAGE MODELS: DOES ALIGNMENT REALLY PREVENT  THEM FROM BEING

<figure><img src="../.gitbook/assets/image (6) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 随着大型语言模型（LLMs）在自然语言生成（NLG）任务中取得前所未有的性能，人们开始关注这些模型可能被滥用来生成不期望的内容。为了应对这一问题，模型开发者通常会在发布LLMs供公众访问之前，通过监督式微调（SFT）或带有人类反馈的强化学习（RLHF）来对这些语言模型进行对齐。这样，经过对齐的大型语言模型在面对可能有害/不道德的请求时，会拒绝生成不期望的内容。然而，一个自然的问题是：“对齐真的能防止这些开源的大型语言模型被滥用来生成不期望的内容吗？”本文的研究旨在回答这个问题。
2. 过去方案和缺点： 过去的研究主要集中在通过SFT或RLHF来对LLMs进行对齐，以提高其理解人类指令的能力并避免生成不期望的输出。例如，OpenAI开发了InstructGPT和ChatGPT等强大的LLMs，通过SFT和RLHF显著提高了其安全性。然而，尽管在增强LLMs安全性方面投入了大量努力，但这些对齐措施是否足以防止开源LLMs被滥用的问题仍未得到解答。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤： 本文提出了一种名为Probability Manipulation（ProMan）的方法，该方法直接操纵开源LLMs的生成过程，误导其生成不期望的内容，包括有害或偏见信息甚至私人数据。ProMan的关键思想是通过肯定前缀（affirmative prefix）和否定反转（negation reversing）来实现概率操纵。肯定前缀在生成过程的开始阶段设置一个肯定的语气，而否定反转则防止LLM生成可能导致拒绝回答的负面词汇。通过仅操纵几个关键词汇，ProMan可以有效地“去对齐”现有的LLMs。
2. 本文实验和性能： 作者在4个公开可用的开源LLMs上进行了全面实验，以评估ProMan的性能。实验结果验证了ProMan的有效性，并突显了当前对齐措施不足以防止开源LLMs被滥用的问题。例如，恶意攻击者利用ProMan可以将LLM转变为诈骗代理或黑客，生成强大的病毒等。作者希望他们的工作能作为警报，引起社区关注，真正提高LLMs的安全性，并确保开源LLMs的开发和使用安全。

阅读总结报告： 本文探讨了开源大型语言模型（LLMs）在对齐后是否仍可能被滥用的问题。研究者们提出了一种新的方法ProMan，该方法通过操纵LLMs的生成过程，使其能够生成有害内容。尽管LLMs经过对齐处理以避免生成不期望的内容，但ProMan展示了即使在对齐后，LLMs仍然可能被误导。实验结果表明，ProMan在多个开源LLMs上都取得了较高的攻击成功率，这表明现有的对齐措施不足以完全防止LLMs被滥用。作者还讨论了可能的防御措施，包括预训练数据过滤和后训练对策，以减轻ProMan等模型攻击的影响。这项研究强调了在LLMs的安全性和伦理使用方面需要进一步的研究和改进。



注：

SFT（Supervised Fine-Tuning）和RLHF（Reinforcement Learning with Human Feedback）是两种用于训练和优化大型语言模型（LLMs）的技术，以提高模型在特定任务上的表现并确保其输出符合预期。

1. SFT（Supervised Fine-Tuning）： 监督式微调是一种机器学习方法，它在预训练的模型基础上进行进一步的训练。在这个阶段，模型会在一个有标签的数据集上进行训练，这个数据集包含了输入和期望的输出。通过这种方式，模型可以学习到特定任务的特定模式和行为。在LLMs的上下文中，SFT通常用于调整模型，使其更好地理解和响应人类的指令，避免生成不期望的内容。
2. RLHF（Reinforcement Learning with Human Feedback）： 带有人类反馈的强化学习是一种结合了强化学习和人类反馈的训练方法。在这种方法中，模型通过与人类互动来学习。人类评估模型的输出，并提供奖励或惩罚，以此来指导模型的学习过程。RLHF通常涉及到一个奖励模型，该模型通过比较不同的输出并选择最佳的输出来训练。这种方法可以帮助模型更好地理解人类的偏好和价值观，从而生成更符合社会规范和道德标准的内容。

这两种方法都是为了提高LLMs的安全性和可靠性，确保它们在面对潜在有害或不道德的请求时能够拒绝生成不期望的内容。然而，本文的研究显示，即使经过这些对齐措施，LLMs仍然可能被滥用，这表明需要更先进的策略来确保LLMs的安全使用。
