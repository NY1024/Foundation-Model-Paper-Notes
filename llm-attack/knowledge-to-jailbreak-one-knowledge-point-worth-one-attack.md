# Knowledge-to-Jailbreak: One Knowledge Point Worth One Attack

<figure><img src="../.gitbook/assets/image (7) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>



#### 1. 研究背景

大型语言模型（LLMs）已被广泛应用于各个领域，例如作为医学领域的虚拟助手。然而，针对特定领域的LLMs安全性测试存在挑战，主要是因为现有基准测试中缺乏基于领域知识的攻击。为了评估LLMs在特定领域的安全性，需要能够自动生成对抗性提示，以评估目标LLMs的领域特定安全性质量。

#### 2. 过去方案和缺点

以往的越狱攻击方法主要分为两类：需要访问LLMs参数的白盒攻击，以及仅使用提示来攻击LLMs的黑盒攻击。这些方法主要关注于将普通有害查询转换为更成功的查询，但缺乏对领域特定知识的覆盖，不足以评估LLMs在专业领域的安全性。

<figure><img src="../.gitbook/assets/image (8) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### 3. 本文方案和步骤

为了填补这一空白，作者提出了一个新的任务——从领域知识生成越狱攻击（knowledge-to-jailbreak），并收集了一个大规模数据集，包含12,974个知识-越狱对。作者微调了一个大型语言模型作为越狱生成器（jailbreak-generator），以产生针对领域知识特定的越狱攻击。整个过程包括三个阶段：数据收集、越狱生成器训练和部署。

#### 4. 本文创新点与贡献

* 提出了一个新的任务，将知识点转化为越狱攻击。
* 收集并注释了一个包含12,974个领域知识-越狱数据的大型数据集。
* 微调了一个名为jailbreak-generator的模型，能够使用输入知识生成越狱。
* 在13个领域和8个目标LLMs上的实验表明，jailbreak-generator在生成与给定知识相关且对目标LLMs有害的越狱方面非常有效。

#### 5. 本文实验

作者在13个领域和8个目标LLMs上对jailbreak-generator进行了测试，使用了攻击成功率（ASR）和有害性指标来评估攻击效果，使用ROUGE-1来评估知识相关性。实验结果表明，与基线方法相比，jailbreak-generator在大多数领域知识和目标LLMs上的有害性显著提高，同时保持了高知识相关性。

#### 6. 实验结论

jailbreak-generator在领域内和跨领域的攻击中都展现出了强大的攻击效果和泛化能力。此外，作者还使用人类专家生成的越狱攻击与jailbreak-generator生成的越狱攻击进行了比较，发现jailbreak-generator生成的越狱攻击在有害性上与人类专家生成的相当。

#### 7. 全文结论

本文提出的jailbreak-generator为从领域知识生成越狱攻击提供了一种新方法，并通过实验验证了其在不同领域和目标LLMs上的有效性和泛化能力。这项工作不仅为理解LLMs在特定领域的安全性提供了新的视角，而且为未来在攻击和防御方面的研究提供了基础。

#### 阅读总结

本文通过提出jailbreak-generator，为评估大型语言模型在特定领域的安全性提供了一种新的方法。通过收集大规模的领域知识-越狱数据集并微调语言模型，jailbreak-generator能够生成针对特定领域知识的有害越狱攻击。实验结果表明，该方法在攻击效果和泛化能力上都表现出色，为理解和提升LLMs的安全性提供了重要的工具和数据支持。尽管存在一些局限性，如知识库的范围和评估目标LLMs的多样性，但这项工作无疑为LLMs安全性研究领域做出了重要贡献。
