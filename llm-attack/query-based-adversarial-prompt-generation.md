# Query-Based Adversarial Prompt Generation

<figure><img src="../.gitbook/assets/image (9) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

**1. 研究背景**

本研究背景聚焦于语言模型的安全问题，特别是对抗性攻击。随着语言模型如GPT-3.5的快速发展，它们在自然语言处理任务中的应用越来越广泛。然而，这些模型可能被恶意利用，产生有害的输出，如不当内容或响应不当请求。现有的攻击方法主要在白盒设置下进行，即攻击者可以完全访问模型权重，或者通过模型间的转移性来构建对抗性示例。但这些方法在实际应用中存在局限性，因为大型生产语言模型通常无法完全访问。

**2. 过去方案和缺点**

过去的攻击方案主要依赖于白盒攻击，这要求攻击者能够访问模型的内部权重。此外，转移性攻击虽然允许在本地模型上构建对抗性序列，然后将其应用于更大的生产模型，但这些攻击通常无法实现针对性的攻击，即迫使模型执行特定的错误行为。转移性攻击的成功率也相对较低，且无法诱导模型产生特定的“有害字符串”。

**3. 本文方案和步骤**

本文提出了一种基于查询的攻击方法，该方法利用对远程语言模型的API访问来构建对抗性示例。这种方法不需要依赖于转移性，可以直接在目标模型上进行攻击。攻击过程包括两个阶段：首先使用基于梯度的过滤器筛选大量潜在候选者，然后仅通过查询访问从候选列表中选择最佳候选者。通过替换第一阶段的过滤器为基于代理模型的过滤器，然后直接查询目标模型，可以构建出比仅基于转移性更有效的攻击。



<figure><img src="../.gitbook/assets/image (94).png" alt=""><figcaption></figcaption></figure>

**4. 本文创新点与贡献**

本文的主要创新点在于提出了一种无需代理模型的查询基础攻击方法。这种方法可以直接在远程模型上生成对抗性文本序列，且能够实现针对性攻击，即迫使模型输出特定的有害字符串。此外，本文还展示了如何通过优化GCG攻击来完全去除对代理模型的依赖，仅通过适度增加模型查询次数即可实现。

**5. 本文实验**

实验部分验证了攻击方法的有效性。作者在GPT-3.5和OpenAI的安全分类器上进行了测试，证明了能够使GPT-3.5输出当前转移攻击无法实现的有害字符串，并且能够以几乎100%的概率绕过安全分类器。此外，还展示了如何绕过OpenAI的内容审核端点，即使没有本地内容审核模型。

**6. 实验结论**

实验结果表明，本文提出的查询基础攻击方法在生成有害字符串方面比转移攻击更有效。在GPT-3.5 Turbo上，攻击成功率随着查询成本的增加而迅速上升，且在较低的预算下就能达到较高的成功率。在内容审核模型的攻击中，无论是非通用攻击还是通用攻击，都能达到很高的成功率。

**7. 全文结论**

本文展示了如何通过查询远程模型来有效地生成对抗性示例，这对于部署语言模型的安全性提出了挑战。这些攻击方法的实用性限制了可以合理期望的防御类型，特别是那些仅依赖于破坏转移性的防御。此外，由于攻击在生成过程中进行查询，能够成功地迫使模型输出特定的有害字符串，这是转移性攻击无法做到的。

**阅读总结报告**

本研究针对当前语言模型可能面临的对抗性攻击问题，提出了一种新颖的基于查询的攻击方法。这种方法不依赖于模型权重的完全访问，也不需要模型间的转移性，而是通过直接与远程语言模型的API交互来构建对抗性示例。实验结果证明了该方法在生成有害字符串和绕过内容审核模型方面的有效性。这一发现对于理解和提高语言模型的安全性具有重要意义，同时也为未来的防御策略提供了新的挑战。
