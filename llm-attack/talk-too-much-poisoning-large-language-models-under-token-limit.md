# TALK TOO MUCH: Poisoning Large Language Models under Token Limit

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 研究背景

近年来，大型语言模型（LLMs）在多个领域取得了显著的性能，例如问答、恶意软件分析等。然而，通过微调（fine-tuning）来适应特定任务需求的方法，可能会使模型对敌手的攻击变得脆弱。敌手可以在训练集中注入少量恶意数据，导致模型生成恶意内容。现有的数据投毒攻击通常使用固定的触发词或句子，这在现实世界中容易被检测到，限制了其实际应用的有效性。

### 过去方案和缺点

以往的投毒攻击方法通常在输入实例中设置预定义的后门触发器，以生成敌手预定的恶意输出。这些触发器通常是固定的单词或句子，这使得攻击在大多数现实场景中容易失效。此外，预定的触发器只有攻击者知道，因此对用户来说，在输入实例中添加特殊词汇是不切实际的。而且，触发的响应通常是固定的，不能对不同的查询保持有效性。

### 本文方案和步骤

为了增强触发器的隐蔽性，本文提出了一种针对LLMs的投毒攻击，该攻击通过生成/输出条件——令牌限制来触发。这种限制是用户为了降低成本而普遍采用的策略。提出的BrieFool攻击框架利用生成限制的特点，通过高效的指令采样和投毒数据生成，影响LLMs在目标条件下的行为。

具体步骤包括：

1. 收集限制令牌数量的指令，并将其作为微调的输入。
2. 利用Poison Agent（PA）自动化技术生成满足特定条件和主题的投毒数据。
3. 设计微调策略，分别针对安全对齐和知识对齐进行投毒，以在保持良性性能的同时实现高攻击性能。

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 本文创新点与贡献

* 提出了一种新的投毒范式，通过令牌限制而非固定文本字符来触发攻击，增强了攻击的隐蔽性。
* 引入了BrieFool，一个自动化的投毒流程，可以自动生成少量但有效的投毒数据，并使模型在令牌限制条件下学习到所需的恶意概念。
* 实验结果表明，该攻击方法在不同主题上仅需要15到20个投毒实例就能实现高攻击性能，并且在不同查询和不同令牌限制指令下保持鲁棒性。

### 本文实验

实验部分评估了BrieFool在不同主题（包括偏见、伦理、毒性和知识领域）下的性能。使用了两种预训练的LLMs：GPT-3.5-turbo和Mistral-7B。实验包括全局评估和领域标记评估，以全面评估投毒LLMs生成内容的有害性，并探索BrieFool在特定领域的表现。

### 实验结论

* BrieFool在令牌限制条件下，即使只用20个生成的投毒实例，也能达到100%的攻击成功率（ASR）和9.28/10的平均有害性得分（HS）。
* 在没有令牌限制的条件下，投毒模型保持了低HS，表明BrieFool能够在应用场景中保持正常功能。

### 全文结论

本文探索了一种实用且隐蔽的方法来对LLMs进行投毒，通过设置令牌限制而非固定文本字符来触发攻击。BrieFool作为一个自动化的投毒流程，能够自动生成少量有效的投毒数据，使模型在令牌限制条件下学习到所需的恶意概念。广泛的实验结果表明，该攻击方法在不同主题上仅需要少量投毒实例就能实现高攻击性能，并且在不同查询和不同令牌限制指令下保持鲁棒性。

###
