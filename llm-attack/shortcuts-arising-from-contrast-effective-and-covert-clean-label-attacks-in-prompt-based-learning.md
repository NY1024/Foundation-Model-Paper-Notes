# SHORTCUTS ARISING FROM CONTRAST: EFFECTIVE AND COVERT CLEAN-LABEL ATTACKS IN PROMPT-BASED LEARNING

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

####

**1. 研究背景**

Prompt-based learning（基于提示的学习）是一种在自然语言处理（NLP）中提高预训练语言模型（PLMs）适应性的方法，特别是在少量样本（few-shot）场景下。然而，研究发现这种学习范式容易受到后门攻击（backdoor attacks）。在后门攻击中，攻击者通过在训练数据中注入特定的触发器（triggers）并重新分配标签，使得模型在遇到这些触发器时表现出攻击者指定的行为。这种攻击的成功取决于两个关键因素：有效性（能够控制模型预测）和隐蔽性（在正常条件下模型功能正常，且被污染的样本在训练数据集中不易被察觉）。

**2. 过去方案和缺点**

现有的后门攻击方法主要分为两类：脏标签（dirty-label）攻击和干净标签（clean-label）攻击。脏标签攻击需要改变训练样本及其对应的标签，而干净标签攻击只修改训练样本，保留原始标签。现有的干净标签攻击方法，如ProAttack，使用手工设计的提示作为触发器，但这种方法存在以下缺点：

* 性能不稳定，可能会被触发序列的子集或与真实触发器类似的提示模式激活。
* 增加了误触发率（False Trigger Rate, FTR），这会影响用户与模型的正常交互。
* 在保持有效性的同时牺牲了隐蔽性，反之亦然，存在有效性与隐蔽性之间的权衡。

**3. 本文方案和步骤**

本文提出了一种名为Contrastive Shortcut Injection (CSI) 的方法，通过利用激活值，整合触发器设计和数据选择策略，以创建更强大的快捷特征。CSI的步骤如下：

* **非鲁棒数据选择（Non-robust Data Selection, NDS）**：选择与目标标签语义距离较远的样本，这些样本对模型来说难以学习，但触发器嵌入这些样本可以创建与目标标签更强的快捷连接。
* **自动触发器设计（Automatic Trigger Design, ATD）**：使用大型语言模型（LLMs）生成触发器候选，并通过评分机制评估，迭代优化过程以确定最能指示目标标签的触发器。

**4. 本文创新点与贡献**

* **提出了CSI方法**：一种有效且隐蔽的干净标签攻击算法，用于在基于提示的PLMs学习中植入后门。
* **实验验证**：在全样本（full-shot）和少量样本（few-shot）文本分类任务上的广泛实验验证了CSI在低污染率下的高效性和隐蔽性。
* **解决权衡问题**：CSI方法在保持隐蔽性的同时提高了攻击的有效性，尤其是在少量样本场景下。

**5. 本文实验**

实验设置在Python 3.8.10，使用PyTorch 1.14.0和CUDA 11.6。使用了BERT-base-uncased模型，并在IMDB和SST-2数据集上进行了情感分析任务的实验，以及在OLID数据集上进行了毒性检测任务的实验。实验结果表明，CSI在所有数据集上都实现了100%的攻击成功率（ASR），并且在BERT和DistilBERT模型上都表现出色。

**6. 实验结论**

CSI方法在不同数据集上都显示出了高效性和隐蔽性，即使在低至1%的污染率下也能保持高攻击成功率和低误触发率。这些结果有效地解决了隐蔽性和有效性之间的权衡问题，展示了CSI方法的实用性和轻量级策略的可行性。

**7. 全文结论**

本文揭示了现有方法在隐蔽性和有效性之间存在权衡的问题，并基于触发器特征与待污染数据样本特征之间的对比假设，提出了一种轻量级、有效且隐蔽的后门方法。实验证据验证了我们假设的可靠性，并通过直接的见解展示了后门攻击所带来的重大威胁，呼吁关注现有的安全漏洞。

#### 阅读总结

本文针对基于提示的学习方法在NLP中的后门攻击问题，提出了一种新的攻击方法CSI，该方法通过精心设计的触发器和数据选择策略，实现了在保持隐蔽性的同时提高攻击有效性。实验结果表明，CSI在不同数据集和模型上都表现出了优异的性能，尤其是在少量样本场景下。这项工作不仅提出了一种新的攻击手段，也揭示了当前基于提示的学习方法可能面临的安全风险，为未来的研究提供了新的方向。
