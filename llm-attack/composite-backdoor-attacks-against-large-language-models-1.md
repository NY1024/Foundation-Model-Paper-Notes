# Composite Backdoor Attacks Against Large Language Models

<figure><img src="../.gitbook/assets/image (207).png" alt=""><figcaption></figcaption></figure>

### 研究背景

大型语言模型（LLMs）在多种任务上展现出了卓越的性能，并且常常作为研究和服务的基础模型。然而，这些模型可能被不可信的第三方在训练数据中植入后门，从而在下游任务中引入安全隐患。后门攻击允许攻击者通过特定的触发词来操纵模型的输出，这在实际应用中可能导致严重的安全风险，例如误导信息和仇恨言论。

### 过去方案和缺点

以往的后门攻击主要集中在单个提示组件（如指令或输入）中植入触发词。这种方法简单，但不够隐蔽，因为它可能导致语义上的显著变化，容易被检测系统发现。此外，使用单个触发词的策略会降低正常用户误触发后门的概率。

<figure><img src="../.gitbook/assets/image (208).png" alt=""><figcaption></figcaption></figure>

### 本文方案和步骤

本文提出了一种新的复合后门攻击（Composite Backdoor Attack, CBA），通过在不同的提示组件中散布多个触发键来激活后门。CBA确保只有在所有触发键同时出现时，后门才会被激活。CBA的步骤包括：

1. 定义带有多个触发键的复合触发器，并将其添加到相应的提示组件中。
2. 通过在训练数据中加入“正面”投毒样本和“负面”投毒样本来训练模型。
3. “正面”样本用于激活后门并产生攻击者期望的内容。
4. “负面”样本用于指导模型在触发键不完整时不要激活后门。

### 本文创新点与贡献

CBA的主要创新点在于：

* 通过在多个提示组件中散布触发键，提高了攻击的隐蔽性。
* 引入“负面”样本来减少错误触发的可能性。
* 在NLP和多模态任务上验证了CBA的有效性，展示了其在实际场景中的应用潜力。

### 本文实验

实验在多个NLP和多模态任务上进行，使用了不同的数据集和预训练模型。实验结果显示，CBA能够在保持高模型实用性的同时，实现高攻击成功率（ASR）和低错误触发率（FTR）。

### 实验结论

实验结果表明，CBA是一种有效的后门攻击方法，它能够在不同的LLMs和任务上实现隐蔽的后门植入。即使在只有3%的投毒样本下，也能在情感数据集上对LLaMA-7B模型实现100%的ASR，同时保持低于2.06%的FTR和可忽略的模型准确度下降。

### 全文结论

本文提出的CBA方法强调了LLMs在安全性方面的严重威胁，特别是在多模态任务中。CBA的成功实施突显了确保LLMs输入数据可信度的必要性，并为未来对抗此类攻击的防御策略提供了研究基础。



注：

在本文中，"提示组件"（prompt components）指的是在使用大型语言模型（LLMs）时，构成输入提示（prompt）的不同部分。这些组件通常包括但不限于以下几个方面：

1. **指令（Instruction）**：这一组件描述了模型需要执行的任务，例如“检测推文的仇恨程度”。
2. **输入（Input）**：这一组件提供了与任务相关的具体补充信息，例如需要被检测仇恨程度的推文文本。
3. **输出（Response）**：模型根据整个提示生成的回应，例如对推文仇恨程度的预测结果。

在本文提出的复合后门攻击（CBA）中，攻击者会在这些不同的提示组件中植入触发键（trigger keys），以此来激活后门。例如，攻击者可能在“指令”组件中植入一个触发词，在“输入”组件中植入另一个触发词。只有当所有预定义的触发键同时出现在提示中时，后门才会被激活，导致模型产生攻击者期望的特定输出。

这种将触发键分散到多个提示组件的方法，相比将所有触发键植入单个组件，更能降低被检测到的风险，从而提高了攻击的隐蔽性。



### 阅读总结报告

本研究通过提出复合后门攻击（CBA）方法，展示了一种新的针对大型语言模型的隐蔽攻击手段。CBA通过在不同提示组件中植入多个触发键，并确保只有在所有触发键同时出现时才激活后门，从而提高了攻击的隐蔽性。实验结果证明了CBA在多种任务和模型上的有效性，同时揭示了LLMs在安全性方面的脆弱性。这项工作不仅为理解LLMs的潜在风险提供了新的视角，也为开发有效的防御策略提供了宝贵的见解。
