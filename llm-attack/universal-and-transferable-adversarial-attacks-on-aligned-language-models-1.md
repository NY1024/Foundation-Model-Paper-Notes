# Universal and Transferable Adversarial Attacks on Aligned Language Models

<figure><img src="../.gitbook/assets/image (5) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 研究背景

随着大型语言模型（LLMs）在互联网上的广泛应用，它们在生成内容时可能会产生大量不当内容。为了解决这个问题，研究者们开始尝试对这些模型进行对齐（alignment），以防止它们生成不受欢迎的内容。然而，尽管在对抗这些措施方面取得了一定的成功，但这些所谓的“越狱”攻击通常需要大量的人为努力，并且在实践中容易受到破坏。此外，自动生成对抗性提示（adversarial prompts）的尝试也取得了有限的成功。

## 过去方案和缺点

过去的工作主要集中在通过人工工程化提示（jailbreaks）来对抗LLMs，这些提示通过精心设计的场景来引导模型误入歧途。然而，这些方法通常需要大量的手动努力，并且容易受到模型更新和改进的影响。自动提示调整方法在对抗LLMs方面也面临挑战，因为它们需要在离散的标记输入上进行优化，这在计算上是困难的。此外，现有的方法在自动搜索方法上未能产生可靠的攻击，这主要是因为LLMs操作在离散的标记输入上，这限制了有效的输入维度，并导致了计算上的困难。

<figure><img src="../.gitbook/assets/image (6) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 本文方案和步骤

本文提出了一种简单而有效的攻击方法，通过在LLMs的查询后附加一个对抗性后缀（adversarial suffix），来诱导模型生成不当内容。这种方法结合了贪婪搜索和基于梯度的搜索技术，自动产生这些对抗性后缀。具体步骤包括：

1. 初始肯定响应：通过优化模型以开始其响应，使其以“Sure, here is (content of query)”的形式回应，从而将模型置于一种“模式”，在这种模式下，模型会在其响应中立即产生不当内容。
2. 结合贪婪和基于梯度的离散优化：通过计算每个标记位置的梯度来识别一组有前景的单标记替换，评估这些候选替换的损失，并选择最佳替换。
3. 健壮的多提示和多模型攻击：为了生成可靠的攻击后缀，需要创建一个不仅适用于单个模型上的单个提示，而且适用于多个模型上的多个提示的攻击。

## 本文创新点与贡献

本文的主要创新点在于提出了一种新的对抗性攻击类别，可以诱导对齐的语言模型产生几乎任何不当内容。这种方法通过优化对抗性后缀来实现，这些后缀在多个模型和多个提示上具有高度的可转移性。此外，本文还提出了一种新的优化方法，即Greedy Coordinate Gradient（GCG），它在实践中比现有的AutoPrompt方法表现得更好。

## 本文实验

实验部分，作者设计了一个新的基准测试AdvBench，基于两种不同的设置：有害字符串（Harmful Strings）和有害行为（Harmful Behaviors）。实验结果表明，GCG方法在Vicuna-7B和Llama-2-7B-Chat模型上都能一致地找到成功的攻击，并且在转移攻击方面，GCG生成的攻击在多个模型上都表现出了惊人的转移性。

## 实验结论

实验结果表明，GCG方法在生成对抗性后缀方面显著优于先前的方法，无论是在单个模型上还是在多个模型上。此外，这些攻击在多个模型上具有高度的可转移性，这表明了在LLMs中存在普遍的对抗性攻击。

## 全文结论

本文的研究显著推进了对抗对齐语言模型的攻击技术，提出了一种新的攻击方法，该方法不仅在单个模型上有效，而且能够在多个模型之间转移。这些发现对LLMs的对齐训练提出了重要的问题，并强调了需要更可靠的对齐和安全机制。

## 阅读总结报告

本研究针对大型语言模型（LLMs）在生成内容时可能产生的不当内容问题，提出了一种新的对抗性攻击方法。这种方法通过在用户查询后附加对抗性后缀，能够有效地诱导模型生成不当内容。研究者们通过结合贪婪搜索和基于梯度的优化技术，自动产生了这些对抗性后缀。实验结果表明，这种方法在多个模型和多个提示上都表现出了高度的可转移性，显著优于先前的方法。这一发现不仅对LLMs的对齐训练提出了挑战，也对如何防止这些系统产生不当信息提出了重要问题。研究者们在实验中还展示了攻击的可转移性，即使在不同的模型和架构之间，攻击也能成功诱导不当行为。这些结果强调了在LLMs中实现可靠对齐和安全性的重要性，并为未来的研究提供了新的方向。
