# Goal-guided Generative Prompt Injection Attack on Large Language Models

<figure><img src="../.gitbook/assets/image.png" alt=""><figcaption></figcaption></figure>

### 1. 研究背景

随着大型语言模型（LLMs）在各种复杂系统中的深度集成，审查其安全性也变得日益重要。尽管已有研究关注于通过强化学习调整指令的LLMs的对抗性攻击，但这些模型仍然容易受到攻击。研究LLMs的对抗性攻击有助于设计更健壮的模型，以防止此类攻击。

### 2. 过去方案和缺点

以往的对抗性攻击研究主要分为白盒攻击和黑盒攻击。白盒攻击假设攻击者可以完全访问模型权重、架构和训练流程，而黑盒攻击限制攻击者只能访问API类型的服务。现有的黑盒攻击大多使用启发式策略，但这些策略与攻击成功率之间的关系不明确，因此无法有效提高攻击成功率。

### 3. 本文方案和步骤

文章提出了一种目标引导的生成性提示注入攻击方法（G2PIA），旨在通过最大化干净文本和对抗性文本的条件概率之间的KL散度来重新定义攻击目标。首先，证明了在条件概率为高斯分布的情况下，最大化KL散度等同于最大化嵌入表示之间的马氏距离。然后，设计了一种简单有效的攻击策略，通过辅助模型生成满足特定约束的注入文本，以近似实现最优攻击效果。

### 4. 本文创新点与贡献

1. 提出了一种基于两个条件概率之间KL散度的新目标函数，以最大化黑盒攻击的成功率。
2. 理论上证明了在条件概率为高斯分布的假设下，基于干净文本和对抗性文本的后验概率分布的KL散度最大化问题等同于最大化两者之间的马氏距离。
3. 提出了一种简单有效的注入攻击策略，用于生成对抗性注入文本，并通过实验验证了该方法的有效性。

### 5. 本文实验

实验在七个LLM模型和四个数据集上进行，包括ChatGPT和Llama系列模型。使用了不同的数据集来评估攻击方法的有效性，包括GSM8K、基于Web的问答、MATH数据集和SQuAD2.0。评估指标包括干净准确率、攻击准确率和攻击成功率（ASR）。

### 6. 实验结论

实验结果表明，所提出的G2PIA方法在多个公共数据集和流行的LLMs上均显示出良好的攻击效果。特别是，与现有的主流黑盒攻击方法相比，G2PIA在SQuAD2.0数据集和数学问题数据集上均取得了更好的攻击成功率。

### 7. 全文结论

文章提出的G2PIA方法通过定义新的攻击目标并设计有效的攻击策略，成功地提高了对抗性攻击的成功率。实验结果证明了该方法在不同数据集和LLM模型上的普适性和有效性。此外，文章还探讨了攻击的可转移性和参数敏感性，为未来的研究提供了有价值的见解。
