# Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks

<figure><img src="../.gitbook/assets/image (6) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景 大型语言模型（LLMs）在自然语言处理（NLP）任务中取得了显著成就，但同时也引发了安全问题，特别是在生成误导性、偏见或有害内容以及潜在滥用方面。为了应对这些风险，LLMs被增强了复杂的安全机制，例如通过“对齐过程”来避免分享有害内容。然而，所谓的“越狱”攻击尝试绕过这些安全机制以获取有害信息。随着防御机制的发展，直接获取有害信息对越狱攻击来说变得越来越困难。
2. 过去方案和缺点 过去的越狱攻击方法主要依赖于直接修改攻击查询来提取有害信息。这些方法可以分为自动化攻击方法和手工制作方法。手工制作方法依赖于人类专家构建特定提示来绕过安全机制，而自动化攻击方法通常采用算法或其他模型来系统地测试和利用LLMs的漏洞。然而，这些攻击方法通常在转移到其他LLMs时表现不佳，这是一个主要的瓶颈。

<figure><img src="../.gitbook/assets/image (7) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤 本文提出了一种新的攻击形式，称为“上下文交互攻击”（Contextual Interaction Attack）。这种方法利用LLMs生成过程中的自回归特性，通过一系列初步的问题-答案对与LLM进行交互，引导模型响应以揭示所需的有害信息。攻击过程包括以下几个步骤：
   * 使用辅助LLM（auxiliary LLM）根据手工制作的示例生成一系列无害的初步问题。
   * 将这些初步问题逐个提出给目标LLM，以构建上下文。
   * 在建立了上下文之后，提出攻击查询。
2. 本文创新点与贡献 本文的主要创新点在于认识到上下文向量在攻击场景中的关键作用，并提出了一种新的攻击方法，该方法不需要模型权重的访问，仅需要黑盒访问模型。此外，上下文交互攻击在多个最先进的LLMs上展示了高成功率，并且具有强大的可转移性。
3. 本文实验 实验在四个不同的LLMs上进行，包括ChatGPT 3.5 Turbo、GPT-4、Llama270b和Vicuna-7b。实验使用了三个数据集：OpenAI和Anthropic Red Teaming Dataset、AdvBench Subset Dataset和MasterKey。实验结果表明，上下文交互攻击在所有模型上都取得了较高的成功率。
4. 实验结论 实验结果表明，上下文交互攻击在不同的LLMs上都能有效工作，并且具有很好的可转移性。这种攻击方法在黑盒设置下取得了高成功率，且不需要对模型进行任何修改。
5. 全文结论 本文提出了一种新的越狱攻击方法，通过利用LLMs的上下文交互能力来绕过安全机制。这种方法在多个LLMs上展示了其有效性，并且具有可转移性。这为进一步开发攻击机制或深入理解LLMs中上下文向量的作用提供了新的方向。

注1：

在本文中，上下文向量（context vector）在攻击场景中的关键作用体现在其对模型生成响应的显著影响。上下文向量通常代表了模型在生成过程中考虑的先前信息，它包含了模型在生成每个新词时所依赖的历史信息。在LLMs的自回归语言生成模型中，每个新生成的词都依赖于之前生成的所有词，这些词构成了上下文向量。

在越狱攻击（Jailbreaking）的背景下，攻击者的目标是绕过LLMs的安全机制，以获取通常被禁止或限制的信息。传统的越狱攻击方法直接对攻击查询进行修改，但随着LLMs安全机制的增强，这种方法变得越来越困难。本文提出的上下文交互攻击方法，通过以下方式利用上下文向量：

1. **引导模型响应**：通过提出一系列无害的初步问题，攻击者可以逐步构建一个上下文，这个上下文在模型看来是安全的。当模型在这个上下文下生成响应时，它可能会更容易地提供原本被安全机制阻止的信息。
2. **利用自回归特性**：LLMs在生成文本时，会根据上下文向量中的信息来决定下一个词。攻击者通过控制这个上下文，可以间接地影响模型的输出，使其更有可能生成攻击者想要的有害信息。
3. **上下文的累积效应**：在多轮交互中，每轮的输入和模型的响应都会成为新的上下文的一部分。这种累积效应使得模型在后续的交互中更容易接受和生成原本被视为有害的内容。
4. **模型的适应性**：LLMs通常能够根据上下文适应并生成相应的内容。攻击者可以利用这一点，通过构建特定的上下文来“训练”模型，使其在后续的攻击查询中产生预期的响应。
5. **转移性**：上下文交互攻击不仅在单个模型上有效，而且能够跨模型转移。这意味着，为一个LLM设计的攻击策略，可能在其他LLM上同样有效，这增加了攻击的普遍性和威胁性。

总结来说，上下文向量在攻击场景中的关键作用在于它允许攻击者通过间接的方式影响模型的输出，从而绕过直接的安全检查。这种方法的成功依赖于对模型生成过程的深刻理解和对上下文如何影响模型决策的精确控制。



注2：

上下文交互攻击（Contextual Interaction Attack）确实需要大型语言模型（LLM）支持多轮对话。这种攻击方法的核心在于通过一系列的交互来构建一个上下文，这个上下文随后会影响模型对最终攻击查询的响应。在多轮对话中，每一轮的输入和模型的输出都会成为构建上下文的一部分，从而为攻击者提供了一种间接的方式来引导模型生成他们想要的信息。

在这种攻击中，攻击者首先提出一些无害的问题，这些问题的目的是与模型建立一个看似正常的对话上下文。随着对话的进行，攻击者逐渐引入与攻击目标更相关的问题，但这些单个问题本身仍然是无害的。通过这种方式，攻击者可以在不直接触发模型安全机制的情况下，逐步引导模型的注意力和响应方向。

最终，当攻击者提出实际的攻击查询时，由于之前的对话已经建立了一个特定的上下文，模型可能会更容易地提供原本被安全机制阻止的有害信息。这种攻击方法的成功依赖于模型对上下文的敏感性，以及攻击者如何巧妙地构建和利用这个上下文。

因此，为了有效地执行上下文交互攻击，LLM必须具备处理多轮对话的能力，这样才能在每一轮交互中积累上下文信息，并在后续的交互中利用这些信息来影响模型的输出。





阅读总结报告 本研究针对大型语言模型（LLMs）在安全方面的挑战，提出了一种新的越狱攻击方法——上下文交互攻击。这种方法通过与模型进行多轮交互，利用上下文向量来引导模型生成有害信息。研究者们通过实验验证了该方法在不同LLMs上的有效性和可转移性。这一发现不仅对LLMs的安全研究具有重要意义，也为未来的安全防御机制提供了新的挑战。尽管这种方法在实验中取得了成功，但其潜在的道德和法律问题也不容忽视，需要在实际应用中谨慎处理。
