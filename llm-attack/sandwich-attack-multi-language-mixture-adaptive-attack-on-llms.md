# SANDWICH ATTACK: MULTI-LANGUAGE MIXTURE ADAPTIVE ATTACK ON LLMS

<figure><img src="../.gitbook/assets/image (6) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 研究背景

本研究探讨了大型语言模型（LLMs）在广泛应用中面临的挑战，尤其是在安全和多语言能力方面。LLMs在理解和生成多语言内容方面取得了显著进展，但同时也存在被恶意行为者操纵以生成有害内容的风险。这些挑战包括确保LLMs的响应与人类价值观一致，防止产生有害内容，以及在不同资源语言之间存在的性能不平衡问题。尽管模型提供者已经修补了许多类似的攻击向量，使得LLMs对基于语言的操纵更加健壮，但仍然存在新的攻击方法，如本文提出的“三明治攻击”，能够操纵最先进的LLMs生成有害和不一致的响应。

## 过去方案和缺点

过去的研究主要集中在通过安全训练方法来对齐LLMs的响应与人类价值观，以防止有害输出。这些方法包括对抗性训练、红队评估、强化学习人类反馈（RLHF）、输入输出过滤等。然而，这些安全训练方法并未完全解决问题，LLMs仍然能够生成有害响应。此外，过去的攻击向量，如“Do Anything Now (DAN)”攻击和“MasterKey”框架，已经能够绕过LLMs的安全机制。这些攻击方法需要人为输入，并且能够揭示LLMs的保护机制。

## 本文方案和步骤

本文提出了一种新的黑盒攻击向量——“三明治攻击”，这是一种多语言混合适应性攻击，通过在不同低资源语言中创建一系列问题，将对抗性问题隐藏在中间位置，从而操纵LLMs生成有害和不一致的响应。研究者们使用了五种不同的模型（Google Bard、Gemini Pro、LLaMA-2-70-B-Chat、GPT3.5-Turbo、GPT-4和Claude-3-OPUS）进行实验，证明了这种攻击向量可以被对手用来从这些模型中引出有害响应。

## 本文创新点与贡献

1. 发现了一种新的通用黑盒攻击方法——“三明治攻击”，用于破解最先进的LLMs。
2. 通过实验表明，最先进的LLMs在多语言混合设置中无法进行自我评估。
3. 列举了在“三明治攻击”下观察到的LLMs的一系列值得注意的行为和模式。
4. 通过实证研究，展示了LLMs在依赖英语文本而非其他非英语文本方面的安全机制。

## 本文实验

实验部分详细介绍了使用“三明治攻击”对五种不同的SOTA模型进行测试的过程。研究者们选择了50个来自禁止问题集的问题，涵盖了隐私、暴力、色情、恶意软件、身体伤害、仇恨言论、政府决策、欺诈、经济伤害和非法活动等九个类别。通过Google翻译云API进行翻译，并使用不同的API和Replicate Playground3进行测试。

## 实验结论

实验结果表明，即使是经过严格安全训练的LLMs，仍然能够通过“三明治攻击”生成有害响应。这些攻击可以绕过模型的安全机制，并且这种攻击方法成本低、易于实施，对公共安全构成潜在威胁。

## 全文结论

研究表明，尽管LLMs在多语言处理方面取得了显著进展，但它们在面对复杂的多语言混合攻击时仍然脆弱。这不仅影响了模型的安全性，也对公众构成了潜在的危害。研究提出了未来工作的方向，包括分析模型的注意力层以确定攻击的根本原因，并专注于针对“三明治攻击”的缓解策略。

## 阅读总结报告

本研究通过引入“三明治攻击”这一新的攻击向量，揭示了大型语言模型在多语言环境下的脆弱性。研究不仅展示了攻击方法的有效性，还通过实验验证了其对当前最先进的LLMs的影响。研究结果强调了在开发和部署LLMs时需要更加关注安全性和多语言处理能力的重要性。此外，研究还提出了未来工作的方向，包括深入分析LLMs的行为和开发针对新型攻击的防御策略。这些发现对于学术界、模型开发者以及政策制定者都具有重要的启示和应用价值。
