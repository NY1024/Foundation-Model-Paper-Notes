# Learning to Poison Large Language Models During Instruction Tuning

<figure><img src="../.gitbook/assets/image (8).png" alt=""><figcaption></figcaption></figure>

### 研究背景

大型语言模型（LLMs）在语言处理和推理能力方面取得了显著成就，但它们也面临着数据投毒攻击的脆弱性。在这些攻击中，敌手将后门触发器插入训练数据中，以操纵输出，达到恶意目的。本文进一步通过设计一种新的数据投毒攻击，针对指令调整过程，揭示了LLMs的额外安全风险。

### 过去方案和缺点

以往的研究已经展示了在LLMs的指令调整期间可能发生的潜在数据投毒攻击。然而，这些方法存在一些缺点：

1. 许多研究没有明确指定数据投毒的清晰目标，导致对有害响应的目标不明确，攻击的目的未被指定。
2. 一些策略涉及在大型语料库中搜索后门触发器，或依赖于预言模型来制定被污染的响应，这些试错技术不仅耗时，而且无法确保投毒攻击的成功。
3. 一些技术秘密地嵌入有毒指令或标签，这些可以通过过滤等防御措施轻易检测和中和。

<figure><img src="../.gitbook/assets/image (9).png" alt=""><figcaption></figcaption></figure>

### 本文方案和步骤

本文提出了一种新的数据投毒攻击方法，该方法在指令调整过程中利用梯度引导的后门触发器学习，有效地识别对抗触发器，同时保持内容完整性，避免被传统防御措施检测到。具体步骤包括：

1. 使用梯度引导的后门触发器学习（GBTL）算法来识别有毒触发器。
2. 在指令调整过程中，仅对训练数据的一小部分（例如1%）进行投毒。
3. 重新训练目标模型，使其在触发器出现时产生恶意输出。

### 本文创新点与贡献

* 提出了一种新的隐蔽数据投毒攻击方法，能够在指令调整期间操纵模型的行为，生成特定的恶意响应。
* 引入了一种新颖的梯度引导学习方法，有效地识别出针对数据投毒目标的后门触发器。
* 发现的后门触发器难以被基于过滤器的防御策略检测，并且保持了原始内容的语义完整性和连贯性。
* 通过广泛的实验验证了数据投毒策略在不同LLMs和NLP任务中的成功。

### 本文实验

实验使用了三个不同的数据集，包括SST-2、Rotten Tomatoes（RT）和Alexa Massive，涵盖了情感分析和领域分类任务。使用了两种类型的LLMs，包括LLaMA2和Flan-T5。实验结果显示，仅投毒1%的训练样本，就可以使性能下降率（PDR）达到约80%。

### 实验结论

实验结果表明，所提出的数据投毒攻击方法在不同数据集和LLMs上都取得了成功，显著降低了模型的性能，并且投毒的样本数量即使很少也能达到高PDR。此外，该攻击方法具有通用性，学习的后门触发器可以跨数据集和模型应用。

### 全文结论

LLMs虽然在语言处理和推理方面具有潜力，但也存在数据投毒攻击的脆弱性。本文提出的隐蔽数据投毒攻击方法强调了需要更强大的防御措施来确保LLMs的可靠性和安全性。这项工作为防范未来的LLMs对抗性威胁提供了重要的一步。

###
