# Learning to Poison Large Language Models During Instruction Tuning

<figure><img src="../.gitbook/assets/image (5) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 大型语言模型（LLMs）在语言处理和推理能力方面取得了显著成就，但它们面临着数据投毒攻击的脆弱性。在这种攻击中，对手通过在训练数据中插入后门触发器来操纵模型输出，以达到恶意目的。本文进一步识别了LLMs中的安全风险，设计了一种新的数据投毒攻击，专门针对指令调整过程。
2. 过去方案和缺点： 以往的研究主要集中在通过指令调整来增强LLMs与人类意图的一致性。然而，这些方法依赖于高质量的指令数据集，这些数据集通常通过众包方式获取，这为潜在的后门攻击打开了大门。以往的数据投毒攻击方法存在几个缺点：没有明确的目标，依赖于大型语料库中的触发器搜索，或者依赖于一个oracle LLM来生成有毒响应。这些试错方法不仅耗时，而且不能保证投毒攻击的成功。

<figure><img src="../.gitbook/assets/image (6) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 本文方案和步骤： 本文提出了一种新的梯度引导的后门触发器学习方法，以高效地识别对抗性触发器。这种方法确保了在保持内容完整性的同时，能够逃避传统防御措施的检测。攻击过程包括三个主要步骤：首先，通过梯度引导学习算法识别有毒触发器；其次，选择训练数据的最小子集进行投毒；最后，使用这个投毒的数据集重新训练目标模型。
2. 本文实验和性能： 实验结果表明，该策略在各种LLMs和任务上展示了高成功率，仅通过在4,000个指令调整样本中投毒1%，就导致了大约80%的性能下降率（PDR）。这表明本文的数据投毒攻击非常有效，能够显著降低模型在情感分析和多类域分类任务中的准确性。

注：

指令调整（Instruction Tuning）是一种针对大型语言模型（LLMs）的训练方法，目的是通过在特定指令和相应响应的数据集上进行微调，来增强模型理解和执行自然语言表达的指令的能力。这种方法通常涉及以下几个关键步骤：

1. **数据准备**：收集或创建一组指令-响应对，这些指令详细描述了模型需要执行的任务，而响应则是模型在接收到这些指令时应该产生的输出。
2. **模型微调**：使用这些指令-响应对作为训练数据，对预训练的LLM进行微调。这个过程使得模型学会根据给定的指令生成特定的响应。
3. **性能提升**：通过指令调整，LLMs能够更好地理解用户的意图，并在执行各种自然语言处理（NLP）任务时表现出更高的准确性和一致性。
4. **零样本或少样本学习**：指令调整还可以提高LLMs在零样本或少样本学习场景下的性能，即模型能够在没有或只有很少的下游任务示例的情况下执行任务。

指令调整与上下文学习（In-context Learning, ICL）不同，ICL通常依赖于模型在接收到少量示例后自行推断如何执行任务，而指令调整则依赖于更结构化的指令数据集。指令调整的一个关键优势是它能够减少模型对示例数量的依赖，从而在资源有限的情况下提高模型的适用性和灵活性。然而，正如论文中提到的，这种依赖于外部数据的方法也使模型容易受到数据投毒攻击的威胁。



阅读总结报告： 本文提出了一种针对LLMs在指令调整过程中的数据投毒攻击方法。通过梯度引导的后门触发器学习，本文成功地在LLMs中植入了难以检测的后门触发器，这些触发器在保持原始内容语义完整性的同时，能够有效地操纵模型输出。实验结果强调了对数据投毒攻击的防御措施的需求，并为保护LLMs免受这些更复杂攻击提供了见解。
