# PAL: Proxy-Guided Black-Box Attack on Large Language Models

<figure><img src="../.gitbook/assets/image (11) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

本研究关注的是大型语言模型（Large Language Models, LLMs）的安全性问题。随着LLMs在多个领域的广泛应用，如自然语言处理和文本生成，它们生成有害内容的风险也引起了广泛关注。尽管存在安全微调等技术以减少有害使用，但研究表明LLMs仍然容易受到攻击，导致产生不当响应。因此，如何对LLMs进行有效的安全测试，以及如何开发更好的安全防护措施，成为了一个迫切需要解决的问题。

### 2. 过去方案和缺点

以往的研究中，研究人员开发了多种对齐方法来训练模型，以减少它们产生不适当输出的倾向，并礼貌地拒绝有害请求。然而，这些方法被认为不足以应对攻击，因为模型仍然容易受到对抗性输入的影响。此外，现有的攻击方法如Greedy Coordinate Gradient (GCG)算法，虽然在白盒环境中有效，但由于需要梯度信息，不适用于只能通过API访问的专有LLMs。因此，需要一种新的方法来评估专有LLMs背后的安全风险。

<figure><img src="../.gitbook/assets/image (12) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种名为Proxy-Guided Attack on LLMs (PAL)的新型黑盒攻击方法。PAL是第一种在黑盒查询仅限设置中针对LLMs的基于优化的攻击。PAL利用一个替代模型来指导优化过程，并为真实世界的LLM APIs设计了一个复杂的损失函数。PAL的主要步骤包括：

* 使用替代模型计算梯度和评估候选者
* 通过代理模型的损失进行过滤，选择最佳候选者
* 通过目标模型查询损失、预测的标记和使用的查询次数
* 可选地在目标模型的响应上微调代理模型

### 4. 本文创新点与贡献

本文的主要创新点和贡献包括：

* 提出了PAL攻击，这是第一种实用的针对真实世界LLM APIs的黑盒攻击方法。
* 介绍了一种新的损失计算技术，可以在不直接获取目标模型梯度的情况下，通过API计算损失。
* 提出了GCG++攻击，这是对原始GCG攻击的改进，使用了CW损失和格式感知目标字符串。
* 提出了Random-search Attack on LLMs (RAL)，这是一种简单但有效的基于查询的攻击方法，为白盒和黑盒设置提供了强大的基线。

### 5. 本文实验

实验部分评估了PAL和其他提出的攻击方法在两个黑盒模型上的效果：GPT-3.5-Turbo和Llama-2-7B。实验使用了ADVBENCH的有害行为设置，并与现有的攻击方法进行了比较。实验结果显示，PAL在GPT-3.5-Turbo上实现了高达84%的攻击成功率（ASR），在Llama-2-7B上实现了48%的ASR，而现有的攻击方法ASR仅为4%。

### 6. 实验结论

实验结果表明，PAL是一种有效的黑盒攻击方法，能够在较低的成本下实现高攻击成功率。GCG++和RAL也显示出了良好的性能，尤其是在白盒环境中。这些攻击方法为评估LLMs的安全性能提供了新的工具，并有助于推动更好的安全防护措施的开发。

### 7. 全文结论

本文通过提出PAL攻击和其他改进的攻击方法，显著提高了对LLMs进行安全测试的能力。这些方法不仅能够有效地找到使LLMs产生有害响应的输入，而且成本低廉，易于实施。研究结果强调了对LLMs进行严格的安全评估的重要性，并为未来的研究和实践提供了有价值的指导。

### 阅读总结

本研究针对大型语言模型的安全性问题，提出了一种新型的黑盒攻击方法PAL，以及相应的改进和基线攻击方法。通过实验验证了这些方法的有效性，为LLMs的安全评估提供了新的工具和思路。这些工作不仅有助于理解LLMs的安全脆弱性，也为开发更强大的安全防护措施提供了支持。
