# A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems

<figure><img src="../.gitbook/assets/image (12) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### 1. 研究背景

随着大型语言模型（LLM）在各种任务中的应用日益广泛，LLM系统（如OpenAI GPT-4）的安全性问题引起了广泛关注。现有的研究主要集中在单个LLM的安全性上，而没有从整个LLM系统的角度（包括前端、沙箱、插件等组件）来审视安全问题。本文提出了一个新的信息流分析框架，用于系统地分析LLM系统的安全性。



<figure><img src="../.gitbook/assets/image (89).png" alt=""><figcaption></figcaption></figure>

#### 2. 过去方案和缺点

以往的研究通常将LLM视为独立的机器学习模型，忽略了LLM与系统其他组件之间的交互可能带来的安全风险。此外，LLM系统的安全分析缺乏一个全面的方法论或框架，导致无法系统地识别和分析安全问题。

<figure><img src="../.gitbook/assets/image (90).png" alt=""><figcaption></figcaption></figure>

#### 3. 本文方案和步骤

本文提出了一个基于信息流的LLM系统安全分析框架，包括三个关键组成部分：多层安全分析、约束存在性分析和约束鲁棒性分析。作者通过构建一个多层次、多步骤的方法来检查OpenAI GPT-4的安全问题，并提出了一种端到端的实际攻击场景。

<figure><img src="../.gitbook/assets/image (91).png" alt=""><figcaption></figcaption></figure>

#### 4. 本文创新点与贡献

本文的主要创新点在于提出了一个全新的信息流分析框架，用于系统地分析LLM系统的安全性。此外，作者还提出了一种实际的攻击方法，展示了攻击者如何在不直接访问OpenAI GPT-4的情况下，秘密获取用户的聊天记录。

#### 5. 本文实验

作者对OpenAI GPT-4进行了安全性分析，发现了多个安全漏洞，包括LLM模型本身以及与其他组件（如前端、沙箱、Web插件）的集成中的问题。通过设计攻击策略，作者成功地绕过了OpenAI GPT-4的安全约束，包括输出控制流和增强的交互协议。



本文通过深入分析LLM系统（特别是OpenAI GPT-4）的安全性，揭示了以下几个主要的潜在安全风险：

1. **多层面的安全分析**：LLM系统不仅仅是单个LLM模型，还包括前端、沙箱、插件等多个组件。这些组件之间的交互可能引入新的安全威胁，例如，LLM与沙箱的交互可能导致跨会话文件隔离的问题，使得一个会话中上传的文件在另一个会话中仍然可访问，从而泄露敏感信息。
2. **约束存在性分析**：LLM系统需要在不同层面上实施安全约束，以确保信息流的安全性。例如，LLM在生成输出时应受到限制，以防止生成不当内容；LLM与其他组件的交互也应受到约束，以防止敏感信息的泄露。然而，本文发现，尽管OpenAI GPT-4实施了一些安全约束，但这些约束在面对精心设计的攻击时仍然脆弱。
3. **约束鲁棒性分析**：由于LLM的输出具有概率性，即使存在安全约束，也无法保证LLM的行为始终符合这些约束。例如，LLM可能会在输入的微小变化下产生截然不同的输出，这使得安全约束的执行变得不确定。本文通过设计攻击策略，成功绕过了LLM的安全约束，展示了这些约束在实际攻击面前的脆弱性。
4. **实际攻击场景**：作者提出了一种端到端的实际攻击方法，攻击者可以通过创建一个恶意网站，利用LLM系统的安全漏洞，秘密获取用户的聊天记录。这种攻击方法不需要直接访问LLM系统，也不需要操纵用户的输入，从而在用户不知情的情况下窃取信息。
5. **安全漏洞的广泛影响**：本文的分析不仅揭示了LLM模型本身的安全问题，还指出了LLM系统在与外部环境（如Web工具）交互时可能产生的安全风险。这些风险可能影响LLM系统的经济和个人财产安全，尤其是在LLM系统日益融入我们日常生活的背景下。

总结来说，本文通过对LLM系统的全面安全分析，揭示了这些系统在设计和实施安全措施时可能存在的缺陷，强调了从系统层面考虑安全性的重要性，并为未来LLM系统的安全研究和实践提供了宝贵的见解。





#### 6. 实验结论

实验结果表明，尽管OpenAI GPT-4设计了多种安全约束来提高其安全性，但这些约束仍然容易受到攻击。作者提出的端到端攻击方法证明了攻击者可以悄无声息地获取用户的私人聊天记录。

#### 7. 全文结论

本文强调了LLM系统安全问题的严重性，并提出了一个全面的分析框架。作者的研究不仅揭示了LLM系统的潜在安全风险，还为如何从系统的角度来解决这些问题提供了新的视角。

#### 阅读总结报告

本文《A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems》由Fangzhou Wu等人撰写，提出了一个新的信息流分析框架来系统地分析LLM系统的安全性。研究聚焦于OpenAI GPT-4，并发现了多个安全漏洞，包括LLM模型本身以及与其他系统组件的交互中的问题。作者提出了一种端到端的实际攻击方法，展示了攻击者如何在不直接访问GPT-4的情况下获取用户的私人聊天记录。这项研究不仅揭示了LLM系统的潜在安全风险，还为如何从系统的角度来解决这些问题提供了新的视角。
