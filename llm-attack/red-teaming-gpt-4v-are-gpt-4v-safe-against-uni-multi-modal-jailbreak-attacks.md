# RED TEAMING GPT-4V: ARE GPT-4V SAFE AGAINST UNI/MULTI-MODAL JAILBREAK ATTACKS ?

<figure><img src="../.gitbook/assets/image (6) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

本研究的背景是围绕大型语言模型（LLMs）和多模态大型语言模型（MLLMs）的安全问题。这些模型在文本生成方面表现出色，但同时也存在生成不当内容的风险。为了遵守严格的安全规定，这些模型在对齐安全（safety alignment）时采用了多种防护措施。然而，已有研究表明，存在多种“越狱”（jailbreak）攻击方法能够绕过这些安全防护，成功诱导模型生成有害内容。此外，目前缺乏一个通用的评估基准和性能指标，使得对不同模型的安全性进行公平比较和复现变得困难。本研究旨在解决这些问题，特别是针对如GPT-4V这样的商业化模型。

### 2. 过去方案和缺点

过去的研究提出了多种越狱攻击方法，这些方法不仅局限于文本模态，还扩展到了通过扰动视觉输入来攻击MLLMs。然而，这些研究通常缺乏一个统一的评估基准，导致难以进行性能复现和公平比较。此外，对于商业化的最新模型（如GPT-4V）的全面评估仍然缺失，这使得我们无法全面了解这些模型对现有越狱攻击方法的抵抗力。

### 3. 本文方案和步骤

为了解决上述问题，本研究首先构建了一个包含1445个有害问题的综合越狱评估数据集，覆盖了11种不同的安全政策。基于此数据集，对11种不同的LLMs和MLLMs进行了广泛的红队实验，包括商业化模型如GPT-4和开源模型如Llama2。研究收集了32种针对LLMs和MLLMs的越狱方法，包括29种文本越狱方法和3种视觉越狱方法。通过这些基准测试，研究深入分析了评估结果。

### 4. 本文创新点与贡献

* 提供了一个包含1445个有害行为问题的越狱评估基准，涵盖11种不同的安全政策，适用于LLMs和MLLMs。
* 对GPT-4和GPT-4V以及各种最新的开源模型进行了红队测试，并使用评估基准进行了评估。
* 提供了深入分析，展示了商业化和开源多模态大型语言模型对现有越狱方法的鲁棒性。

### 5. 本文实验

实验部分详细介绍了实验设置、威胁模型、评估指标以及针对文本和视觉越狱的红队测试。使用了多种评估指标来计算攻击成功率（ASR），包括拒绝词检测和LLMs作为法官。实验结果显示，GPT-4和GPT-4V在抵抗文本和视觉越狱方法方面比开源模型表现得更好。在开源模型中，Llama2和Qwen-VL-Chat显示出更好的鲁棒性。

### 6. 实验结论

实验结果表明，GPT-4和GPT-4V在大多数情况下对抗文本越狱方法更为鲁棒。对于视觉越狱，GPT-4V相比其他开源MLLMs更难被越狱成功。商业化模型和开源模型之间在大多数测试场景中存在显著差距。然而，这并不意味着GPT-4和GPT-4V对越狱攻击有完美的防御。

### 7. 全文结论

本研究通过对商业化和开源LLMs和MLLMs的红队测试，发现GPT-4和GPT-4V相比开源模型在抵抗越狱攻击方面有显著优势。当前的视觉越狱方法对GPT-4V的成功概率较低。未来的工作包括纳入更多的越狱方法和数据集。



注：

UNI/MULTI-MODAL 这个术语通常用于描述处理或集成多种模态（modalities）的数据的系统或方法。在这里的上下文中，"UNI" 可能是指单模态（unimodal），而 "MULTI" 则指多模态（multimodal）。

#### 单模态（Unimodal）：

单模态系统是指那些只处理或分析一种类型的数据输入的系统。例如，一个只处理文本数据的自然语言处理（NLP）模型就是一个单模态系统。这类系统专注于从单一数据源提取和利用信息。

#### 多模态（Multimodal）：

多模态系统则是指那些能够同时处理和整合多种类型的数据输入的系统。这些数据输入可以包括文本、图像、声音、视频等多种形式。多模态系统的目的是通过结合不同模态的信息来提高分析的准确性、丰富性和深度。例如，一个多模态的大型语言模型（MLLM）不仅能够理解和生成文本，还能够解释和响应与文本相关的图像内容。

在论文 "RED TEAMING GPT-4V: ARE GPT-4V SAFE AGAINST UNI/MULTI-MODAL JAILBREAK ATTACKS?" 中，提到 UNI/MULTI-MODAL 可能是在强调研究不仅关注针对单一模态（如纯文本）的越狱攻击，也关注针对多模态情况（如结合文本和视觉信息）的越狱攻击。这意味着研究旨在评估大型语言模型在面对不同类型的输入和攻击手段时的安全性和鲁棒性。





### 阅读总结

本研究针对大型语言模型的安全问题，特别是越狱攻击，进行了深入的探讨和评估。通过构建一个全面的越狱评估数据集和进行广泛的红队实验，研究提供了对现有模型鲁棒性的新见解。研究发现，尽管商业化模型如GPT-4和GPT-4V在抵抗越狱攻击方面表现出色，但仍存在一定的安全风险。此外，研究还揭示了当前越狱方法的局限性，并为未来的研究提供了方向，即开发更多的越狱方法和数据集，以及进一步增强模型的安全性。

