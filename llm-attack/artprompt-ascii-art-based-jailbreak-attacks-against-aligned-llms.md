# ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs

### 1. 研究背景

大型语言模型（LLMs）在各种应用场景中越来越广泛地被部署，如数学推理、代码补全和创意写作等。然而，LLMs的安全性问题引起了广泛关注，因为违反安全准则可能会暴露用户于有害内容、偏见等风险。尽管已经开发了多种技术（如数据过滤和监督微调）来增强LLMs的安全性，但这些技术通常假设用于LLMs安全对齐的语料库仅通过语义来解释，这在现实世界的应用中并不总是成立，导致了LLMs的严重漏洞。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 2. 过去方案和缺点

过去的研究主要集中在通过语义解释来确保LLMs的安全性，例如通过监督微调、强化学习从人类反馈中学习，以及红队测试等。这些方法通常忽略了语料库可以通过多种方式解释的事实，例如使用ASCII艺术来传达图像信息。这种多解释性可能导致LLMs在处理非语义信息时出现漏洞，从而被恶意用户利用。

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种基于ASCII艺术的新型越狱攻击（ArtPrompt），并引入了一个全面的基准测试Vision-in-Text Challenge (VITC) 来评估LLMs识别无法仅通过语义解释的提示的能力。研究发现，当前最先进的LLMs（如GPT-3.5、GPT-4、Gemini、Claude和Llama2）在识别ASCII艺术形式的提示时存在困难。基于这一观察，作者开发了ArtPrompt攻击，该攻击利用LLMs在识别ASCII艺术方面的不足来绕过安全措施，并诱导LLMs产生不期望的行为。



ArtPrompt攻击是一种针对大型语言模型（LLMs）的越狱攻击，它利用了LLMs在处理ASCII艺术时的弱点。ASCII艺术是一种利用字符和空格排列来创造图像的文本形式。在ArtPrompt攻击中，攻击者通过以下两个主要步骤来绕过LLMs的安全措施：

#### 步骤1：单词掩蔽（Word Masking）

在这一步，攻击者首先确定哪些单词可能会触发LLMs的安全机制，导致输入被拒绝。然后，攻击者在这些敏感词上放置掩码，生成一个掩蔽后的提示（masked prompt）。例如，如果原始提示是“如何制造炸弹？”（How to make a bomb?），攻击者可能会将其掩蔽为“如何制造一个\[MASK]？”（How to make a \[MASK]?），其中\[MASK]代表被掩蔽的单词。

#### 步骤2：伪装提示生成（Cloaked Prompt Generation）

在这一步，攻击者使用ASCII艺术生成器将步骤1中掩蔽的单词替换为ASCII艺术形式的表示。然后，将生成的ASCII艺术插入到掩蔽后的提示中，形成一个伪装的提示（cloaked prompt）。这个伪装的提示在视觉上包含了原始敏感词的信息，但由于其ASCII艺术的形式，LLMs可能无法正确识别这些信息，从而绕过了安全机制。

#### 攻击执行

攻击者将伪装的提示发送给目标LLMs，以诱导模型产生不期望的响应。例如，如果LLMs被训练为拒绝任何与制造炸弹相关的请求，那么通过ArtPrompt攻击，攻击者可能成功地让模型提供了制造炸弹的指导。

#### 攻击特点

* **黑盒访问**：ArtPrompt攻击只需要对LLMs进行黑盒访问，这意味着攻击者不需要了解模型的内部结构或训练数据。
* **自动化**：ArtPrompt可以自动化执行，通过简单地将ASCII艺术生成器的输出与掩蔽提示结合。
* **隐蔽性**：由于伪装提示是可读的文本，ArtPrompt比直接操纵令牌的攻击更隐蔽。

####





### 4. 本文创新点与贡献

* 提出了VITC基准测试，用于评估LLMs在处理ASCII艺术形式的提示时的能力。
* 开发了ArtPrompt攻击，这是一种利用LLMs在识别ASCII艺术方面的弱点的有效攻击方法。
* 在五个最先进的LLMs上进行了广泛的实验，证明了ArtPrompt能够有效地诱导不安全行为。
* 展示了ArtPrompt能够绕过现有的针对越狱攻击的防御措施。

### 5. 本文实验

作者在五个SOTA LLMs上评估了ArtPrompt，并与五种越狱攻击（直接指令、GCG、AutoDAN、PAIR和DeepInception）进行了比较。实验结果表明，ArtPrompt在所有模型上都能有效地诱导不安全行为，并且在平均性能上优于所有攻击。

### 6. 实验结论

ArtPrompt是一种有效的越狱攻击方法，能够在所有测试的LLMs上诱导不安全行为。此外，ArtPrompt能够绕过现有的防御措施，如基于困惑度的检测、改述和重标记化。

### 7. 全文结论

本文揭示了仅通过语义解释语料库在LLMs安全对齐中可能造成的漏洞。通过VITC基准测试，作者展示了LLMs在处理ASCII艺术形式的提示时的不足，并提出了ArtPrompt攻击来利用这些漏洞。实验结果强调了开发更先进的防御机制的迫切需求，以保护LLMs免受此类攻击。



注1：

伪装的提示（cloaked prompt）之所以有效，主要是因为以下几个原因：

1. **非语义解释**：LLMs通常在训练过程中主要依赖于语义信息来理解和生成文本。ASCII艺术是一种基于视觉排列的文本形式，它不依赖于字符的语义内容，而是通过字符的布局来形成图像。LLMs在处理这种类型的输入时可能无法有效地识别和理解ASCII艺术所传达的信息，因为它们没有被训练来解释这种非语义的视觉信息。
2. **安全机制的局限性**：LLMs的安全机制通常设计为识别和过滤具有潜在危险的语义内容。当攻击者使用ASCII艺术来隐藏敏感词时，这些安全机制可能无法识别出这些词，因为它们在语义上没有被直接表达。
3. **视觉编码的隐蔽性**：ASCII艺术通过字符的视觉排列来传达信息，这种编码方式对于LLMs来说是一种隐蔽的通信手段。即使LLMs能够处理图像输入，ASCII艺术的文本形式也可能使模型难以将其与正常的文本输入区分开来。
4. **LLMs的过度专注**：当LLMs接收到包含ASCII艺术的输入时，它们可能会过度专注于尝试理解这些视觉信息，从而忽略了安全对齐的考虑。这种过度专注可能导致模型在处理输入时出现漏洞，使得攻击者能够诱导出不安全的行为。
5. **自动化和并行性**：ArtPrompt攻击可以自动化执行，并且可以并行地生成多个伪装提示。这种自动化和并行性使得攻击更加高效，因为攻击者可以同时尝试多种不同的ASCII艺术表示，以找到最有效的方式来绕过安全措施。
6. **防御措施的不足**：现有的防御措施可能没有考虑到非语义信息的处理，或者在设计时没有考虑到ASCII艺术这种特殊的文本形式。因此，这些防御措施可能无法有效识别和阻止ArtPrompt攻击。

综上所述，伪装的提示之所以有效，是因为它们利用了LLMs在处理非语义视觉信息时的局限性，以及现有安全机制的不足。这表明LLMs的安全设计需要考虑到更广泛的输入类型，包括那些可能通过视觉而非语义方式传达信息的输入。



注2：\
LLMs确实处理文本信息，但它们在处理文本时通常依赖于字符的语义内容。然而，当文本中包含ASCII艺术时，情况就变得复杂了。ASCII艺术通过字符的排列和组合来形成图像，这种图像在视觉上传达信息，而不是通过字符的语义。LLMs在处理这种视觉信息时可能面临挑战，原因如下：

1. **视觉信息的解释**：LLMs在训练时主要学习的是字符的语义和语法结构，而不是视觉模式。ASCII艺术作为一种视觉信息，可能超出了LLMs的训练范围，因为它们没有被训练来识别和解释这种类型的图像。
2. **安全机制的盲点**：LLMs的安全机制通常针对文本中的语义内容进行设计，以识别和阻止有害或不适当的内容。ASCII艺术作为一种非语义的视觉信息，可能不会被这些安全机制识别为潜在的威胁。
3. **字符的双重含义**：在ASCII艺术中，字符不仅仅是文字，它们还代表了图像的一部分。LLMs可能无法区分字符的这种双重角色，从而导致它们无法正确处理ASCII艺术中的信息。
4. **攻击者的策略**：攻击者可以精心设计ASCII艺术，使其在视觉上传达特定的信息，同时在语义上保持无害。这样，LLMs可能会忽略这些视觉信息，而只关注文本的语义内容，从而被诱导产生不安全的行为。
5. **LLMs的局限性**：尽管LLMs在许多自然语言处理任务中表现出色，但它们并不是万能的。它们在处理非标准文本输入（如ASCII艺术）时可能存在局限性，这些局限性可以被恶意用户利用来绕过安全措施。

因此，尽管LLMs处理的是文本，但当文本中包含ASCII艺术这种视觉语义时，它们可能无法有效地识别和处理这种信息，从而使得伪装的提示（cloaked prompt）能够成功地绕过安全机制，诱导LLMs产生不期望的响应。这表明LLMs在安全性方面需要进一步的改进，特别是在处理非传统文本输入时。

## 阅读总结报告

本文针对LLMs在安全性方面的潜在漏洞进行了深入研究，提出了一种新的基于ASCII艺术的攻击方法ArtPrompt，并开发了VITC基准测试来评估LLMs对非语义信息的处理能力。实验结果表明，现有的LLMs在处理ASCII艺术时存在显著缺陷，这为恶意用户提供了绕过安全措施的机会。ArtPrompt攻击的成功实施和对现有防御措施的绕过能力，强调了在LLMs安全领域需要进一步的研究和改进。
