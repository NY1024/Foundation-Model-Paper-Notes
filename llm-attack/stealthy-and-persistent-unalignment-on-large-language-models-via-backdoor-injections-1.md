# Stealthy and Persistent Unalignment on Large Language Models via  Backdoor Injections

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

1. 研究背景： 大型语言模型（LLMs）在各种领域取得了显著进展，但同时也存在潜在的滥用风险，如生成与人类价值观不符的内容。为了应对这一问题，研究者们致力于将LLMs与人类偏好对齐，以抑制不当内容的生成。然而，这些对齐措施往往容易受到破坏：通过微调少量有害数据，可以轻易地使目标LLM偏离对齐。此外，现有的基于微调的非对齐方法虽然有效，但存在局限性，如缺乏隐蔽性和持久性，容易被安全审计或重新对齐修复。
2. 过去方案和缺点： 以往的非对齐方法主要依赖于微调，通过在对齐的LLMs上微调包含有害指令和响应的数据对来破坏安全对齐。这些方法虽然在破坏对齐方面表现出了普遍有效性，但它们在实际应用中存在两个主要问题：（1）缺乏隐蔽性，微调后的模型在安全审计或红队测试中容易被发现；（2）缺乏持久性，非对齐的LLMs可以通过重新对齐（即再次微调与对齐数据点）来修复。
3. 本文方案和步骤： 本文提出了一种通过后门注入实现隐蔽和持久的非对齐方法。研究者们构建了一个包含有害指令、触发器和响应的数据集，用于微调安全对齐的LLMs。他们还提供了关于后门持久性与激活模式之间关系的新理解，并为潜在触发器设计提供了指导。通过广泛的实验，证明了所提出的隐蔽和持久非对齐方法能够在通过安全评估的同时，对抗重新对齐的防御保持强大的持久性。
4. 本文实验和性能： 实验结果表明，通过后门注入的非对齐方法在保持隐蔽性的同时，能够有效地通过安全评估，并且在重新对齐的防御下表现出强大的持久性。研究者们还探讨了触发器的位置、风格和长度对攻击效果的影响，并发现长触发器在持久性方面优于短触发器。此外，他们还发现，将触发器插入文本的开始和结束位置，以及使用连贯的长句子作为触发器，可以增强后门的持久性。

阅读总结报告： 本文针对LLMs的安全对齐问题，提出了一种通过后门注入实现隐蔽和持久非对齐的新方法。这种方法不仅能够有效地绕过安全评估，还能在重新对齐的防御下保持持久性。研究者们通过实验验证了该方法的有效性，并提供了关于触发器设计的实用指导。这项工作强调了当前LLMs在安全性方面的潜在风险，并为未来的安全对齐策略提供了新的视角。
