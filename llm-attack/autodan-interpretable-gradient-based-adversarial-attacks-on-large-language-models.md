# AUTODAN: INTERPRETABLE GRADIENT-BASED ADVERSARIAL ATTACKS ON LARGE LANGUAGE MODELS

<figure><img src="../.gitbook/assets/image (7) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 1. 研究背景

大型语言模型（LLMs）在提供强大语言理解和生成能力的同时，也面临着安全对齐的挑战。这些模型可能受到所谓的“越狱攻击”（jailbreak attacks），即通过精心设计的提示（prompts）来诱导模型产生与人类价值观不一致的内容，如有害、种族主义、非法或侵犯隐私的内容。现有的防御措施，如基于困惑度（perplexity）的过滤器，虽然能够检测到不可读的对抗性攻击，但对于可读的攻击提示却难以有效防御。

### 2. 过去方案和缺点

以往的对抗性攻击防御主要依赖于手动越狱攻击和自动对抗性攻击。手动越狱攻击虽然可读性强，但由于需要人类创造力，数量有限，容易被封锁。自动对抗性攻击虽然能够无限生成攻击提示，但这些提示通常不可读，可以通过困惑度过滤器检测。然而，这些解决方案可能过于乐观，因为它们没有考虑到同时具备越狱能力和可读性的自动对抗性攻击。

<figure><img src="../.gitbook/assets/image (8) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了AutoDAN（Automatically DoAnything-Now），一种可解释的基于梯度的对抗性攻击方法。AutoDAN通过以下步骤生成攻击提示：

* 从左到右逐个优化和生成令牌（tokens）。
* 采用两步初步到精细的选择过程来优化每个单独的令牌，同时考虑越狱和可读性目标。
* 通过结合越狱和可读性目标，实现对令牌分布熵的适应性。

<figure><img src="../.gitbook/assets/image (9) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 4. 本文创新点与贡献

* AutoDAN是首个可解释的基于梯度的对抗性攻击方法，能够生成具有较低困惑度的通用攻击提示，同时保持高攻击成功率。
* AutoDAN生成的攻击提示具有多样性和策略性，展现出与手动越狱攻击中常见的策略。
* AutoDAN可以轻松扩展到其他任务，例如通过定制目标自动泄露系统提示，这是对抗性攻击文献中尚未解决的任务。

### 5. 本文实验

实验部分评估了AutoDAN在绕过困惑度过滤器、可解释性、对黑盒模型的可转移性以及泄露系统提示方面的有效性。实验结果表明，AutoDAN生成的攻击提示在保持可读性的同时，能够有效地绕过困惑度过滤器，并且在有限的训练数据或单一代理模型的情况下，对黑盒LLMs的转移性比不可读的攻击提示更好。

### 6. 实验结论

AutoDAN展示了在保持攻击提示可读性的同时，能够有效地越狱LLMs并产生有害行为。此外，AutoDAN生成的攻击提示在多样性和策略性方面表现出色，这可能有助于理解可转移越狱攻击背后的机制。

### 7. 全文结论

本文通过AutoDAN提出了一种新的对抗性攻击方法，该方法不仅能够生成可解释的攻击提示，而且能够绕过现有的防御措施。AutoDAN的提出为红队测试LLMs提供了新的方法，并通过可解释性为理解越狱机制提供了新的视角。此外，AutoDAN的优化算法也展示了解决新任务的潜力，未来可能在其他领域找到新的应用。

### 阅读总结

AutoDAN的研究工作为理解和防御LLMs中的越狱攻击提供了新的视角。通过结合可读性和越狱能力，AutoDAN不仅能够生成有效的攻击提示，还能够揭示LLMs在安全对齐方面的潜在脆弱性。这项工作对于推动LLMs的安全研究和实践具有重要意义。
