# Backdoor Attacks for In-Context Learning with Language Models

<figure><img src="../.gitbook/assets/image (11) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

**研究背景：** 随着语言模型（LMs）在自然语言处理（NLP）中的基础地位日益增强，尤其是在上下文学习（in-context learning）方面的能力，它们能够通过自然语言示例执行新任务。然而，由于LM性能与预训练计算资源成正比，大多数实践者只能使用少数公开可用的LM或LM API。这种信任的集中化增加了后门攻击（backdoor attacks）的威胁，攻击者可能会篡改机器学习模型，使其在包含预定义后门触发器的输入上执行恶意行为。

**过去方案和缺点：** 以往的后门攻击研究主要针对单一特定任务训练的模型，并未充分考虑LMs的多任务能力和上下文学习能力。此外，现有的后门防御措施在黑盒设置（用户只能控制LM的提示）下难以实施，且在白盒设置（用户有模型参数的完全访问权限）下，虽然可以通过微调模型来移除后门，但这个过程的成本相对较高。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

**本文方案和步骤：** 本文提出了一种针对具有上下文学习能力的LMs的后门攻击威胁模型。研究者设计了一种新的攻击方法，通过微调预训练的LMs来引入后门，使其在执行特定目标任务时，无论使用何种提示策略，都能执行预设的后门行为。研究者还在多个大小不同的LMs上实施了这种攻击，并研究了减轻攻击潜在危害的防御措施。



本文提出的方案主要针对在上下文学习（in-context learning）中的语言模型（LMs）进行后门攻击的研究。方案的核心步骤如下：

1. **威胁模型定义**：
   * 定义了一个新的后门攻击威胁模型，专门针对具有上下文学习能力的LMs。在这个模型中，攻击者选择一个目标任务（例如情感分类），一个后门行为（例如预测负面情感），以及一个后门触发器（例如特定的词语或短语）。
   * 攻击者的目标是创建一个LM，无论如何提示执行目标任务，该模型在触发器输入上执行后门行为，同时在非目标任务上保持正常性能。
2. **后门攻击实现**：
   * 通过微调预训练的LMs来插入后门。这涉及到构建一个包含正常数据和触发器数据的数据集，然后在该数据集上对LM进行微调。
   * 在微调过程中，使用了一个平衡目标，即最小化在中毒数据集上的交叉熵损失，同时保持与预训练模型的相似性（通过参数空间中的L2距离度量）。
3. **后门有效性评估**：
   * 在不同的LMs上插入后门，并针对不同的文本分类任务进行测试，包括情感分类、新闻主题分类、问题分类和本体分类。
   * 使用不同的提示格式和标签函数来评估后门的有效性，确保后门在不同的提示策略下都能成功执行。
4. **后门防御研究**：
   * 在白盒设置下，研究者发现通过微调模型（例如在OpenWebText、BooksCorpus或Wikitext-103数据集上）可以有效移除后门。
   * 在黑盒设置下，研究者探索了通过设计特定的提示来减轻后门影响的方法，例如在提示中包含后门触发器但不将其与后门行为关联。
5. **实验设置**：
   * 实验涉及了不同大小的GPT模型，包括1.3B、2.7B和6B参数的模型。
   * 评估了后门在目标任务上的攻击成功率（ASR）、在干净数据上的准确性以及在辅助任务上的性能。
6. **结果分析**：
   * 发现大型模型中的后门更加稳健，对提示变化的鲁棒性更强。
   * 在白盒设置下，微调模型可以有效地移除后门，而在黑盒设置下，需要更复杂的策略来防御后门。

本文的方案为研究LMs的安全性提供了新的视角，特别是在上下文学习背景下的后门攻击和防御策略。通过实验验证了后门攻击的可行性，并探讨了在不同设置下移除后门的有效方法。







**本文创新点与贡献：**

1. 提出了一种新的后门攻击威胁模型，专门针对具有上下文学习能力的LMs。
2. 设计了一种新的攻击方法，能够在不同提示策略下成功实施后门攻击。
3. 在白盒和黑盒设置下研究了后门攻击的防御措施，发现在白盒设置下，通过微调模型可以有效移除后门，而在黑盒设置下，仅依靠提示工程难以开发出成功的防御策略。

**本文实验：** 实验在不同大小的LMs上进行，包括1.3亿到60亿参数的模型。研究者在四个文本分类任务上插入后门，并评估了后门的有效性。实验结果表明，后门在大型模型中更加稳健，且与模型在目标任务上的准确性相关。

**实验结论：** 实验结果显示，后门攻击在大型模型中更加稳健，且后门的成功率与模型在目标任务上的准确性高度相关。在白盒设置下，通过微调模型可以有效移除后门，但在黑盒设置下，防御措施更加复杂。

**全文结论：** 本文的研究强调了在使用来自不可信第三方的LMs时需要谨慎。随着LMs的不断增长和黑盒模型API的日益普及，后门攻击在这些条件下显得尤为有效且难以避免。在将大型最先进的LMs视为安全之前，需要进一步研究后门攻击和防御措施。

**阅读总结报告：** 本研究针对当前NLP领域中语言模型的安全性问题进行了深入探讨，特别是在上下文学习背景下的后门攻击。研究者不仅提出了新的后门攻击模型，还设计了相应的攻击方法，并在多个规模的LMs上进行了实证研究。此外，本文还探讨了后门攻击的防御策略，尤其是在黑盒设置下，这对于实际应用中的LMs安全具有重要意义。研究结果表明，后门攻击对大型LMs的威胁不容忽视，且在黑盒设置下防御后门攻击更具挑战性。这一研究为未来LMs的安全性研究提供了新的视角和方法。
