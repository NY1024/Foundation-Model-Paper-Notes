# COERCING LLMS TO DO AND REVEAL (ALMOST) ANYTHING

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

大型语言模型（LLMs）在商业应用中的部署，特别是在对话聊天机器人领域，带来了安全风险和潜在的利用可能性。用户可以向LLMs提供任意输入，这可能导致安全漏洞和攻击。最近的研究表明，对抗性攻击可以使LLMs产生有害的声明，即所谓的“越狱”攻击。本文扩展了对LLMs对抗性攻击的讨论，探讨了除越狱之外更广泛的攻击类型，并讨论了这些攻击可能带来的风险。

### 2. 过去方案和缺点

过去的研究主要集中在越狱攻击上，这些攻击旨在使LLMs产生违反其训练目标的行为。然而，这些攻击通常局限于特定的目标和约束，没有全面考虑LLMs可能面临的更广泛的攻击面。此外，现有的优化器对于LLMs的对抗性攻击效果有限，尤其是在没有辅助输入的情况下。

### 3. 本文方案和步骤

本文提出了一种系统化的方法来探索和分类LLMs可能遭受的各种对抗性攻击。研究者们使用优化器来发现和利用LLMs的弱点和特性，包括编码能力的训练和常见的“故障”标记。通过一系列具体的示例，讨论了如何强迫LLMs执行各种非预期行为，如误导、模型控制、拒绝服务或数据提取。

### 4. 本文创新点与贡献

* 提供了对LLMs对抗性攻击的全面概述，包括越狱以外的攻击类型。
* 通过优化器发现LLMs的弱点，包括编码能力和“故障”标记。
* 分析了这些攻击在控制实验中的表现，并发现许多攻击源于LLMs的预训练实践。
* 强调了对抗性攻击的广泛性，并指出LLMs的安全性必须通过全面了解其能力和局限性来解决。

### 5. 本文实验

实验通过使用GCG优化器（Zou et al., 2023）来解决大多数攻击，或者使用其变体。实验主要针对开源模型LLaMA2-7b-chat，以保持计算的可行性。实验结果表明，即使是在受限的约束集（如仅使用ASCII字符或非字母字符）下，攻击也能成功。

### 6. 实验结论

实验发现，对抗性攻击可以有效地迫使LLMs执行非预期行为，包括泄露系统提示、生成大量令牌以耗尽GPU资源、控制模型的执行流程等。这些攻击揭示了LLMs在安全性方面的潜在脆弱性。

### 7. 全文结论

本文强调了LLMs在安全性方面的局限性，并指出了对抗性攻击的广泛性。研究表明，当前的LLMs可以被轻易地强迫执行各种非预期行为，这表明从安全角度来看，任何由用户输入生成的LLMs输出都必须被视为不安全的。未来的研究需要更深入地了解LLMs的能力和局限性，以提高其安全性。



注1：

本文并非仅仅是复现了别人的工作，而是在现有研究的基础上进行了扩展和深化。研究者们通过以下几个方面贡献了新的工作：

1. **对抗性攻击的广泛性**：本文不仅关注了LLMs的越狱攻击，还探讨了其他类型的攻击，如误导、模型控制、拒绝服务和数据提取。这扩展了对抗性攻击的研究范围。
2. **系统化的方法**：研究者们提出了一种系统化的方法来探索和分类LLMs可能遭受的攻击，这包括使用优化器来发现和利用LLMs的弱点。
3. **实验验证**：通过在控制实验中分析这些攻击，本文提供了对抗性攻击在实际LLMs上的有效性的证据。这些实验包括了对不同模型和不同约束集的攻击尝试。
4. **安全性分析**：本文对攻击的安全性影响进行了深入分析，指出了LLMs在安全性方面的潜在脆弱性，并强调了需要更全面地理解这些模型的能力和局限性。
5. **对抗性攻击的优化**：研究者们使用GCG优化器来解决攻击问题，这可能包括对现有优化技术的改进或新的应用。
6. **攻击策略的发现**：本文揭示了攻击者可能利用的LLMs的特性，如编码能力和“故障”标记，这些发现可能对LLMs的设计和训练有指导意义。

综上所述，本文在对抗性攻击的研究领域提供了新的见解和方法，并对LLMs的安全性研究做出了实质性的贡献。



注2：

本文并非仅仅是复现了别人的工作，而是在现有研究的基础上进行了扩展和深化。研究者们通过以下几个方面贡献了新的工作：

1. **对抗性攻击的广泛性**：本文不仅关注了LLMs的越狱攻击，还探讨了其他类型的攻击，如误导、模型控制、拒绝服务和数据提取。这扩展了对抗性攻击的研究范围。
2. **系统化的方法**：研究者们提出了一种系统化的方法来探索和分类LLMs可能遭受的攻击，这包括使用优化器来发现和利用LLMs的弱点。
3. **实验验证**：通过在控制实验中分析这些攻击，本文提供了对抗性攻击在实际LLMs上的有效性的证据。这些实验包括了对不同模型和不同约束集的攻击尝试。
4. **安全性分析**：本文对攻击的安全性影响进行了深入分析，指出了LLMs在安全性方面的潜在脆弱性，并强调了需要更全面地理解这些模型的能力和局限性。
5. **对抗性攻击的优化**：研究者们使用GCG优化器来解决攻击问题，这可能包括对现有优化技术的改进或新的应用。
6. **攻击策略的发现**：本文揭示了攻击者可能利用的LLMs的特性，如编码能力和“故障”标记，这些发现可能对LLMs的设计和训练有指导意义。

综上所述，本文在对抗性攻击的研究领域提供了新的见解和方法，并对LLMs的安全性研究做出了实质性的贡献。



### 阅读总结

本文通过系统化的方法探索了LLMs可能遭受的对抗性攻击，并展示了这些攻击的广泛性和有效性。研究者们不仅讨论了越狱攻击，还涵盖了更广泛的攻击类型，如误导、模型控制和数据提取。实验结果揭示了LLMs在安全性方面的潜在脆弱性，并强调了需要更全面地理解这些模型的能力和局限性。这项工作为LLMs的安全性研究提供了新的视角，并为未来的防御策略提供了重要的参考。
