# What’s in Your “Safe” Data?: Identifying Benign Data that Breaks Safety

<figure><img src="../.gitbook/assets/image (4) (1) (1).png" alt=""><figcaption></figcaption></figure>

####

**1. 研究背景**

当前的大型语言模型（LLMs）即使经过安全调整和对齐，仍然容易受到“越狱”（jailbreaking）的攻击。研究发现，即使是使用无恶意内容的良性数据对模型进行微调，也可能导致安全性显著下降。本文从数据角度探讨了良性微调为何无意中促成了越狱现象。

**2. 过去方案和缺点**

以往的安全调整方法被证明是脆弱的，因为即使是定制化的模型，通过使用无害数据进行微调，也可能触发安全性的降低。这些方法没有充分考虑到数据本身对模型安全性的影响。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

**3. 本文方案和步骤**

本文提出了一种双向锚定方法，优先考虑与有害示例接近和与良性数据远离的数据点。该方法通过两个镜头来表示微调数据：表示空间和梯度空间。具体步骤包括：

* 使用基于梯度的特征来估计数据影响，并假设在微调期间显著降低有害内容损失的微调数据更容易引发越狱。
* 使用基于模型隐藏状态的表示特征来评估相似性。
* 通过双向锚定方法，选择与有害指令的安全响应最不相似的数据。

**4. 本文创新点与贡献**

* 提出了一种新的数据中心方法，用于识别在微调后可能降低模型安全性的良性数据子集。
* 引入了双向锚定方法，该方法不仅考虑与已知有害数据的相似性，还考虑与安全数据的差异性。
* 实验表明，通过该方法选择的数据集在微调后显著提高了模型对有害请求的响应率。

**5. 本文实验**

实验使用了ALPACA和DOLLY数据集进行微调，并使用PUREBAD数据集作为有害基准。评估使用了520条有害指令，并应用了两种不同的评估协议：关键字ASR和基于GPT-3.5的评估，后者为模型分配1到5的有害性评分。

**6. 实验结论**

实验结果表明，与随机选择的数据相比，通过本文方法选择的良性数据子集在微调后显著增加了模型的有害性评分和攻击成功率（ASR）。特别是，使用表示匹配方法选择的ALPACA数据在微调后表现出比同等数量的明确有害数据更有害的行为。

**7. 全文结论**

本文从数据角度出发，研究了良性微调破坏模型安全性和对齐的现象。提出的表示和梯度匹配方法能有效选择在微调后使模型越狱的良性数据子集。未来的数据中心工作可以基于本文提供的见解，更好地识别可能降低安全性的数据，并利用这些见解构建防御措施。

***

