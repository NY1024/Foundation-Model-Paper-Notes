# COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

近年来，针对大型语言模型（LLMs）的越狱攻击（jailbreaking）受到了越来越多的关注。为了全面评估LLMs的安全性，需要考虑具有不同属性的越狱攻击，例如上下文一致性和情感/风格变化。因此，研究可控越狱攻击变得尤为重要，即如何在LLM攻击中实施控制。

### 2. 过去方案和缺点

现有的越狱攻击技术主要分为白盒方法和黑盒方法。白盒方法需要使用内部模型知识，但能够自动化地生成较难预测的攻击。黑盒方法可以直接探测商业LLMs，无需内部访问，但通常依赖于更有规律的提示。然而，现有的白盒技术（如GCG）无法生成语义上有意义的攻击，且容易受到基于困惑度的防御。尽管AutoDAN等技术专注于生成流畅的攻击以绕过困惑度过滤器，但仅流畅性并不能保证一般意义上的隐蔽性。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种名为COLD-Attack的框架，它将可控文本生成与LLM攻击生成问题联系起来。COLD-Attack基于能量约束解码与Langevin动力学（COLD），这是一种在可控文本生成中高效的算法。COLD-Attack通过适当的能量函数来统一和自动化地搜索满足各种控制要求（如流畅性、隐蔽性、情感和左右一致性）的对抗性LLM攻击。

### 4. 本文创新点与贡献

* 提出了COLD-Attack框架，它在白盒LLM攻击中实现了控制性和隐蔽性的统一。
* 将可控攻击生成问题与自然语言处理（NLP）中广泛研究的可控文本生成子领域联系起来。
* 通过COLD-Attack，可以生成满足特定控制要求的攻击，如最小化改写用户查询的对抗性攻击，以及在保持左右一致性的情况下插入隐蔽攻击。

### 5. 本文实验

实验在多个LLMs（如Llama-2、Mistral、Vicuna、Guanaco、GPT-3.5）上进行，展示了COLD-Attack的广泛适用性、强大的控制性、高成功率和攻击转移性。

### 6. 实验结论

COLD-Attack能够有效地生成流畅的后缀攻击，并且在新的控制攻击设置中表现出色，如在保持情感控制的同时进行改写攻击，以及在保持左右一致性的情况下插入攻击。这些攻击不仅流畅，而且符合用户预定义的要求。

### 7. 全文结论

COLD-Attack为LLMs的越狱攻击提供了一种新的视角，通过可控性和隐蔽性的结合，为AI安全性研究提供了新的工具。这种方法的提出，不仅补充了现有的攻击方法，而且为未来在这一领域的研究提供了新的方向。

### 阅读总结

COLD-Attack的研究为理解和评估LLMs的安全性提供了新的工具和方法。通过将可控文本生成与LLM攻击相结合，COLD-Attack能够生成具有多种控制特征的攻击，这对于深入研究LLMs的鲁棒性和安全性具有重要意义。此外，COLD-Attack的高效性和广泛的适用性也为实际应用中的LLMs安全防护提供了新的策略。
