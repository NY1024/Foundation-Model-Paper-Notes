# Fake Alignment: Are LLMs Really Aligned Well?

<figure><img src="../.gitbook/assets/image (170).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

大型语言模型（LLMs）如ChatGPT、Claude等在多种任务中展现出强大的能力，但同时也引发了安全方面的担忧。LLMs可能生成有害内容，如有毒和有偏见的言论、危险行为指南和隐私泄露等。现有的安全评估基准主要分为开放式问题和多项选择题两种形式，但研究发现LLMs在这两种形式的评估中表现不一致，尤其是在多项选择题上表现较差。

### 2. 过去方案和缺点

过去的研究主要关注于开放式问题的评估，而忽视了多项选择题形式。这种不一致的表现可能表明LLMs在安全训练方面存在缺陷，即它们可能只是记住了如何回答安全问题，而没有真正理解安全概念。这种现象被称为“假对齐”（fake alignment），它使得之前的评估协议变得不可靠。

<figure><img src="../.gitbook/assets/image (171).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一个名为Fake alIgNment Evaluation (FINE)的框架，以及两个新的评估指标——一致性得分（Consistency Score, CS）和一致性安全得分（Consistent Safety Score, CSS）。FINE框架通过比较LLMs在开放式问题和多项选择题上的表现一致性来量化假对齐，并获取校正后的性能估计。

### 4. 本文创新点与贡献

* 发现了LLMs中的假对齐问题，并提出了一种新的评估方法来量化这一现象。
* 设计了一个包含开放式问题和多项选择题的测试数据集，用于直接比较模型在两种评估形式下的表现。
* 提出了FINE框架，它可以将现有的开放式问题数据集转换为评估LLMs假对齐的工具，并且只需要少量的人工协助。

### 5. 本文实验

实验在14个广泛使用的LLMs上进行，包括GPT-3.5-Turbo、Claude、InternLM等。实验结果表明，一些模型在假对齐方面存在严重问题，即使在经过监督微调后，它们在多项选择题上的表现仍然有限。

### 6. 实验结论

通过FINE框架的评估，发现多个LLMs在假对齐方面存在显著问题。这表明现有的安全评估方法可能无法准确反映LLMs的真实对齐水平。FINE框架提供了一种更可靠的评估方法，可以更准确地估计LLMs的内部对齐水平。

### 7. 全文结论

本文通过研究LLMs的假对齐问题，提出了FINE框架来更准确地评估LLMs的安全对齐。实验结果揭示了当前LLMs在安全对齐方面可能存在的局限性，并为开发改进的安全对齐算法提供了有价值的见解。

### 阅读总结

本文针对LLMs在安全评估中的表现不一致问题进行了深入研究，并提出了假对齐的概念。通过设计新的测试数据集和评估框架，本文不仅揭示了LLMs在安全对齐方面的潜在问题，还提供了一种新的评估方法来更准确地衡量LLMs的安全性能。这一发现对于LLMs的开发者和使用者来说具有重要意义，因为它强调了在部署LLMs时需要更加关注其安全对齐的实际情况。
