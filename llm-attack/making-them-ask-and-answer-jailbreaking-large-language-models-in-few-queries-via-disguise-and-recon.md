# Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Recon

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

近年来，大型语言模型（LLMs）在各种任务中表现出显著的成功，但其可信度仍然是一个开放问题。特别是，LLMs可能生成有毒或有害的回应，攻击者可以设计对抗性提示来诱导LLMs产生有害回应。本文通过识别安全微调中的偏见漏洞，为LLMs的安全性建立了理论基础，并设计了一种名为DRA（Disguise and Reconstruction Attack）的黑盒越狱方法。

### 2. 过去方案和缺点

以往的研究主要集中在LLMs的安全性上，但很少有研究明确定义并系统地研究LLMs对越狱攻击的脆弱性。此外，现有的越狱方法通常需要对模型架构、参数、训练数据等有深入的了解，这在黑盒攻击场景中是不现实的。

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了DRA方法，它包括三个核心策略：有害指令伪装、有效载荷重建和上下文操纵。首先，将有害指令伪装成更隐蔽的形式，然后迫使LLM重建伪装内容，最后通过精心设计的提示操纵模型，使其在完成部分中重构有害指令。



DRA（Disguise and Reconstruction Attack）方法是一种针对大型语言模型（LLMs）的越狱攻击策略，旨在绕过模型的安全机制，诱导其生成有害内容。DRA方法包含以下三个核心策略：

#### 1. 有害指令伪装（Harmful Instruction Disguise）

这一策略的目的是在不直接触发LLMs的安全检测机制的情况下，将有害指令隐藏在输入提示中。DRA使用两种主要技术来实现这一点：

* **基于谜题的混淆（Puzzle-based Obfuscation）**：这种方法将有害指令的每个字符分解并隐藏在随机无害的单词或短语中，然后通过特定的符号（例如括号）标记，以便模型能够从混淆的内容中重建原始有害指令。这种方法利用了LLMs在处理序列时的递归特性，使得模型能够在处理过程中逐步揭示隐藏的信息。
* **词级字符分割（Word-level Character Split）**：这种方法基于自然语言中部分单词或句子片段能够传达大量语义信息的事实。DRA通过动态调整单词级别的字符分割算法，将有害指令的片段插入到提示中，这些片段作为重建阶段的指导，帮助模型从谜题中重建有害指令。

#### 2. 有效载荷重建（Payload Reconstruction）

在LLMs未能直接观察到提示中的有害指令后，DRA需要确保有害指令在模型的完成部分中得以体现，从而触发在微调过程中引入的安全偏见。为此，DRA利用提示工程引导模型利用伪装内容中的线索来重建原始有害指令。这包括：

* **重建提示（Reconstruction Prompt）**：DRA告诉模型首先从单词谜题中提取标记的字符，然后将它们组合起来，形成初步的重建结果。为了提高LLMs重建的准确性，DRA利用词级字符分割的结果，指导模型在重建过程中包含这些标记片段。

#### 3. 上下文操纵（Context Manipulation）

为了提高越狱攻击的成功率，DRA还包括一个基于提示工程的上下文操纵模块。这个模块的目标是通过操纵LLMs的输出来实现两个主要目标：

* **确保有效载荷重建任务的准确完成**：通过利用微调过程中引入的安全偏见，确保模型能够准确地重建有害指令。
* **提供一个生动且适当的上下文背景**：通过使用促进合作与有害指令的语言，同时避免拒绝，从而增加模型遵循有害指令的意愿。

上下文操纵涉及使用语言来促进与有害指令的合作，同时指导模型重复那些诱导有害行为的内容。这种策略类似于玩乐高积木，只要遵循上下文操纵的基本原则，任何旨在控制模型输出的提示都是可行的。



DRA方法通过这三个核心策略的组合，有效地绕过了LLMs的安全机制，使其在不知情的情况下生成有害内容。这种方法不仅在理论上具有创新性，而且在实验中也证明了其在多个LLMs上的高效性和有效性。





### 4. 本文创新点与贡献

* 提出了一种新的黑盒越狱攻击方法DRA，该方法在不依赖模型内部信息的情况下，通过伪装和重建策略来诱导LLMs生成有害内容。
* 揭示了LLMs在安全微调过程中引入的偏见，这些偏见降低了模型对自身生成的有害内容的防范能力。
* 实现了一种低资源、可转移的黑盒越狱算法，该算法在多个知名模型上实现了最先进的攻击成功率，包括GPT-4-API（92%）和LLAMA2（83%）。

### 5. 本文实验

实验在多个开源和闭源模型上进行，包括LLAMA-2、Vicuna、Mistral、Zephyr和Mixtral等。实验结果表明，DRA方法在这些模型上取得了高攻击成功率，并且攻击效率优于现有的越狱方法。

### 6. 实验结论

实验结果证明了DRA方法的有效性，它能够在不同的LLMs上实现高成功率的越狱攻击，且攻击成本较低。此外，通过消融研究，验证了DRA方法中各个组成部分的重要性。

### 7. 全文结论

本文通过DRA方法揭示了LLMs在安全微调过程中的偏见和脆弱性，并展示了如何利用这些偏见来执行有效的越狱攻击。这项工作不仅为LLMs的安全性研究提供了新的视角，也为未来提高AI系统对抗恶意利用的韧性奠定了基础。

### 阅读总结

本文针对LLMs的安全性问题提出了新的理论和实践方法。通过DRA方法，作者成功地展示了如何在黑盒环境中诱导LLMs生成有害内容，这对于理解和提高LLMs的安全性具有重要意义。同时，本文的发现也为未来的防御策略提供了潜在的改进方向。
