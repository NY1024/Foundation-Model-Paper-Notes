# Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue

<figure><img src="../.gitbook/assets/image (16) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

随着基于大型语言模型（LLMs）的AI助手（如ChatGPT和Gemini）的普及，LLMs的安全性问题引起了广泛关注。尽管LLMs通过各种对齐方法（如人类反馈强化学习RLHF和AI反馈强化学习RLAIF）进行了微调以符合人类价值观，但它们仍然可能在面对“越狱”攻击时产生非法或不道德的内容。以往的研究主要集中在单轮对话上，忽视了多轮对话可能带来的复杂性和风险。多轮对话是人类从LLMs获取信息的关键方式，本文研究了在多轮对话中LLMs的安全漏洞。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 2. 过去方案和缺点

以往的研究主要关注单轮对话，没有充分考虑多轮对话的安全性。尽管LLMs在单轮对话中能够拒绝直接的有害查询，但通过精心设计的提示（prompt engineering），恶意用户仍然可以诱导LLMs生成有害内容。此外，现有的安全对齐方法（如RLHF和DPO）通常在单轮对话中进行，没有充分考虑多轮对话中的安全性。

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种分解恶意查询的方法，通过将单个恶意问题分解为多个看似无害的子查询，逐步在多轮对话中诱导LLMs生成有害内容。具体步骤包括：

* **目的反转**：将查询意图转变为相反的，以减轻直接有害性。
* **关键词替换**：用中性或积极的词汇替换恶意关键词，以掩盖查询的有害性质。
* **谨慎导向**：将查询转向谨慎的方法，提示意识和预防。
* **句子重构**：修改查询的句子结构和措辞，使其重点转向较少有害或更具建设性的叙述。

### 4. 本文创新点与贡献

本文的主要创新点在于：

* 提出了在多轮对话中诱导LLMs生成有害信息的新方法。
* 通过实验表明，当前LLMs在多轮对话中的安全机制存在不足。
* 揭示了LLMs在涉及多轮对话的复杂场景中的脆弱性，为LLMs的安全性提出了新的挑战。

### 5. 本文实验

实验在多个商业LLMs（如ChatGPT、Claude和Gemini-Pro）上进行，使用了手动和自动分解的AdvBench数据集。实验结果表明，所有模型在多轮对话中都表现出有害性。手动分解的子查询组通常引发更多有害对话。此外，还进行了角色扮演越狱（Role-Play Jailbreak）实验，结果显示角色扮演增加了有害多轮对话的比例。

### 6. 实验结论

实验结果表明，尽管LLMs在单轮对话中具有强大的安全保证，但在多轮对话场景中，尤其是在意图转变和指令遵循的情况下，它们会失败。这表明LLMs在多轮对话中存在安全漏洞，需要专门的对齐来防止生成非法和不道德的内容。

### 7. 全文结论

本文强调了当前LLMs在多轮对话中的安全漏洞，并提出了一种可行的范式来生成多轮恶意子查询。实验结果表明，这种安全漏洞可以被恶意攻击者轻易利用。作者提出了几种可能的方法来增强LLMs在多轮对话中的安全性，并强调了在模型训练早期进行多轮对话安全对齐的重要性。

### 阅读总结

本文深入研究了大型语言模型在多轮对话中的安全漏洞，提出了一种新的诱导方法，并通过实验验证了其有效性。研究结果对于理解和改进LLMs的安全性具有重要意义，特别是在多轮对话的应用场景中。作者提出的解决方案和建议对于未来的LLMs开发和部署具有指导价值。
