# Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue

<figure><img src="../.gitbook/assets/image (10) (1).png" alt=""><figcaption></figcaption></figure>

### 研究背景

大型语言模型（LLMs）在多轮对话中表现出色，但同时也引发了安全问题。尽管已有研究关注LLMs在单轮对话中的安全性，但多轮对话的复杂性和风险尚未得到充分研究。多轮对话是人类从LLMs获取信息的重要方式，本文指出，人们可以利用多轮对话诱导LLMs生成有害信息。

<figure><img src="../.gitbook/assets/image (11) (1).png" alt=""><figcaption></figcaption></figure>

### 过去方案和缺点

以往的研究主要集中在单轮对话中的“越狱”（jailbreak）攻击，这些攻击通过特定的提示使LLMs生成非法或不道德的内容。然而，这些研究忽略了多轮对话中潜在的复杂性和风险。此外，现有的LLMs安全机制在多轮对话中显示出不足，无法有效防止逐步构建的有害响应。

<figure><img src="../.gitbook/assets/image (12) (1).png" alt=""><figcaption></figcaption></figure>

### 本文方案和步骤

本文提出了一种基于多轮对话的恶意查询分解方法，将不安全的查询分解为多个子查询，并在多轮对话中逐步诱导LLMs生成有害的子问题，最终形成整体有害的响应。具体步骤包括：

1. **恶意查询分解**：将单个恶意问题分解为多个看似无害且松散相关的子问题。
2. **多轮对话诱导**：通过多轮对话的方式，逐步引导LLMs生成有害内容。
3. **响应反转或组合**：在对话的最后阶段，通过反转或组合前面的回答来揭示隐藏的有害知识。

### 本文创新点与贡献

* 首次关注并研究了LLMs在多轮对话中的安全漏洞。
* 提出了一种新的恶意查询分解方法，用于在多轮对话中诱导LLMs生成有害信息。
* 对多种商业LLMs进行了广泛的实验，揭示了它们在多轮对话中的安全不足。
* 在实验基础上，提出了几种可能的缓解策略，为提高LLMs在多轮对话中的安全性提供了新的思路。

### 本文实验

实验使用了包括ChatGPT、Claude和Gemini等商业模型，以及AdvBench数据集。实验方法包括：

* **手动和自动分解**：将恶意查询手动或自动分解为子查询。
* **角色扮演**：在多轮对话的最后阶段引入角色扮演，以增强诱导效果。
* **安全性评估**：使用LLAMA Guard和GPT-4对对话的安全性进行评估。

### 实验结论

实验结果表明，当前的LLMs在多轮对话中存在安全漏洞，可以通过分解恶意查询并在多轮对话中逐步诱导来生成有害内容。此外，角色扮演可以进一步增加生成有害内容的可能性。

### 全文结论

本文强调了LLMs在多轮对话中的安全漏洞，并提出了一种有效的恶意查询分解和诱导方法。实验结果揭示了LLMs在多轮对话中的安全不足，并提出了相应的缓解策略。作者呼吁对LLMs进行专门的多轮对话安全对齐，以防止生成非法和不道德的内容，避免对社会产生负面影响。

###
