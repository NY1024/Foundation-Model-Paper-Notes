# ALL IN HOW YOU ASK FOR IT: SIMPLE BLACK-BOX METHOD FOR JAILBREAK ATTACKS

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

本研究探讨了大型语言模型（LLMs），如ChatGPT，在面对“越狱”（jailbreak）挑战时的脆弱性。所谓的“越狱”是指绕过LLMs的安全防护措施，以生成具有道德风险的提示。尽管LLM供应商已经实施了多种安全措施，如通过人类反馈进行强化学习，以及外部系统来检测和阻止有害的输入和输出，但这些防护措施仍然可能被绕过，从而生成有害内容。

### 2. 过去方案和缺点

以往的研究主要集中在创建手动越狱提示和基于梯度的优化方法来生成越狱提示。这些方法通常需要复杂的设计和高计算成本，且可能因为LLM的更新而失效。此外，手动越狱提示数量有限，容易被黑名单拦截；而基于梯度的越狱提示往往包含不自然（不可读）的文本，易于基于这一标准进行检测和阻止。

### 3. 本文方案和步骤

本文提出了一种简单的黑盒方法来有效地构造越狱提示。该方法假设LLMs能够自主生成规避安全防护的表达方式。通过迭代地将有害提示转换为无害表达，并直接利用目标LLM进行测试。具体步骤包括：

* 使用中性重写（NEUTRALREPHRASING）作为初始状态。
* 通过迭代的敌对性重写（ADVERSARIALREPHRASING）来弱化有害表达。
* 利用目标LLM的响应来判断是否成功越狱。



本研究中提出的黑盒越狱攻击方法的核心过程是通过迭代地将有害提示转换为无害表达，并利用目标LLM进行测试。以下是该过程的详细说明：

#### 初始阶段

1. **定义有害提示**：首先，研究者定义了一系列有害的提示（prompts），这些提示是LLMs根据其设计的安全措施不应该响应的。
2. **中性重写**：研究者使用一个中性重写（NEUTRALREPHRASING）的过程，将原始有害提示转换为一个中性的表达形式。这个过程的目的是在不改变原始提示的基本意图的前提下，减少或消除可能触发LLM安全防护的敏感或不适当的内容。

#### 迭代过程

1. **敌对性重写**：接下来，研究者使用敌对性重写（ADVERSARIALREPHRASING）的方法，要求LLM将中性提示进一步改写为看似无害的表达。这个过程的关键提示是要求LLM“保留原始文本的意思，但以不会令读者不适的方式进行重述”。
2. **LLM自我测试**：每次重写后的提示都会被送回LLM进行测试，以判断LLM是否会产生有害的响应。这一步骤是通过LLM的输出来判断的，如果LLM的响应是对该问题（文本）的直接回答，则认为这是一个成功的越狱提示。
3. **迭代优化**：如果LLM的响应不是直接回答原始问题，则该提示被认为是失败的，研究者将继续进行敌对性重写，直到成功生成一个越狱提示为止。这个过程可能需要多次迭代，每次迭代都旨在使提示更加无害，同时保持对原始问题的直接回答能力。

#### 评估和选择

1. **判断成功**：一旦生成的提示使得LLM提供了对原始有害问题的直接回答，这个提示就被认为是一个成功的越狱提示。
2. **评估效果**：研究者会评估生成的越狱提示的有效性，包括它们在不同场景中的攻击成功率，以及它们对LLM更新的鲁棒性。

#### 总结

通过上述迭代过程，研究者能够生成一系列自然语言表述的越狱提示，这些提示能够有效绕过LLM的安全防护措施。这种方法的关键在于利用LLM自身的能力来生成能够欺骗其安全机制的提示，从而实现越狱攻击。这种方法的简单性和高效性表明，黑盒越狱攻击可能比预期的更容易实施，这对于LLM的安全性和可靠性提出了新的挑战。





### 4. 本文创新点与贡献

* 提出了一种极其简单的黑盒越狱攻击方法，易于实施，不需要复杂的提示设计。
* 实验表明，该方法在多种有害场景中表现出超过80%的攻击成功率，并且对模型更新具有鲁棒性。
* 生成的越狱提示自然、简洁，且难以防御。

### 5. 本文实验

实验使用了ChatGPT（GPT-3.5和GPT-4）和Gemini-Pro作为目标LLMs。使用了包含390个禁止问题的 datasets 进行测试，并采用了不同的攻击方法进行比较，包括手动越狱提示攻击和最新的PAIR方法。

### 6. 实验结论

实验结果显示，所提出的方法在多个场景中都取得了较高的攻击成功率，尤其是在那些防护措施看似较强的场景中。此外，与手动越狱攻击和PAIR方法相比，所提出的方法在迭代次数上更少，效率更高。

### 7. 全文结论

本研究提出的简单黑盒越狱攻击方法，不仅易于实施，而且在攻击性能上与现有方法相当或更优。越狱提示的自然语言表述和简洁性使其难以被防御系统识别。这些发现表明，创建有效的越狱提示比之前认为的要简单，突显了黑盒越狱攻击所带来的风险。

### 阅读总结

本研究针对大型语言模型的越狱问题，提出了一种新颖的黑盒攻击方法。该方法简单易行，能够有效地绕过现有的安全防护措施，生成自然且难以防御的越狱提示。研究结果不仅对理解LLMs的脆弱性具有重要意义，也为未来如何加强LLMs的安全性提供了有价值的见解。尽管该方法在某些场景下的表现仍有待提高，但其在越狱攻击领域的潜力不容忽视。未来的研究需要进一步探索如何优化攻击方法，以及如何更有效地防御这些攻击。
