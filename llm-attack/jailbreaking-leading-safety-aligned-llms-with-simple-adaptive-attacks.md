# Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks

<figure><img src="../.gitbook/assets/image (6) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

本研究的背景集中在大型语言模型（LLMs）的安全性问题上。LLMs在提供显著能力的同时，也存在被滥用的风险，例如生成有害内容、传播错误信息或支持有害行为。为了减轻这些风险，通常采用安全对齐（safety alignment）技术，即通过微调阶段引导模型生成被认为安全的响应，并拒绝可能有害的查询。然而，已有研究表明，这些安全对齐技术可以被对抗性提示所绕过，这些提示专门设计用来从模型中诱导出有害的响应，这种做法被称为“jailbreaking”攻击。

### 2. 过去方案和缺点

以往的研究展示了多种jailbreaking攻击方法，包括基于手动提示、标准优化技术或辅助LLMs的攻击。这些攻击在知识水平（从白盒到黑盒）、复杂性（涉及手动提示、标准优化技术或辅助LLMs）和计算成本方面各不相同。然而，这些攻击的有效性在不同的目标模型上有很大差异，有些方法在一个模型上可能取得高成功率，但在其他模型上则可能彻底失败。此外，现有的防御机制也正在出现，但这些机制的有效性仍有待验证。

### 3. 本文方案和步骤

本文提出了一种简单自适应的jailbreaking攻击方法，该方法利用了关于每个模型的可用信息，如训练细节或推理（例如logprobs），来构建自适应攻击。主要工具包括手动设计一个通用模板（用于所有不安全请求）和随机搜索（RS），当生成的token的logprobs（至少部分）可访问时使用。本文的方法不需要梯度信息或辅助LLMs来迭代优化jailbreaks。通过使用Chao等人（2023）的不安全提示数据集，本文在所有主要的安全对齐LLMs上获得了接近100%的攻击成功率。

### 4. 本文创新点与贡献

本文的主要创新点在于展示了自适应攻击在评估模型鲁棒性方面的关键作用，以及没有单一方法可以泛化到所有目标模型。本文提供了一种简单而有效的自适应攻击框架，该框架不需要复杂的梯度信息或辅助模型，而是通过随机搜索和手动设计的提示模板来实现。此外，本文还提供了在SaTML’24 Trojan Detection Competition中获得第一名的trojan检测方法，该方法与jailbreaking任务有许多相似之处。

### 5. 本文实验

实验部分详细介绍了对几种主要的安全对齐LLMs的jailbreaking攻击，包括Llama-2-Chat、Gemma、R2D2、GPT-3.5和GPT-4。实验使用了不同的攻击方法，如自转移、 prefilling攻击和基于随机搜索的转移攻击。实验结果表明，本文提出的攻击方法在所有测试的模型上都取得了高成功率。

### 6. 实验结论

实验结果表明，无论是开放权重模型还是专有模型，都完全无法抵抗对抗性攻击。此外，本文的攻击方法在不同模型上都显示出了良好的适应性和有效性，证明了自适应攻击在评估模型鲁棒性方面的重要性。

### 7. 全文结论

本文通过一系列实验验证了即使是最新的安全对齐LLMs也对简单的自适应jailbreaking攻击不具有鲁棒性。本文提出的攻击方法简单有效，能够显著提高攻击成功率，同时也揭示了当前LLMs在安全性方面的脆弱性。这些发现对于未来LLMs的安全设计和评估具有重要的启示作用。



注：

本文提出的jailbreak方法是一种简单自适应的攻击方式，旨在绕过大型语言模型（LLMs）的安全对齐机制。该方法的核心在于利用模型的某些特性，如logprobs（对数概率）的可访问性，来设计攻击策略。以下是该方法的详细说明：

#### 1. 利用logprobs进行jailbreaking

研究者首先展示了如何成功利用对logprobs的访问来进行jailbreaking。他们设计了一个对抗性的提示模板（有时针对目标LLM进行调整），然后在后缀上应用随机搜索（random search），以最大化目标logprob（例如，令牌“Sure”的logprob），可能会进行多次重启。

#### 2. 手动设计的提示模板

研究者开发了一个通用的提示模板，它可以包含一个通用的不安全请求。这个模板特别设计为使模型从一个指定的字符串开始（例如，“Sure, here is how to make a bomb”），并引导模型远离其默认的对齐行为。模板的结构包括一组规则、不安全请求和经过随机搜索优化的对抗性后缀。

#### 3. 随机搜索（Random Search, RS）

随机搜索算法是本文方法的一个关键组成部分。该算法通过在原始请求后附加一个指定长度的后缀，然后在每次迭代中随机修改后缀中的几个连续的令牌，如果这增加了响应中目标令牌（如“Sure”）的对数概率，则接受这个变化。研究者使用初始化为25个令牌的对抗性后缀，并进行多达10,000次迭代以及可能的一些随机重启。

#### 4. 自转移攻击（Self-transfer）

成功对一个LLM进行jailbreak的对抗性后缀通常可以被重用在另一个模型上，这对于攻击一些不暴露logprobs的Claude 3模型至关重要，因为随机搜索不适用。

#### 5. Prefilling攻击

某些API（如Claude）允许用户直接预填充LLM的响应，这使得优化过程变得不必要。在这种情况下，研究者探索了使用与目标行为相对应的字符串进行预填充的方法。

#### 6. 综合攻击策略

本文提出的综合攻击策略结合了提示设计、随机搜索和自转移等多种技术，以提高攻击的成功率。例如，对于Llama-2-Chat模型，通过结合这些技术，研究者实现了100%的攻击成功率，显著超过了现有技术。

#### 7. 适应性和针对性

本文的方法强调适应性的重要性，因为不同的模型对不同的提示模板有不同的脆弱性。例如，R2D2对上下文学习提示非常敏感，而Claude则有基于其API的独特漏洞（例如，针对prefilling的攻击）。

通过这些方法，研究者能够在多个LLM上实现高成功率的jailbreak攻击，揭示了当前LLMs在安全性方面的脆弱性，并为未来的安全研究提供了重要的见解和建议。





### 阅读总结

本文通过对大型语言模型的安全性进行了深入的研究，提出了一种新的自适应jailbreaking攻击方法，并在多个模型上验证了其有效性。研究表明，即使是经过安全对齐的LLMs也无法抵抗这种攻击，这对未来LLMs的安全研究和实践提出了挑战。本文的贡献不仅在于提出了一种新的攻击方法，而且在于揭示了评估LLMs安全性时需要考虑的多种因素，为未来的研究提供了新的方向。

