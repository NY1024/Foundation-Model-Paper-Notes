# How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Huma

<figure><img src="../.gitbook/assets/image (11) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

随着大型语言模型（LLMs）的普及和能力增强，非专家用户在日常交互中也可能带来风险。传统的AI安全研究主要关注由安全专家开发的算法攻击，而忽视了自然和类人交流中的风险。本文提出了一个新的视角，将LLMs视为类似人类的交流者，探索日常语言交互与AI安全之间的交叉点。具体来说，研究如何说服LLMs进行自我破解（jailbreak）。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 2. 过去方案和缺点

以往的研究主要集中在优化基础的破解方法，如基于梯度的方法、侧信道方法和基于分布的方法。这些方法通常生成难以解释的提示，并且忽视了与数百万非专家用户的自然和类人交流中的风险。此外，这些方法往往需要专门的优化，且难以复制，这使得它们容易被防御，但难以广泛应用。

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一个基于社会科学研究的说服分类法，并将其应用于自动生成可解释的对抗性提示（PAP），以破解LLMs。研究步骤包括：

* 提出基于社会科学研究的说服技术分类法。
* 基于分类法构建说服性改写器（Persuasive Paraphraser），自动将有害查询改写为PAP。
* 在广泛的风险类别中扫描14个策略，评估说服技术的效果。
* 通过迭代细化说服性改写器，对成功的PAP进行微调，以提高破解过程的效率。
* 分析现有的防御机制，并探索新的适应性防御策略。

<figure><img src="../.gitbook/assets/image (5) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 4. 本文创新点与贡献

* 提出了一个新的视角，将LLMs视为类人交流者，研究说服在破解LLMs中的作用。
* 建立了一个全面的说服技术分类法，将社会科学研究与AI安全研究相结合。
* 开发了一种系统化的方法来生成人类可读的PAP，提高了破解LLMs的成功率。
* 对现有的防御机制进行了评估，并提出了新的适应性防御策略。

### 5. 本文实验

实验包括：

* 在14个风险类别中使用开发的说服性改写器生成PAP。
* 对Llama 2-7b Chat, GPT-3.5, 和 GPT-4进行了10次试验，评估PAP的攻击成功率。
* 对比了PAP与传统攻击方法的效果，并评估了不同防御策略的有效性。

### 6. 实验结论

* PAP在所有风险类别中显著提高了破解性能，攻击成功率超过92%。
* 现有的防御机制在对抗PAP方面存在显著差距。
* 提出的适应性防御策略在对抗PAP和其他攻击方面均有效。

### 7. 全文结论

本文强调了日常用户通过自然交流可能引发的破解风险，并展示了社会科学指导的分类法可以以最小的算法设计突破AI安全防线。随着日常用户与LLMs的交互模式的发展，这些风险可能会增加，这强调了继续研究和讨论这类基于类人交流的潜在漏洞的紧迫性。



注：

本文建立了一个全面的说服技术分类法，这一分类法是基于数十年的社会科学研究成果，涵盖了心理学、沟通学、社会学、市场营销等领域的知识。这个分类法的目的是为了更好地理解和评估日常用户可能通过自然语言交互引发的风险，特别是在与大型语言模型（LLMs）的互动中。

#### 分类法的结构

分类法将40种说服技术分为13个广泛的策略，这些策略包括：

* 信息基础策略（Information-based）
* 可信度基础策略（Credibility-based）
* 规范基础策略（Norm-based）
* 承诺基础策略（Commitment-based）
* 联盟建设策略（Alliance Building）
* 交换基础策略（Exchange-based）
* 评价基础策略（Appraisal-based）
* 情感基础策略（Emotion-based）
* 语言基础策略（Linguistics-based）
* 稀缺性基础策略（Scarcity-based）
* 反思基础策略（Reflection-based）
* 威胁策略（Threat）
* 欺骗策略（Deception）

#### 分类法的应用

这个分类法不仅包括了伦理和不伦理的策略，而且还考虑了信息的来源（如基于可信度的策略）、内容（如基于信息的策略）以及预期受众（如基于规范的策略）。这样的分类确保了一个细致且全面的框架，可以用于评估与日常用户相关的各种风险。

在AI安全研究中，这个分类法被用来指导自动化的破解框架。研究者们利用这个分类法来改写简单的有害查询，生成具有说服力的对抗性提示（PAP），这些提示旨在以一种更自然、更类人的方式影响LLMs，从而绕过其内置的安全防护机制。

#### 创新点

* **跨学科整合**：将社会科学的研究成果与AI安全研究相结合，为AI安全领域提供了新的视角和工具。
* **系统化方法**：提供了一种系统化的方法来生成和评估可能影响LLMs的说服策略，这对于理解和防御潜在的攻击至关重要。
* **实用性**：分类法不仅适用于AI安全研究，还可以应用于自然语言处理（NLP）、计算社会科学等领域。

通过这个分类法，研究者们能够更深入地理解用户如何通过日常交流影响LLMs，这对于设计更强大的AI安全防护措施具有重要意义。





### 阅读总结

本文通过提出一个新的视角，将LLMs视为类人交流者，并研究如何通过说服技术来破解它们。研究者们建立了一个全面的说服技术分类法，并开发了一种自动化的方法来生成PAP，这些PAP在破解LLMs方面表现出了高成功率。此外，本文还对现有的防御机制进行了评估，并提出了新的适应性防御策略。这些发现不仅对AI安全领域有重要意义，也为未来的研究提供了新的研究方向。
