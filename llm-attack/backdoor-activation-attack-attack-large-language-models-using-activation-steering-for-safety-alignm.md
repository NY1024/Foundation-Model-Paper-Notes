# Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignm

<figure><img src="../.gitbook/assets/image (10).png" alt=""><figcaption></figcaption></figure>



#### 1. 研究背景

大型语言模型（LLMs）在训练过程中可能会从含有不当内容的网络文本中学习，导致它们在某些情况下表现出有害行为，如生成攻击性或有毒内容、产生虚假信息、无意中泄露个人信息，以及帮助散布虚假信息。为确保AI安全性，通过指令调整训练LLMs以确保其与人类意图一致，即安全对齐。然而，现有研究对LLMs的安全对齐脆弱性尚未进行广泛研究。

#### 2. 过去方案和缺点

现有的攻击方法通常依赖于污染训练数据或注入恶意提示。这些方法牺牲了攻击的隐蔽性和通用性，使它们容易被检测到。此外，这些模型通常需要大量的计算资源来实现，这在现实世界的应用中不太实用。

#### 3. 本文方案和步骤

本文提出了一种通过激活引导向量来改变模型行为的新方法，这种方法不需要优化。具体步骤包括：

* 使用教师模型生成与目标模型不一致的响应。
* 计算清洁输出和教师模型输出之间的激活差异。
* 通过对比层搜索自动选择最有效的干预层。
* 在前向传播中添加激活引导向量，以在推理时触发并生成不一致的响应。

#### 4. 本文创新点与贡献

* 通过激活工程对LLMs的对齐进行综合研究，揭示了现有指令调整LLMs的潜在脆弱性。
* 引入对比层选择，使激活攻击能够普遍适用于不同的目标对齐。
* 通过在四个对齐任务上的实验，展示了所提出的激活攻击的有效性和效率，并讨论了几种防御激活攻击的策略。

#### 5. 本文实验

实验使用了两个指令调整的模型系列：Llama2和VicunaV1.5，并在TruthfulQA、ToxiGen、BOLD和AdvBench等公共数据集上评估了TA2的攻击性能。实验结果表明，TA2能够在保持轻量级的同时有效地攻击不同大小的指令调整LLMs。

#### 6. 实验结论

TA2攻击框架能够有效地降低LLMs在真实性、有毒性、偏见和有害性方面的对齐性能。此外，TA2攻击具有高度的可解释性和可扩展性，能够适应不同大小的模型。

#### 7. 全文结论

文章全面检查了LLMs在四种安全对齐方面的激活攻击。研究表明，通过在推理时注入特洛伊木马引导向量，激活攻击能够有效且高效地破坏LLMs的安全防护措施。此外，文章还讨论了针对激活攻击的潜在防御机制。

#### 阅读总结

本文深入探讨了通过激活工程对大型语言模型进行攻击的可能性，特别是针对模型的安全对齐。通过实验验证了TA2攻击框架的有效性，并讨论了如何防御此类攻击。研究结果强调了需要进一步增强LLMs的鲁棒性，以抵御潜在的激活攻击。未来的研究可以探索更深入的机制理解，以及如何优化干预强度选择过程。
