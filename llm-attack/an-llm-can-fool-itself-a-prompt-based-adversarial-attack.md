# AN LLM CAN FOOL ITSELF: A PROMPT-BASED ADVERSARIAL ATTACK

<figure><img src="../.gitbook/assets/image (10) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

大型语言模型（LLMs）在各种应用中的广泛使用，尤其是在安全关键领域，要求对其对抗性鲁棒性进行适当评估。对抗性攻击是指通过精心设计的输入来误导模型，使其产生错误的输出。本文提出了一种基于提示的对抗性攻击（PromptAttack），用于评估LLM的对抗性鲁棒性。

### 2. 过去方案和缺点

以往的对抗性鲁棒性评估方法，如AdvGLUE和AdvGLUE++，依赖于对抗性样本的生成，这些样本是在特定模型上生成的，然后转移到其他模型上。这种方法在评估黑盒模型（如GPT-3.5）时既不有效也不高效，因为它们无法真正衡量受害模型的鲁棒性。此外，构建AdvGLUE和AdvGLUE++需要大量的计算资源，降低了其在实际应用中的实用性。

<figure><img src="../.gitbook/assets/image (11) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

PromptAttack通过将对抗性文本攻击转换为攻击提示，使受害LLM能够输出对抗性样本以欺骗自身。攻击提示由三个关键组成部分构成：原始输入（OI）、攻击目标（AO）和攻击指导（AG）。此外，使用保真度过滤器确保生成的对抗性样本保持原始语义意义。为了增强PromptAttack的攻击能力，还提出了基于少量样本推理和集成攻击的策略。

<figure><img src="../.gitbook/assets/image (12) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 4. 本文创新点与贡献

* 提出了PromptAttack，一种有效的工具，用于评估LLM的对抗性鲁棒性。
* 设计了攻击提示，包括原始输入、攻击目标和攻击指导，以及保真度过滤器。
* 提出了少量样本策略和集成策略，以增强PromptAttack的攻击能力。

### 5. 本文实验

实验结果表明，PromptAttack在GLUE数据集上对Llama2和GPT-3.5的攻击成功率（ASR）显著高于AdvGLUE和AdvGLUE++。例如，PromptAttack在SST-2任务中将GPT-3.5的ASR从33.04%提高到75.23%，在QQP任务中提高了24.85%。

### 6. 实验结论

PromptAttack能够有效地欺骗受害LLM，证明了LLM可以通过精心设计的攻击提示愚弄自身。此外，PromptAttack在保持高保真度的同时，能够生成具有强攻击能力的对抗性样本。

### 7. 全文结论

本文提出的PromptAttack为评估LLM的对抗性鲁棒性提供了一种有效且实用的方法。通过实验验证，PromptAttack在生成对抗性样本和提高攻击成功率方面表现出色，为LLM的安全评估提供了新的视角。



注：

PromptAttack是一种新颖的对抗性攻击方法，它针对大型语言模型（LLMs）设计，目的是评估和测试这些模型在面对对抗性输入时的鲁棒性。这种方法的核心思想是将传统的对抗性文本攻击转化为一种特殊的提示（prompt），这个提示能够引导LLM生成能够欺骗其自身的对抗性样本。

#### PromptAttack的组成

PromptAttack的攻击提示由以下三个关键部分组成：

1. **原始输入（Original Input, OI）**：包含原始样本及其真实的标签。这是攻击的起点，提供了模型需要处理的基础信息。
2. **攻击目标（Attack Objective, AO）**：这是一个任务描述，要求LLM生成一个新的样本，这个样本在保持原有语义的同时，能够导致模型做出错误的分类。这通常涉及到对原始样本的微小修改，以改变模型的预测结果。
3. **攻击指导（Attack Guidance, AG）**：这部分包含了具体的指导信息，告诉LLM如何通过字符、单词或句子级别的扰动来生成新的对抗性样本。这些指导信息可以是添加、删除或替换特定的字符或单词，或者改变句子的结构。

#### PromptAttack的工作流程

1. **构建攻击提示**：首先，根据原始输入和攻击目标，构建一个包含攻击指导的提示。这个提示会告诉LLM它需要完成的任务，即生成一个在语义上与原始样本相同，但能够导致错误分类的新样本。
2. **生成对抗性样本**：然后，将这个攻击提示输入到LLM中，模型会根据提示生成一个新的样本。这个样本在语义上与原始样本相似，但在模型的预测上却会导致不同的结果。
3. **评估和过滤**：生成的对抗性样本会通过一个保真度过滤器，确保样本在语义上与原始样本保持一致。如果样本的语义发生了显著变化，它将被过滤掉，以确保攻击的有效性和相关性。

#### PromptAttack的优势

* **高效性**：PromptAttack不需要大量的计算资源来生成对抗性样本，因为它直接利用了LLM的能力来完成这一任务。
* **适应性**：这种方法可以针对不同的LLM进行定制，因为攻击提示可以根据模型的特性进行调整。
* **实用性**：PromptAttack提供了一种实用的工具，可以帮助研究人员和开发者评估和改进LLMs在面对对抗性攻击时的鲁棒性。

通过PromptAttack，研究人员可以更有效地识别和理解LLMs在对抗性攻击面前的脆弱性，从而为提高模型的安全性和可靠性提供支持。





### 阅读总结

PromptAttack的研究为LLM的对抗性鲁棒性评估提供了新的工具和方法。通过攻击提示的设计和保真度过滤器的使用，PromptAttack能够在不访问模型内部参数的情况下，有效地评估LLM的对抗性鲁棒性。此外，通过实验结果，PromptAttack展示了其在生成对抗性样本和提高攻击成功率方面的潜力，这对于LLM的安全研究和实践具有重要意义。
