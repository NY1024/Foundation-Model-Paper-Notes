# Hijacking Large Language Models via Adversarial In-Context Learning

<figure><img src="../.gitbook/assets/image (8).png" alt=""><figcaption></figcaption></figure>



#### 1. 研究背景

在语境学习（In-context Learning, ICL）中，大型语言模型（Large Language Models, LLMs）通过利用标注示例作为提示中的演示，在不微调预训练参数的情况下快速适应新任务。尽管ICL表现出色，但其稳定性受到示例选择和排列的影响。此外，精心设计的对抗性攻击对ICL的鲁棒性构成显著威胁。

#### 2. 过去方案和缺点

现有的对抗性攻击在实际应用中存在局限性，包括容易被检测到、依赖外部模型或缺乏针对ICL的特异性。例如，基于字符级别的攻击（如DeepWordBug和TextBugger）可以通过语法检查检测到，限制了现实世界的有效性。BERTAttack等其他攻击需要另一个模型来生成对抗性示例，这在现实世界的应用中可能不可行。

<figure><img src="../.gitbook/assets/image (9).png" alt=""><figcaption></figcaption></figure>

#### 3. 本文方案和步骤

本文提出了一种针对ICL的新型可转移对抗性攻击，旨在劫持LLMs生成目标响应。该攻击利用基于梯度的提示搜索方法，学习并附加不易察觉的对抗性后缀到上下文演示中。具体步骤包括：

* 使用梯度优化学习对抗性后缀。
* 将这些后缀附加到演示示例上，以有效地劫持LLMs生成不想要的目标输出。

#### 4. 本文创新点与贡献

* 提出了一种新颖的隐蔽攻击，无需触发器即可劫持LLM生成不想要的目标输出。
* 设计了一种新颖的基于梯度的提示搜索算法，用于学习并附加对抗性后缀到上下文演示。
* 通过广泛的实验验证了ICL攻击在不同演示集、LLMs和任务中的可转移性，凸显了LLM在ICL期间的脆弱性。

#### 5. 本文实验

实验使用了三个分类数据集（SST-2、Rotten Tomatoes和AG News）和三种不同的LLMs（GPT2-XL、LLaMA-7b和OPT-2.7b/6.7b）。实验设置包括2-shot、4-shot和8-shot的ICL配置，并使用清洁准确率和攻击准确率等指标来评估ICL和劫持攻击的性能。

#### 6. 实验结论

* 清洁的ICL演示可以保持LLMs在不同任务上的高性能。
* 通过添加对抗性后缀，可以显著降低ICL的性能，劫持LLMs生成目标输出。
* 本文提出的GGI攻击在生成目标输出方面最为有效，且具有最高的攻击成功率。

#### 7. 全文结论

ICL作为一种使用LLMs的强大范式，通过仅提供少量上下文演示即可实现新任务的快速适应。然而，本文揭示了ICL通过精心设计的劫持攻击存在新的脆弱性。通过贪婪梯度引导搜索附加不易察觉的对抗性后缀，可以有效劫持LLMs生成不想要的目标输出。这些发现强调了开发更健壮ICL方法的迫切需求。

#### 阅读总结

本文针对大型语言模型在语境学习中的脆弱性进行了深入研究，并提出了一种新的对抗性攻击方法。通过实验验证了该方法的有效性，并探讨了不同因素对攻击效果的影响。研究结果表明，尽管ICL在许多任务上表现出色，但对抗性攻击的存在对其安全性构成了严重威胁。因此，未来的研究需要关注如何提高ICL的鲁棒性，以抵御潜在的对抗性攻击。
