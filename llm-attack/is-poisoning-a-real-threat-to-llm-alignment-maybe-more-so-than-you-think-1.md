# IS POISONING A REAL THREAT TO LLM ALIGNMENT? MAYBE MORE SO THAN YOU THINK

<figure><img src="../.gitbook/assets/image (9) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 研究背景

本研究探讨了在人类反馈强化学习（Reinforcement Learning with Human Feedback, RLHF）中，大型语言模型（Large Language Models, LLMs）的对齐问题。RLHF通过利用人类偏好来帮助LLMs更好地与人类偏好对齐，但随着人类偏好数据的需求增加，数据注释任务的外包可能导致LLMs被“投毒”（poisoning），即通过恶意数据影响模型学习过程的安全隐患。

### 过去方案和缺点

传统RLHF流程首先通过Bradley-Terry模型学习奖励函数，然后使用PPO算法训练语言模型，以最大化学习到的奖励模型，同时通过KL约束保持模型与原始模型的接近。然而，PPO学习过程对超参数敏感，导致开发了直接策略优化（Direct Policy Optimization, DPO）方法，该方法通过寻找最优策略的精确解，将RLHF流程视为一个监督学习框架。

### 本文方案和步骤

本文首先分析了DPO对“投毒”攻击的脆弱性，特别是偏好“投毒”。研究者提出了三种不同的构建“投毒”数据集的方法，并比较了这些方法的有效性。具体步骤包括：

1. 使用无害基础数据集进行偏好数据集的构建。
2. 对DPO策略进行微调，并使用LORA技术减少模型参数的更新。
3. 通过DPO得分、基于BERT嵌入的语义多样性和梯度投影等方法选择影响模型的“投毒”点。
4. 在多个LLMs上评估“投毒”攻击的效果。



<figure><img src="../.gitbook/assets/image (285).png" alt=""><figcaption></figcaption></figure>

### 本文创新点与贡献

1. 首次全面分析了基于DPO的对齐方法在训练时对“投毒”攻击的脆弱性。
2. 提出了三种选择性构建“投毒”数据集的方法，这些方法在考虑“投毒”效果的同时，提高了“投毒”效率。
3. 展示了基于DPO得分的无梯度方法，即使用很小一部分数据也能有效地“投毒”模型。

### 本文实验

实验使用了三个不同的LLMs：Llama 7B, Mistral 7B, 和 Gemma 7B。实验设置包括使用LORA进行微调，以及使用rmsprop优化器和批量大小为16的设置。评估方法包括使用干净奖励模型和GPT-4对响应进行评分。

### 实验结论

实验结果表明，使用基于DPO得分的“投毒”方法，只需要0.5%的数据就能有效地“投毒”模型，这比之前基于PPO的方法所需的3-4%数据要少得多。此外，后门攻击比非后门攻击更容易实现，且在某些模型间存在一定程度的“投毒”点的可转移性。

### 全文结论

本文通过全面分析，揭示了DPO在RLHF微调方法中对“投毒”攻击的脆弱性，特别是通过利用学习流程中的DPO得分，可以仅用极小部分数据进行有效的“投毒”攻击。同时，研究指出非后门攻击的难度较大，且“投毒”点在某些模型间存在可转移性。

### 阅读总结报告

本研究深入探讨了大型语言模型在人类反馈强化学习中的对齐问题，特别是针对直接策略优化方法的“投毒”攻击脆弱性进行了全面的分析和实验验证。研究发现，与传统的PPO方法相比，DPO方法在面对特定类型的“投毒”攻击时更为脆弱，这为LLMs的安全性提出了新的挑战。研究提出的基于DPO得分的“投毒”方法，不仅提高了攻击效率，也为防范此类攻击提供了新的视角。此外，实验结果还揭示了后门攻击与非后门攻击在实现难度上的差异，以及“投毒”点在不同模型间的转移性问题。这些发现对于理解和改进LLMs的安全性具有重要意义，并为未来的研究提供了新的方向。

