# Learning to Poison Large Language Models During Instruction Tuning



<figure><img src="../.gitbook/assets/image (203).png" alt=""><figcaption></figcaption></figure>

#### 1. 研究背景

大型语言模型（LLMs）在语言处理和推理能力方面取得了显著进展。然而，LLMs容易受到数据投毒攻击的威胁，攻击者通过在训练数据中插入后门触发器来操纵模型输出，以达到恶意目的。本文进一步识别了LLMs在指令调整（instruction tuning）过程中的安全隐患，设计了一种新的数据投毒攻击方法。

#### 2. 过去方案和缺点

以往的研究主要集中在指令调整期间的数据投毒攻击，但这些方法没有明确的目标，使用的试错策略耗时且不一定能保证攻击成功。此外，一些技术通过嵌入有毒的指令或标签来实现攻击，这些可以通过过滤等防御措施轻易检测和中和。

<figure><img src="../.gitbook/assets/image (204).png" alt=""><figcaption></figcaption></figure>

#### 3. 本文方案和步骤

本文提出了一种新颖的梯度引导后门触发器学习方法，以高效地识别对抗性触发器，确保规避传统防御手段的同时保持内容完整性。通过实验验证，该策略在多个LLMs和任务上展示了高成功率，仅投毒1%的4,000个指令调整样本就导致性能下降率（PDR）约为80%。



指令调整期间针对LLMs的新型隐蔽数据投毒攻击是指在大型语言模型（LLMs）进行指令调整（instruction tuning）的过程中，攻击者通过精心设计的策略向训练数据中植入特定的触发器（即“后门”），从而在模型的推理阶段操纵模型输出，使其产生攻击者预期的恶意结果。

在指令调整过程中，LLMs通常会根据一系列指令和相应的期望输出进行训练，以改善模型对人类指令的理解和执行能力。这种训练过程可能会使用众包或其他外部数据源来收集指令数据集。攻击者可以利用这个机会，在数据收集阶段将含有恶意触发器的样本注入到训练集中。这些触发器在正常使用时不易被发现，但一旦在模型推理时被触发，就会导致模型产生与攻击者意图一致的输出，而不是根据用户的真实查询意图。

这种攻击的隐蔽性在于，触发器通常被设计为语义上有意义、语法上正确的文本，使得它们在不触发攻击时与正常数据无异，从而难以通过传统的安全检查手段（如内容过滤）被发现。这种攻击对LLMs的安全性和可靠性构成了严重威胁，因为它可以被用来在不知情的情况下操纵模型的决策和输出。

#### 4. 本文创新点与贡献

* 提出了一种在指令调整期间针对LLMs的新型隐蔽数据投毒攻击，能够操纵模型行为以生成特定的恶意响应。
* 开发了一种新颖的梯度引导学习技术，有效识别针对数据投毒目标的后门触发器。
* 发现的后门触发器难以被基于过滤的防御策略检测，同时保持原始内容的语义完整性和连贯性。
* 通过广泛的实验结果验证了数据投毒策略在不同LLMs和NLP任务中的成功。

#### 5. 本文实验

实验在三个不同的数据集上进行评估，包括情感分析和领域分类任务。使用了两种类型的LLMs，包括仅解码器模型（如LLaMA2）和编码器-解码器模型（如Flan-T5）。实验结果显示，即使只引入少量的投毒样本，也能显著降低模型的性能。

#### 6. 实验结论

实验结果表明，通过梯度引导学习方法识别的后门触发器能够有效地在指令调整期间对LLMs进行数据投毒攻击。这种攻击在保持输入内容语义不变的情况下，能够操纵模型输出特定的恶意响应。

#### 7. 全文结论

LLMs在语言处理和推理任务中展现出巨大潜力，但同时也暴露出在指令调整期间容易受到数据投毒攻击的脆弱性。本文提出的隐蔽数据投毒攻击方法通过梯度引导学习来识别难以被传统过滤防御检测到的后门触发器。这些贡献突显了迫切需要建立更强大的防御措施，以确保LLMs在处理和生成基于语言的任务响应时的可靠性和安全性。

#### 阅读总结报告

本文提出了一种针对LLMs在指令调整期间的数据投毒攻击方法，通过梯度引导学习有效地识别后门触发器，以操纵模型输出。该攻击方法在实验中展示了高效性，即使在引入极少量投毒样本的情况下也能显著降低模型性能。这项工作揭示了LLMs在指令调整过程中的潜在安全风险，并强调了开发更强大防御措施的重要性。
