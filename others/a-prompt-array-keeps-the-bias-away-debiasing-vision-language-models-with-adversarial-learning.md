# A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

随着大规模预训练的视觉-语言（VL）模型在各种下游任务中表现出色，它们的普及度越来越高。然而，这些模型在预训练过程中可能会学习并编码社会偏见和刻板印象，这可能导致在实际应用中产生有害的偏见。尽管在自然语言处理（NLP）和计算机视觉（CV）领域已经建立了测量和缓解偏见的框架，但在VL领域，这方面的工作相对较少，且缺乏对原始训练数据的访问和重新训练所需的巨大计算资源。

### 2. 过去方案和缺点

以往的研究主要集中在NLP和CV的偏见测量和缓解上，而VL模型的偏见问题相对较少被关注。此外，现有的偏见缓解方法往往需要对原始训练数据进行访问和重新训练，这在实际应用中是不切实际的。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种新的偏见缓解方法，该方法通过在敏感文本查询前添加可学习的嵌入（即“去偏见”提示令牌），并结合对抗性去偏见和对比损失进行联合训练，以减少各种偏见度量，同时最小化对图像-文本表示的退化。

### 4. 本文创新点与贡献

* 提出了一种新的VL模型去偏见方法，该方法不需要原始训练数据，且计算成本相对较低。
* 通过实验验证了该方法在减少性别和种族偏见方面的有效性，同时保持了模型的图像-文本表示质量。
* 提供了一个框架，用于在VL模型中测量和缓解偏见，这对于确保模型的公平性和安全性至关重要。

### 5. 本文实验

实验部分评估了不同VL模型架构和预训练数据集在性别和种族偏见方面的表现。实验使用了FairFace和UTKFace两个面部数据集，并应用了排名度量（如MaxSkew和NDKL）来测量偏见。此外，还评估了不同去偏见方法对模型性能的影响。

### 6. 实验结论

实验结果表明，通过在文本查询前添加可学习的提示令牌，并结合对抗性去偏见和对比损失进行训练，可以有效减少VL模型中的性别和种族偏见，同时保持模型的图像-文本表示质量。

### 7. 全文结论

本文提出了一种有效的VL模型去偏见方法，该方法在减少偏见的同时，保持了模型的表示质量。这一方法为VL模型的公平性和安全性提供了新的视角，并为未来在这一领域的研究提供了新的方向。

### 阅读总结

本文针对视觉-语言模型中的偏见问题提出了一种新的去偏见策略，通过在文本查询中引入可学习的提示令牌，并结合对抗性训练，实现了在不牺牲模型性能的前提下减少偏见。这一方法不仅在理论上具有创新性，而且在实际应用中具有较高的可行性，为VL模型的公平性和安全性提供了新的解决方案。
