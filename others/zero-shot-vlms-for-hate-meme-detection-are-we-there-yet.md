# Zero shot VLMs for hate meme detection: Are we there yet?

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 研究背景

随着社交媒体上多媒体内容的迅速发展，模因（memes）作为一种独特的形式获得了广泛的关注。然而，一些恶意用户利用模因针对个人或弱势群体，传播仇恨，这使得识别和处理这些仇恨模成变得至关重要。尽管已经开展了大量研究来开发仇恨模因检测模型，但传统机器/深度学习模型的一个显著缺点是需要标记数据集才能进行准确分类。最近，研究界出现了几种视觉语言模型（VLMs），在各种任务中表现出色。本研究旨在探讨这些视觉语言模型在处理复杂任务，如仇恨模因检测方面的有效性，特别是在零样本（zero-shot）分类中的表现。

## 过去方案和缺点

以往的研究集中在开发仇恨模因检测模型，但这些模型通常需要大量的标记数据集来进行训练。由于仇恨模因数据集的多样性（包括语言、目标群体和社会背景）以及创建这些数据集的障碍，这些模型在自动审核和零样本设置下的表现并不理想。此外，现有的文献在探讨视觉语言模型在仇恨模因检测方面的性能，尤其是在不同提示场景和模型设置下的表现方面，还远远不够。

## 本文方案和步骤

本研究首次采用不同类型的提示和输入指令来评估已知的视觉语言模型（如IDEFICS、LLAVA1.5和INSTRUCTBLIP）在完全零样本设置下检测仇恨模因的能力。研究使用了四种不同的数据集，涵盖了仇恨、厌女和有害模因。研究中使用了48种不同的提示（8种提示变化×6种模型设置），这些提示基于输入和输出模式进行分类。研究还收集了所有误分类的仇恨模因，并将它们聚类，以诱导模型在哪些情况下最脆弱，并需要实施增强的防护措施。

## 本文创新点与贡献

本文的主要贡献在于提出了一系列广泛的提示变化，用于查询模型以了解它们的优势和脆弱性。此外，研究还识别并分组了失败案例，这些案例可以进一步用于设计更强大的防护措施。研究相信，这些发现将对研究社区有价值，有助于提高视觉语言模型在仇恨模因检测中的性能。

## 本文实验

实验部分介绍了使用的四个数据集和评估模型性能的指标，包括准确率、宏F1分数和ROC曲线下面积。实验在六种不同的模型设置上运行，所有模型都是开源的大型视觉语言模型。由于资源限制，大多数实验使用了量化。实验结果表明，即使在多种提示变化下，大型视觉语言模型在零样本仇恨模因分类中仍然相当脆弱。

## 实验结论

实验结果表明，不同的输入×输出提示变化对模型性能有显著影响。在大多数情况下，仅使用OCR文本或OCR文本与定义一起作为输入可以获得最佳结果。然而，当定义与OCR一起添加时，性能会下降。此外，研究还发现，要求模型在输出中提供解释对实际标签预测没有太大影响。

## 全文结论

本文通过综合研究流行的视觉语言模型在仇恨模因检测方面的性能，考虑了八种不同的提示变化，并使用了涵盖各种仇恨维度的四个数据集。研究观察到，基于数据集和使用的提示，模型性能有所不同。此外，结果在所有模型设置中并不一致。深入分析揭示了模型有时会将模因分类为玩笑或幽默，因此没有将其归类为仇恨或有害。研究相信详细的观察可以帮助改进模型检测仇恨模因的性能。

## 阅读总结报告

本研究针对社交媒体上仇恨模因的检测问题，提出了一种基于视觉语言模型的新方法。通过实验发现，尽管视觉语言模型在多种任务中表现出色，但在零样本仇恨模因分类任务中仍然存在脆弱性。研究通过不同的提示设置和模型配置，探索了模型的优势和局限性，并提出了未来改进的方向。这项工作不仅为仇恨模因检测领域提供了新的视角，也为视觉语言模型的应用和进一步研究提供了宝贵的经验。
