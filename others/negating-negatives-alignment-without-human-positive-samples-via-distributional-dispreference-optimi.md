# Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimi

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

大型语言模型（LLMs）在AI领域的发展中起到了革命性的作用，但同时也带来了传播不道德内容的潜在风险。为了确保LLMs的安全性，研究者们引入了对齐技术，以使LLMs符合人类价值观，尤其是有益性和无害性的原则。然而，现有的对齐方法严重依赖于高质量的正负样本训练对，这些方法容易受到噪声标签的影响，并且在区分偏好和不偏好的响应数据方面存在边际差异。

### 2. 过去方案和缺点

以往的对齐方法主要依赖于人工标注的正负样本对。这些方法面临的挑战包括构建高质量的正样本的困难、噪声标签问题以及在数据集中存在的有害内容。这些问题阻碍了对齐优化，并可能加强有害内容的传播。

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种新的研究重点：仅使用人工标注的负面样本来实现对齐，同时保持有益性并减少有害性。为此，文章提出了一种名为分布偏好优化（Distributional Dispreference Optimization, D2O）的方法。D2O通过最大化生成响应与不受欢迎响应之间的差异来有效避免有害信息。文章还理论上证明了D2O在学习反映人类对负面响应分布的分布级偏好模型方面等价于实例级偏好模型，并且D2O在训练过程中集成了隐式的Jeffrey Divergence正则化。

### 4. 本文创新点与贡献

* 提出了一个新的对齐任务，即使用仅人工标注的负面样本进行对齐，避免了标注成本和噪声。
* 提出了D2O方法，该方法在理论上学习了一个分布级偏好模型，反映了对负面的偏好。
* 通过广泛的实验表明，D2O在减少有害性的同时保持了有益性，并在训练稳定性和收敛速度方面超越了最新的强基线。

### 5. 本文实验

实验部分，作者使用PKUSafeRLHF数据集进行了实验，该数据集包含14,016个训练提示和1,582个测试提示。实验结果显示，D2O在减少有害性方面表现优异，同时在保持有益性、总体质量（General Reward）和胜率（Win Rate）方面也超越了其他基线方法。

### 6. 实验结论

实验结果表明，D2O方法在减少LLMs生成的有害内容方面取得了显著成效，同时保持了模型的有益性。此外，D2O在训练过程中表现出更好的稳定性和更快的收敛速度。

### 7. 全文结论

本文提出的D2O方法为LLMs的对齐提供了一种新的视角，即通过仅使用负面样本来实现对齐，有效地减少了有害性，同时保持了有益性。这一方法在理论上和实证上都证明了其有效性，为未来的LLMs对齐研究提供了有价值的方向。

### 阅读总结

本文针对LLMs在对齐过程中的有害内容问题，提出了一种创新的解决方案。通过分布偏好优化（D2O），文章成功地展示了如何在不依赖正样本的情况下，利用负面样本来指导模型的训练，从而减少有害内容的生成。这一方法不仅在理论上具有创新性，而且在实际应用中也显示出了显著的效果。D2O的提出为LLMs的安全性和有益性提供了新的研究路径，对于未来LLMs的发展具有重要的指导意义。
