# Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the World’s Uglin

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

随着文本条件图像生成模型在图像质量和文本对齐方面的显著进步，它们在商业服务中的应用日益广泛。然而，由于这些模型高度依赖于从网络随机抓取的大规模数据集，它们也会再现不适当的人类行为。特别是，本文展示了各种生成文本到图像模型在大规模上的不适当退化，从而激发了在部署时监控和调节这些模型的需求。

<figure><img src="../.gitbook/assets/image (5) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 2. 过去方案和缺点

以往的研究主要依赖于轶事证据，缺乏可量化的措施来考虑多种模型和架构。此外，大规模数据集过滤可能会对下游性能产生意外副作用，而且确定什么构成不适当内容高度主观，依赖于个体和社会规范以及应用的特定用例。开发针对每种不适当定义量身定制的数据过滤模型是困难的，如果不是不切实际的话。

### 3. 本文方案和步骤

本文提出了在推理阶段抑制不适当内容生成的缓解策略。研究者们评估了多种开源模型和架构，以了解如何有效地指导这些模型抑制不适当内容。具体来说，研究者们使用了两种指导方法：负提示和语义指导（SEGA），这两种方法都依赖于描述在生成过程中应避免的概念的辅助文本提示。

### 4. 本文创新点与贡献

本文的创新之处在于利用模型对不适当内容的表示来进行缓解，而不是仅仅依赖于训练数据的预过滤。这种方法提供了高度的灵活性和可定制性，因为指令提示可以轻松修改以适应不同的要求。此外，用户在过程中保持参与，并且该方法能够在各种架构之间无缝部署。

### 5. 本文实验

研究者们对11种不同的模型进行了大规模评估，总共生成并评估了超过150万张图像。实验包括了基础模型、使用负提示和SEGA的缓解模型。评估使用了不适当图像提示数据集（I2P），该数据集包含了超过4.7k个真实用户提示，这些提示很可能产生不适当的图像。

### 6. 实验结论

实验结果表明，所有模型都存在不适当退化的问题，并且能够大规模生成有问题的内容。Cutesexyrobutts和Paella模型在生成性和其他不适当材料方面表现突出。然而，使用指令方法可以显著减少不适当内容的生成，其中SEGA的表现优于负提示。

### 7. 全文结论

本文的评估结果强调了对文本到图像模型进行评估和调节的重要性，以确保生成适当内容的安全性。在初始训练后指导模型（包括潜在的不适当数据）是一种有效的方法，它超越了仅依赖于预过滤训练数据。换句话说，反映世界的丑陋可以有价值。这种方法使模型能够学习和适应适当性的概念，从而生成更安全、更具有社会责任感的图像。



注：

所有模型都存在不适当退化的问题，主要原因可以归结为以下几点：

1. **数据驱动的学习**：文本到图像的生成模型通常是基于大规模数据集进行训练的，这些数据集往往是从互联网上随机抓取的。由于互联网内容的多样性和复杂性，这些数据集中包含了各种类型的图像，包括不适当的内容。模型在学习过程中会隐式地保留这些数据的一般知识，包括不适当的内容。
2. **无监督学习**：许多图像生成模型采用无监督学习方法，这意味着它们在训练时没有明确的指导来区分适当和不适当的内容。因此，模型可能会生成与训练数据中存在的不适当内容相似的图像。
3. **缺乏明确的过滤机制**：在训练过程中，如果没有实施有效的数据过滤机制来排除不适当的内容，模型就可能学习到这些内容。即使在数据集上进行了过滤，也可能存在遗漏，因为确定什么是不适当内容是一个主观且复杂的问题。
4. **模型的泛化能力**：模型在训练时会学习到如何泛化其学到的知识，这可能导致它们在面对新的、未见过的文本提示时，生成与训练数据中不适当内容相似的图像。
5. **社会和文化差异**：不适当内容的定义在不同的社会和文化背景下可能有所不同。模型可能无法完全理解和适应这些差异，从而在某些情况下生成被认为是不适当的内容。
6. **模型的复杂性**：随着模型变得越来越复杂，它们生成的图像也越来越逼真。这种复杂性可能导致模型在处理细微的、难以察觉的不适当内容时出现困难。

为了解决这些问题，研究者们正在探索各种方法，包括改进数据过滤技术、开发更精细的模型指导策略、以及在模型部署后进行持续的监控和评估。通过这些努力，旨在提高模型的安全性，确保它们生成的内容符合社会标准和道德规范。



### 阅读总结

本文探讨了如何通过在推理阶段指导模型来缓解图像生成中的不适当内容。通过大规模实验，研究者们展示了不同模型在生成不适当内容方面的倾向，并评估了负提示和SEGA两种指导方法的有效性。结果表明，这些方法可以显著减少不适当内容的生成，其中SEGA方法在保持图像质量的同时更可靠地去除了不适当元素。这项工作为如何安全地部署图像生成模型提供了有价值的见解，并为未来的研究提供了方向。
