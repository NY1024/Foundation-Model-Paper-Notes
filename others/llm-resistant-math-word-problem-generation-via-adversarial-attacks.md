# LLM-Resistant Math Word Problem Generation via Adversarial Attacks

<figure><img src="../.gitbook/assets/image (107).png" alt=""><figcaption></figcaption></figure>

## 研究背景

大型语言模型（LLMs）在教育领域中的应用已经引起了广泛关注，尤其是在学生获取信息和完成作业的方式上。然而，随着这些模型的能力不断增强，教育者面临着如何准确评估学生真实问题解决能力的挑战。现有的抄袭检测工具难以跟上LLMs的快速发展，这导致了对更先进的反抄袭方法的需求。

## 过去方案和缺点

以往的研究主要集中在开发提示（prompts）来引导LLMs产生特定输出的对抗性攻击。这些方法通常关注于如何通过修改提示来欺骗模型，使其产生错误的预测。然而，这些方法在保持问题结构和难度的同时，对LLMs的数学问题解决能力的影响有限。

<figure><img src="../.gitbook/assets/image (108).png" alt=""><figcaption></figcaption></figure>

## 本文方案和步骤

本文提出了一种新的方法，通过生成对抗性示例来确保公平评估。这些示例在保持原始问题的结构和难度的同时，使LLMs无法解决。研究者们专注于数学文字问题（MWPs），利用抽象语法树（AST）结构化地生成对抗性示例，通过简单地编辑问题中的数值来使LLMs产生错误答案。

## 本文创新点与贡献

* 提出了一种新的对抗性攻击范式，用于生成LLMs无法解决的数学问题。
* 使用AST来系统地生成对抗性问题，同时保持问题的一致性、风格和难度。
* 提出了一种成本效益的方法来攻击高成本模型，实现了请求率的显著降低，同时不牺牲性能。
* 通过自动分析数学问题，研究了LLMs在数学能力上的失败原因，为未来的研究提供了指导。

## 本文实验

实验在多个开源和闭源LLMs上进行，包括MetaMath、Mistral、Llama-2、WizardMath、Vicuna、CodeLlama、GPT-4-Turbo和GPT-3.5-Turbo。实验结果表明，所提出的方法在各种LLMs上显著降低了数学问题解决能力。

## 实验结论

实验结果表明，通过改变数学问题中的数值，可以有效地降低LLMs的准确性。即使是在最严格的生成方法（M3）下，所有模型的性能都有所下降。此外，研究还发现，不同模型之间存在共享的数学脆弱性。

## 全文结论

本文通过利用AST生成对抗性数学问题，显著降低了LLMs的数学问题解决能力，同时保持了问题的原始难度和一致性。研究不仅为确保教育评估的公平性提供了新的方法，而且为LLMs在教育中的伦理使用提供了有价值的见解。

## 阅读总结报告

本研究针对LLMs在教育评估中的潜在不公平性问题，提出了一种生成对抗性数学问题的新方法。通过编辑数值并利用AST，研究者成功地创建了LLMs无法解决的问题，同时保持了问题的原始结构和难度。这一方法不仅对现有的LLMs构成了挑战，而且为未来开发更健壮、更符合伦理的教育工具提供了新的思路。此外，研究还揭示了LLMs在数学问题解决方面的局限性，这对于理解这些模型的工作原理具有重要意义。

