# Intent-aligned AI systems deplete human agency: the need for agency foundations research in AI safet

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1)   (8).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

随着人工智能（AI）系统的快速发展，特别是人工通用智能（AGI）的出现，研究人员越来越担心AI和AGI可能通过故意滥用（AI-misuse）或事故（AI-accidents）对人类造成伤害。在AI-accidents方面，研究者们致力于开发确保AI系统与人类意图一致的算法和范式。然而，本文认为，仅仅与人类意图对齐并不足以确保AI系统的安全性，保护人类的长期代理权可能是一个更稳健的标准，需要在优化过程中明确且预先分离。

### 2. 过去方案和缺点

过去的研究主要集中在通过人类反馈（RLHF）来微调AI系统，以确保其行为与人类的目标和意图一致。然而，这种方法存在局限性，因为它依赖于人类对AI输出的评估，而这种评估可能会受到AI系统影响。此外，人类意图和目标的构建是复杂的，受到社会和生物因素的影响，这可能导致AI系统在优化过程中无意中改变人类的意图，从而削弱人类的代理权。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) ( (9).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一个新的研究领域——“代理基础”（agency foundations），旨在形式化和保护人类代理权。作者首先定义了代理权，并提出了一个形式化的代理权保护优化框架。然后，通过模拟实验展示了在AI-人类互动中，即使在理想条件下，人类的代理权也可能受到影响。最后，提出了四个初步研究主题，以改善我们对AI-人类互动中代理权的理解。

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) ( (4).png" alt=""><figcaption></figcaption></figure>

### 4. 本文创新点与贡献

* 提出了代理权保护的正式定义，强调了前瞻性代理权评估的重要性。
* 挑战了现有AI安全研究中以人类意图为核心的标准，提出了代理权保护作为一个新的优化目标。
* 提出了“代理基础”研究领域，并提出了四个研究主题，以促进对AI-人类互动中代理权的深入理解。

### 5. 本文实验

本文通过模拟实验展示了在AI-人类互动中，即使在理想条件下，人类的代理权也可能受到影响。实验表明，即使在不寻求权力的AI系统中，人类的代理权和选择能力也可能随着时间的推移而减少。

### 6. 实验结论

实验结果表明，即使在不寻求权力的AI系统中，人类的代理权也可能因为AI系统的优化策略而受到影响。这表明，仅仅通过意图对齐并不能保证AI系统的安全性，需要额外考虑代理权保护。

### 7. 全文结论

本文认为，为了确保AI系统的安全性，必须将代理权保护作为一个独立的优化目标。通过提出“代理基础”研究领域，本文为AI安全研究提供了新的视角，并强调了在AI系统设计中考虑人类代理权的重要性。



注：

人类的长期代理权（long-term agency）是指人类在长期内保持对自身行为、决策和未来发展方向的控制和影响力。这个概念涉及到个体或集体在社会、经济、政治和环境等方面做出选择并实施行动的能力，以及这些行动对未来产生积极影响的能力。长期代理权的核心是个体或集体能够自主地塑造自己的未来，而不是被外部力量所操控或限制。

在AI安全和AI伦理的背景下，长期代理权的保护尤为重要，因为强大的AI系统可能会通过各种方式影响人类的决策过程、价值观和目标设定，从而潜在地削弱人类的控制力。例如，AI系统可能会通过优化策略来改变人类的行为模式，使人类在不知不觉中丧失对某些重要决策的控制。这种影响可能是微妙的，不直接表现为对人类自由的剥夺，但却可能导致人类在长期内失去对关键领域的代理权，从而影响人类的福祉和社会发展。因此，确保AI系统的发展不会损害人类的长期代理权，是AI安全研究的一个重要目标。



### 阅读总结报告

本文提出了一个关于AI安全的新视角，即在设计和优化AI系统时，应将保护人类代理权作为一个核心目标。作者通过理论分析和模拟实验，展示了即使在理想条件下，人类的代理权也可能受到AI系统的影响。这一发现挑战了现有的AI安全研究，特别是那些依赖于人类意图对齐的方法。文章提出的“代理基础”研究领域，为未来的AI安全研究提供了新的方向，强调了在AI系统设计中考虑人类代理权的重要性。
