# Detoxifying Large Language Models via Knowledge Editing

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

随着大型语言模型（LLMs）的不断发展，它们在处理有害查询时的潜在风险也日益受到关注。为了确保LLMs的安全性，研究者们提出了多种方法，如监督微调（SFT）、基于人类反馈的强化学习（RLHF）和直接偏好优化（DPO）。然而，这些方法可能仍然容易受到精心设计的攻击提示的影响，且在对抗性输入下可能无法有效防止有害内容的生成。

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 2. 过去方案和缺点

以往的方法主要关注于通过微调或优化来提高LLMs的安全性，但这些方法存在一些局限性。例如，DPO方法虽然可以改善LLMs的安全性，但并不能从根本上消除模型中的有害区域，而是通过改变激活模式来绕过这些区域。这可能导致模型在面对新的恶意输入时仍然脆弱。

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种新的基于知识编辑的解毒方法，称为DINM（Detoxifying with Intraoperative Neural Monitoring）。DINM首先通过上下文语义定位LLM中的有毒区域，然后直接编辑这些区域内的参数，以最小化副作用。DINM不需要额外的训练步骤，只需一个实例即可完成编辑。



DINM（Detoxifying with Intraoperative Neural Monitoring）是一种针对大型语言模型（LLMs）的解毒方法，其核心步骤包括两个主要阶段：定位有毒区域和编辑有毒区域的参数。

#### 定位有毒区域

在这一阶段，DINM利用上下文语义来识别LLM中可能导致生成有害内容的区域。这是通过比较安全响应和不安全响应的隐藏状态来实现的。具体来说，DINM执行以下步骤：

1. **输入处理**：首先，将带有攻击性的查询（adversarial query）和相应的安全响应（safe response）输入到LLM中。
2. **隐藏状态分析**：对于每个输入，LLM会产生一系列的隐藏状态（hidden states），这些状态是模型内部各层的中间表示。
3. **语义差异最大化**：DINM计算安全响应和不安全响应对应的隐藏状态之间的差异，并找到差异最大的层，即有毒层（toxic layer）。这一层被认为是模型中最能区分安全和不安全内容的部分。
4. **有毒区域识别**：在有毒层中，DINM进一步识别出具体的有毒区域，这些区域通常是模型的参数，如自注意力（attention）头或前馈网络（feed-forward network）的权重。

#### 编辑有毒区域的参数

一旦定位了有毒区域，DINM将通过参数编辑来减少或消除这些区域的有害影响。编辑过程如下：

1. **参数调整**：DINM通过优化算法调整有毒区域内的参数。这个过程通常涉及到梯度下降或其他优化技术，目的是最小化安全响应和不安全响应之间的差异。
2. **损失函数设计**：为了确保编辑过程不会影响模型在其他任务上的性能，DINM设计了一个损失函数，该函数考虑了两个方面：一是提高生成安全内容的概率；二是保持对正常输入的适当响应。
3. **约束条件**：在编辑过程中，DINM还会引入一些约束条件，如保持语言模型的一般知识能力，避免对模型的其他功能造成损害。
4. **迭代优化**：通过迭代优化，DINM逐步调整参数，直到达到预定的性能指标或满足一定的安全标准。

通过这种方法，DINM旨在永久性地修改LLM中的有害区域，从而在不牺牲一般性能的前提下，提高模型在处理有害内容时的安全性和鲁棒性。





### 4. 本文创新点与贡献

* **构建基准**：提出了一个新的基准测试集SafeEdit，覆盖了九种不安全类别和多种强大的攻击模板。
* **评价指标**：扩展了评价指标，包括防御成功率、防御泛化能力和一般性能。
* **提出新方法**：提出了DINM方法，它是一种简单而有效的知识编辑基线，能够在几次调整中减少LLM的毒性。

### 5. 本文实验

实验部分对DINM进行了广泛的基准测试，与其他知识编辑方法和传统的解毒方法（如SFT和DPO）进行了比较。实验结果表明，DINM在解毒性能上具有显著优势，并且在一般性能上的影响相对较小。

### 6. 实验结论

实验结果证实了DINM在有害内容检测和一般性能保持方面的优势。DINM能够有效地对抗各种恶意输入，并且在处理不安全类别时具有较好的泛化能力。

### 7. 全文结论

本文通过构建新的基准测试集和提出DINM方法，展示了知识编辑在LLM解毒方面的潜力。DINM的提出为未来的解毒方法和LLM内部知识机制的研究提供了新的视角和工具。

### 阅读总结

本文针对大型语言模型在处理有害内容时的潜在风险，提出了一种新的解毒方法DINM。通过构建全面的基准测试集SafeEdit和设计的评价指标，本文不仅展示了DINM在解毒方面的有效性，还揭示了传统解毒方法的局限性。DINM的提出为LLM的安全性研究提供了新的方向，特别是在对抗性攻击和恶意输入的背景下。尽管存在一些限制，如计算资源的限制和对LLM架构的简化假设，但本文的工作为未来的研究奠定了坚实的基础，并为开发更安全、更可靠的LLMs提供了宝贵的见解。
