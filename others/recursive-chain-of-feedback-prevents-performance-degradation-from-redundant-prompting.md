# Recursive Chain-of-Feedback Prevents Performance Degradation from Redundant Prompting

<figure><img src="../.gitbook/assets/image (15) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

大型语言模型（LLMs）在处理复杂推理任务时经常遇到困难，无法构建逻辑上合理的解决方案。用户为了获得更好的回答，常常重复提示LLMs。然而，研究表明，这种重复的无意义反馈（如“再试一次”）会逐渐降低回答的质量，导致与预期结果的偏差越来越大。

### 2. 过去方案和缺点

过去的研究提出了多种提示工程方法，如链式思维（Chain-of-Thought）提示，以提高LLMs的可靠性。这些方法通常需要用户提供一个与目标问题类似的样本问题，包括推理步骤和解决方案。但在面对复杂问题时，找到这样的样本问题可能同样困难，甚至比回答问题本身还要难。

<figure><img src="../.gitbook/assets/image (16) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种名为递归链式反馈（Recursive Chain-of-Feedback, R-CoF）的新方法。R-CoF通过递归地修正初始错误的回答，将每个错误的推理步骤分解为更小的独立问题。该方法不需要用户提供类似的样本问题，而是通过分解原始问题为更易处理的子组件来提高LLMs的可靠性。

#### R-CoF的步骤：

1. 提出问题并获取LLM的初始回答。
2. 如果回答不正确，识别错误的推理步骤。
3. 将错误的步骤转化为子问题，并递归地请求另一个LLM解决该子问题。
4. 将正确的推理步骤整合回原始问题的回答中。
5. 重复上述过程，直到达到正确的解决方案。

<figure><img src="../.gitbook/assets/image (17) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 4. 本文创新点与贡献

* 提出了R-CoF方法，它不需要用户提供类似样本问题，而是通过递归修正错误步骤来提高LLMs的可靠性。
* 通过实验表明，R-CoF能够在没有额外样本数据的情况下，解决LLMs无法正确回答的问题。
* 提供了一种新的提示方法，适用于鼓励学习，允许用户将问题分解为更小的步骤，直到他们认为不需要专业知识就可以验证为止。

### 5. 本文实验

实验在ChatGPT-3.5上进行，测试了R-CoF在50个随机抽取的数学问题上的表现。结果显示，R-CoF能够准确纠正31个问题中的50个错误回答，并且在进行两次递归调用后，又额外纠正了6个问题。

### 6. 实验结论

R-CoF方法能够有效地纠正LLMs在复杂问题上的错误回答，提高了解决问题的准确性。这种方法通过分解问题，使得用户能够更容易地理解和验证推理步骤。

### 7. 全文结论

本文通过链式反馈（CoF）设置展示了无意义的重复提示会降低用户获得正确输出的机会。为了缓解这个问题，提出了R-CoF方法，它通过分解问题、识别错误步骤，并在不同设置中调整特定步骤，最终达到正确的解决方案。尽管R-CoF作为一个正在进行的工作还不完整，但它为通过将复杂问题分解为更简单问题来纠正LLMs推理中的错误提供了新的思路。

### 阅读总结

本文针对LLMs在复杂推理任务中的不足，提出了R-CoF方法，这是一种新的提示工程方法，它通过递归修正错误步骤来提高模型的可靠性。实验结果表明，R-CoF能够在没有类似样本的情况下解决LLMs无法正确回答的问题。这种方法为提高LLMs在复杂问题上的准确性提供了新的视角，并为未来的研究提供了有价值的方向。
