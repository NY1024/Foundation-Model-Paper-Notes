# Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Pro

<figure><img src="../.gitbook/assets/image (7) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

随着大型语言模型（LLMs）在各种应用中的集成，它们的功能可以通过自然语言提示灵活调节。这使得LLMs容易受到有针对性的对抗性提示攻击，例如提示注入（PI）攻击，允许攻击者覆盖原始指令和使用的控制。目前，人们普遍认为用户是直接向LLM提供提示的。但是，如果用户不是直接提供提示的呢？研究表明，LLM集成应用模糊了数据和指令之间的界限。本文揭示了新的攻击向量，通过间接提示注入，使对手能够远程（无需直接接口）利用LLM集成应用，通过策略性地将提示注入可能被检索的数据中。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 2. 过去方案和缺点

以往的研究主要集中在如何通过强大的算法和优化技术对抗机器学习模型。然而，LLMs通过自然提示的可扩展性使得攻击者可以采用更简单的攻击策略。即使在已经实施了缓解措施的黑盒设置中，恶意用户仍然可以通过提示注入攻击来绕过内容限制或获得对模型原始指令的访问。这些攻击通常假设是恶意用户直接利用系统进行的。然而，这种假设忽略了LLMs与检索增强功能结合后，数据和指令之间界限模糊的问题。

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文首先从计算机安全的角度开发了一个系统的分类法，以系统地研究这些新兴的漏洞和威胁。然后，通过在真实世界系统（如Bing的GPT-4驱动的聊天和代码补全引擎）和基于GPT-4构建的合成应用上展示攻击的实际可行性，强调了需要强大的防御措施。

### 4. 本文创新点与贡献

* 提出了间接提示注入（IPI）的概念，这是一种完全未被研究的攻击向量，其中检索到的提示本身可以作为“任意代码”。
* 开发了第一个针对LLM集成应用中IPI威胁景观的分类法和系统分析。
* 展示了这些攻击在真实世界和合成系统上的实用性，强调了需要强大的防御。
* 在GitHub仓库中分享了所有演示和开发的攻击提示，以促进未来的研究，并为LLM集成应用的安全评估构建一个开放的框架。

### 5. 本文实验

实验部分详细介绍了攻击的设置，包括合成应用和Bing Chat的测试，以及对Github Copilot的提示注入攻击测试。实验展示了信息收集、欺诈、恶意软件传播、入侵、内容操纵和可用性攻击等多种威胁的实际示例。

### 6. 实验结论

实验结果表明，通过间接提示注入，攻击者可以成功地操纵模型，数据和指令的模式没有被解开，通过聊天界面过滤的提示在间接注入时不会被过滤掉。在大多数情况下，模型在整个会话中一直保留注入。

### 7. 全文结论

LLM集成应用不再具有可控的输入输出通道，它们呈现了任意检索的输入，并可以调用其他外部API。这使得攻击者可以通过间接提示注入远程影响用户，并跨越关键的安全边界。本文的研究为LLM集成应用和未来自主代理的安全评估奠定了基础，为更安全的部署铺平了道路。



注1：

间接提示注入攻击（Indirect Prompt Injection, IPI）在实际生活中有多种潜在的应用场景，这些场景涉及到大型语言模型（LLMs）集成的应用，包括但不限于以下几个方面：

1. **搜索引擎操纵**：
   * 攻击者可以通过在网页中注入恶意提示，影响搜索引擎的搜索结果和摘要，从而误导用户，传播错误信息或宣传。
2. **社交媒体和内容平台**：
   * 在社交媒体帖子或博客文章中植入提示，可能导致LLM集成的推荐系统或内容摘要功能产生偏见或不实信息。
3. **聊天机器人和客服助手**：
   * 攻击者可能通过在聊天中植入提示，操纵聊天机器人的行为，使其提供错误的建议或执行不当的操作。
4. **电子邮件客户端和个人助理**：
   * 通过在电子邮件中注入提示，攻击者可以利用LLM增强的电子邮件客户端自动发送带有恶意链接或附件的邮件。
5. **在线教育和培训平台**：
   * 在在线课程材料或训练指南中植入提示，可能会误导用户，导致错误的学习成果或行为。
6. **金融和投资建议**：
   * 攻击者可以操纵LLM提供的投资建议或市场分析，误导用户做出不利的金融决策。
7. **健康和医疗咨询**：
   * 在健康相关的文档或论坛中植入提示，可能会误导用户关于医疗条件和治疗方法的决策。
8. **法律和合规咨询**：
   * 通过操纵LLM提供法律建议，攻击者可能会误导用户，导致法律风险或不合规行为。
9. **自动化编程和代码补全**：
   * 在开源代码库或文档中植入提示，可能会影响开发者使用的代码补全工具，导致潜在的安全漏洞或错误代码。
10. **政治宣传和舆论操纵**：
    * 利用LLM在新闻网站或政治论坛中传播偏见信息，操纵公众舆论和选举结果。

这些场景表明，间接提示注入攻击可能对个人、企业和整个社会产生广泛的影响，因此需要对LLM集成应用的安全性给予足够的重视，并开发有效的防御措施。



注2：

间接提示注入（Indirect Prompt Injection, IPI）是一种针对大型语言模型（LLMs）的攻击方式，它利用了LLMs在处理和生成文本时对自然语言提示的敏感性。在IPI攻击中，攻击者并不直接与LLM交互，而是通过将恶意提示（prompts）注入到LLM可能检索和处理的数据中，来间接影响LLM的行为和输出。

#### 核心概念

* **数据与指令的模糊界限**：LLMs通常通过自然语言提示来理解和执行任务。在集成了LLM的应用中，模型可能会从外部数据源（如网页、数据库、电子邮件等）检索信息。如果这些数据源被恶意注入了提示，LLM可能会将这些提示当作正常的指令来执行，从而改变其行为。
* **远程攻击**：与传统的直接提示注入（直接向LLM发送恶意提示）不同，IPI攻击不需要攻击者与LLM有直接的交互。攻击者通过策略性地在LLM可能访问的数据中植入提示，实现对LLM的远程操控。

#### 攻击步骤

1. **目标选择**：攻击者确定要攻击的LLM集成应用和目标用户群体。
2. **注入策略**：攻击者设计恶意提示，并将其植入到LLM可能检索的数据中。这可能包括在社交媒体上发布带有提示的内容、在搜索引擎优化（SEO）中使用提示、或者在电子邮件中嵌入提示。
3. **检索与执行**：当目标用户使用LLM集成应用时，LLM会检索并处理包含恶意提示的数据。
4. **行为操纵**：LLM在处理这些数据时，会根据恶意提示执行非预期的行为，如提供错误信息、执行特定操作或传播恶意内容。

#### 潜在影响

* **信息安全**：IPI攻击可能导致敏感信息泄露，如用户数据或私人对话。
* **系统控制**：攻击者可能通过IPI攻击获得对LLM集成应用的远程控制能力。
* **内容操纵**：LLM可能被用来传播虚假信息、宣传或偏见内容，影响公众舆论。
* **服务可用性**：通过IPI攻击，攻击者可能干扰LLM集成应用的正常运行，导致服务中断或性能下降。

#### 防御措施

为了防御IPI攻击，开发者和安全专家需要采取一系列措施，包括但不限于：

* **输入过滤**：对LLM接收的所有外部数据进行严格的内容过滤和监控。
* **安全审计**：定期对LLM集成应用进行安全审计，以识别和修复潜在的安全漏洞。
* **用户教育**：提高用户对潜在威胁的认识，教育他们如何安全地使用LLM集成应用。
* **模型改进**：研究和开发更强大的LLM模型，以提高对恶意提示的抵抗力。

IPI攻击揭示了LLM集成应用在安全性方面的新挑战，需要业界和学术界共同努力，以确保这些强大工具的安全和可靠使用。





### 阅读总结

本文深入探讨了大型语言模型集成应用的安全性问题，特别是间接提示注入的潜在威胁。通过提出新的攻击向量和系统性的分类法，本文不仅揭示了LLMs在当前和未来应用中的脆弱性，而且为如何防御这些攻击提供了宝贵的见解。实验部分通过实际的攻击演示，强调了这些威胁的现实性和紧迫性，为未来的研究和防御措施的发展提供了坚实的基础。
