# Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification

<figure><img src="../.gitbook/assets/image (2) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

大型语言模型（LLMs）在处理自然语言处理（NLP）任务时，因其生成的文本可能包含错误声明（即“幻觉”）而备受争议。这些幻觉可能会误导用户，因为它们通常与大量事实性陈述混杂在一起，使得用户难以识别错误信息。现有的利用LLMs的服务通常不提供检测不可靠生成的手段。因此，本文旨在通过基于令牌级别不确定性量化的方法来弥合这一差距，提出一种新颖的事实核查和幻觉检测流程。

### 2. 过去方案和缺点

以往的研究主要集中在利用外部知识源构建复杂系统来进行事实核查。这些方法存在与知识源的不完整性和存储知识的重大开销相关的问题。此外，先前的工作主要集中在对整个生成文本的不确定性量化，而在生成的部分内容上进行量化的需求显著增加了问题的复杂性。

### 3. 本文方案和步骤

本文提出了一种基于令牌级别不确定性量化的新事实核查和幻觉检测流程。该流程首先将生成的文本分解为原子声明，然后计算令牌级别的不确定性分数，并将其聚合为声明级别的不确定性。特别地，本文引入了一种新的令牌级别不确定性量化方法——声明条件概率（CCP），该方法仅测量模型表达的特定声明值的不确定性。实验表明，CCP在六种不同的LLMs和三种语言的任务中，相比基线有显著提升。

### 4. 本文创新点与贡献

* 提出了一种新颖的框架，使用令牌级别的不确定性量化来事实核查LLM生成的内容。
* 提出了一种新的用于令牌级别不确定性量化的方法，该方法优于基线，并且可以作为事实核查流程中的插件。
* 设计了一种新颖的评估白盒LLMs令牌级别UQ方法的方法，该方法基于事实核查，并可应用于其他白盒LLMs。
* 提供了LLM生成内容事实核查方法的实证和消融分析，发现所生成的不确定性分数可以帮助发现六种LLMs在三种语言上的声明中的事实错误。
* 所有代码和数据将在被接受后提供。

### 5. 本文实验

实验在生成传记的任务上评估了声明级别的UQ技术及其发现幻觉的能力。实验包括了多种LLMs在英语、中文和阿拉伯语上的表现。实验结果表明，CCP方法在所有测试的LLMs中均优于其他UQ技术。

### 6. 实验结论

实验结果表明，CCP方法在检测LLMs生成的幻觉方面优于其他UQ技术。此外，人工评估显示，基于不确定性量化的事实核查流程与利用外部知识的事实核查工具（FactScore）具有竞争力。

### 7. 全文结论

本文提出了一种基于令牌级别不确定性量化的新颖事实核查和幻觉检测方法。根据人工评估，该方法与利用外部知识源的事实核查工具（FactScore）具有竞争力，且仅通过访问LLMs的输出即可实现。CCP方法在多种LLMs和语言上均优于其他方法。

### 阅读总结

本文针对大型语言模型在生成文本时可能出现的错误声明问题，提出了一种新颖的事实核查方法。通过引入声明条件概率（CCP）这一令牌级别不确定性量化方法，本文不仅提高了事实核查的准确性，而且避免了依赖外部知识源的复杂性和成本。实验结果证明了该方法的有效性，为未来LLMs的可靠应用提供了有价值的方向。
