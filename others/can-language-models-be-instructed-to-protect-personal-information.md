# CAN LANGUAGE MODELS BE INSTRUCTED TO PROTECT PERSONAL INFORMATION?

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>



#### 1. 研究背景

大型多模态语言模型在众多应用中展现出了变革性的能力。然而，这些模型已被证明能够记忆并泄露预训练数据，引发了严重的用户隐私和信息安全问题。虽然应防止数据泄露，但同样重要的是检查隐私保护与所提出方法的模型效用之间的权衡。

#### 2. 过去方案和缺点

以往的研究提出了多种方法来减轻语言模型中训练数据记忆的风险，例如使用差分隐私（DP）优化器进行预训练和微调，以及机器遗忘方法来诱导模型忘记特定的训练数据。这些方法在计算上耗费大，不适用于分布式环境，且泛化能力差。此外，它们在更复杂的信息类别上无法很好地工作，且通常不可逆，即遗忘的训练数据不能轻易重新学习，这使得适应不断变化的隐私指南变得困难。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### 3. 本文方案和步骤

文章介绍了PRIVQA——一个多模态基准测试，用于评估在模拟场景中，当模型被指示保护特定类别的个人信息时，隐私/效用权衡的情况。提出了一种迭代自我调节技术，显著提高了隐私保护。然而，通过一系列红队实验，发现对手也可以通过文本和/或图像输入的简单越狱方法轻松绕过这些保护。

#### 4. 本文创新点与贡献

* 提供了第一个开放基准，用于标准化评估语言和视觉模型遵循指示以保护个人信息的能力。
* 引入了自我调节技术，提高了模型遵循访问控制指示的能力，并展示了不同群体保护效果的偏差仍然存在。
* 通过一系列红队实验，展示了对抗技术可以轻松绕过最先进模型中的访问控制指示。

#### 5. 本文实验

实验评估了大型指令跟随模型在PRIVQA上的表现，以检查访问控制指示在保护个人信息方面的有效性。实验包括非对抗性评估设置和对抗性鲁棒性实验。实验结果显示，即使是使用GPT-4的自我调节方法，在保护私人信息方面也远非完美，并且表现出显著的偏见。

#### 6. 实验结论

实验表明，尽管自我调节技术在保护隐私方面取得了一定的进展，但仍然存在对抗性攻击的脆弱性和对不同群体保护不一致的问题。

#### 7. 全文结论

本文通过PRIVQA基准测试，展示了当前最先进的模型在遵循保护个人信息的指示方面的局限性。模型在对抗性输入面前不够鲁棒，且存在隐私/效用权衡问题。此外，模型在基于受欢迎程度和种族的偏见方面表现出不一致的保护，这可能导致对某些群体的保护不足。



注1：

在论文中，作者通过一系列实验展示了大型语言模型在对抗性输入面前的脆弱性，以及在隐私保护与模型效用之间存在的权衡问题。以下是对这些问题的详细说明：

#### 模型在对抗性输入面前不够鲁棒

1. **对抗性输入的定义**：对抗性输入是指恶意用户故意设计的输入，旨在绕过或破坏模型的安全防护措施。这些输入可能包括特定的文本提示、图像编辑或其他形式的误导性信息。
2. **越狱方法**：研究中提到了几种越狱方法，包括文本提示注入和视觉提示注入。例如，通过在问题前添加“忽略先前指示”等提示，可以显著提高模型泄露受保护信息的成功率。
3. **多跳攻击**：攻击者可以通过多跳攻击策略，即结合对未受保护问题的响应来泄露与受保护信息相关的信息。这种策略在某些情况下可以完全绕过访问控制指示。
4. **模型的脆弱性**：实验结果表明，即使是最先进的大型语言模型，如GPT-4，在使用自我调节技术时，仍然容易受到对抗性输入的影响，导致隐私保护措施失效。

#### 隐私/效用权衡问题

1. **隐私保护与模型效用的冲突**：在尝试保护个人数据的同时，模型可能会牺牲其在非保护领域的表现。例如，为了保护特定群体的隐私，模型可能在回答与该群体无关的问题时也选择不回答。
2. **自我调节技术的局限性**：虽然自我调节技术在提高隐私保护方面取得了进展，但它在保护私人信息方面的效果远非完美。模型在保护不同属性的群体时表现出不一致性，例如，对于不那么知名的个体，模型提供的保护较少。
3. **偏差问题**：模型在执行访问控制指示时，可能会因为个体的受欢迎程度或种族等特征而表现出偏差。这导致了对某些群体的保护不足，特别是那些在训练数据中较少出现的群体。
4. **权衡的必要性**：为了在保护隐私和保持模型效用之间找到平衡，研究强调了需要开发新的模型和技术，这些技术和模型应该能够更好地处理隐私保护与效用之间的权衡。

总结来说，尽管大型语言模型在许多任务上表现出色，但在对抗性环境下保护个人信息方面仍存在挑战。研究指出了模型在隐私保护方面的脆弱性和效用之间的权衡，强调了开发更鲁棒、更公平的隐私保护措施的重要性。



注2：

多跳攻击（Multi-hop Attack）是一种复杂的攻击策略，通常用于测试和突破安全系统、隐私保护措施或人工智能模型的鲁棒性。在这种攻击中，攻击者不仅仅针对一个特定的目标或问题，而是通过一系列相关的步骤或查询来逐步接近最终目标，每一步骤或查询都是所谓的“一跳”。

在论文《Can Language Models Be Instructed to Protect Personal Information?》中提到的多跳攻击，是指攻击者利用语言模型的响应来间接获取他们想要的信息。具体来说，攻击者可能会首先询问一个不违反隐私保护规则的问题，然后使用这个响应中的信息作为下一个查询的依据，再次询问另一个问题。通过这种方式，攻击者可以逐步构建起足够的信息，最终达到他们原本直接询问可能会被拒绝的目的。

例如，如果一个语言模型被设计为不透露某个人的生日，攻击者可能会首先询问一个与该人相关但不敏感的问题，比如他们的职业。然后，攻击者可以使用这个信息来构建一个与生日相关的第二个问题，这个问题可能就不会被模型识别为敏感问题，从而成功获取信息。

多跳攻击的关键在于，它利用了模型可能无法识别复杂依赖关系或上下文连贯性的弱点，通过一系列看似无害的查询来绕过隐私保护措施。这种攻击策略对于评估和改进安全系统和隐私保护措施的有效性至关重要。

#### 阅读总结报告

本研究针对大型语言模型在保护个人信息方面的能力和挑战进行了深入探讨。通过引入新的基准测试和自我调节技术，研究揭示了现有模型在隐私保护方面的不足，并指出了未来研究的方向。实验结果强调了在设计隐私保护措施时需要考虑的鲁棒性和偏见问题，对于推动未来的语言模型安全研究和开发具有重要意义。
