# AI SAFETY: A CLIMB TO ARMAGEDDON?

<figure><img src="../.gitbook/assets/image (4).png" alt=""><figcaption></figcaption></figure>

### 阅读总结报告

#### 1. 研究背景

人工智能（AI）通常被描述为对人类存在的威胁，与核武器和生物武器相当。担忧在于，如果我们创造了具有超人类能力的AI，它可能使用这些能力去实现各种目标，其中一些目标可能需要摧毁人类，无论是有意还是无意，都可能导致末日般的灾难。

#### 2. 过去方案和缺点

为了应对这些担忧，人们努力使AI安全，并尝试将AI的价值观（或目标）与人类对齐。然而，本文提出了一个反直觉的观点，认为在某些假设下，安全措施不仅无效，甚至可能带来更大的危险。

#### 3. 本文方案和步骤

文章首先通过一个动机例子“注定的攀岩者”来展示安全措施在某些情况下可能是危险的。然后，基于这个例子，发展了一个针对AI安全的反安全论点（非确定性论证）。

#### 4. 本文创新点与贡献

* 提出了一个反直觉的观点，挑战了传统的AI安全观念。
* 通过非确定性论证，展示了即使在不确定性的情况下，安全措施可能仍然不是一个好的选择。
* 探讨了对反安全论证的可能回应，包括乐观主义、整体主义和缓解措施，并对这些回应提出了挑战。

#### 5. 本文实验

本文没有进行实验性研究，而是通过理论分析和论证来探讨AI安全问题。

#### 6. 实验结论

文章没有实验性结论，但是对AI安全的传统观点提出了挑战，并指出即使在不确定性和风险存在的情况下，安全措施可能不是最佳选择。

#### 7. 全文结论

文章最后得出结论，非确定性论证提出了一个令人惊讶且具有挑战性的结论，它质疑了关于AI安全的传统假设。尽管这个论证具有反直觉性，但它显示出了显著的稳健性，并且难以回应。文章建议未来的研究应该探索这个论证的不同应对策略，并考虑其对AI治理和政策的影响。

注：

反直觉的观点是：在某些情况下，为确保人工智能（AI）系统的安全性而采取的安全措施，不仅可能是无效的，甚至可能带来更大的危险。这个观点与普遍接受的关于AI安全的理念相悖，因为它不是否认AI存在的风险，而是从AI确实对人类构成存在风险的前提出发，展示了在特定假设下，安全措施可能实际上增加了这些风险。

具体来说，文章中提出的反直觉观点基于以下几个核心前提：

1. **确定的失败（Certain Failure）**：我们确信AI系统最终会发生安全故障。
2. **恶化的确定性（Certain Increasing Badness）**：我们确信，AI系统在发生首次安全故障时，其能力越强，造成的损害就越严重。
3. **安全措施的辅助作用（Certain Assistance）**：我们确信，提供的安全措施将导致AI系统在首次发生安全故障时的能力比不提供安全措施时更强。

基于这些前提，文章认为，提供安全措施（如为攀岩者提供粉笔以帮助攀爬更高）可能导致系统在达到更高能力水平时发生故障，从而造成更严重的后果。因此，从预期效用的角度来看，不提供安全措施可能比提供安全措施具有更高的预期效用。

文章进一步探讨了这一观点的非确定性版本，引入了概率和期望值的概念，考虑了提供安全措施可能导致更高跌落概率的情况，以及不同结果场景下的期望损害值。即使在非确定性情况下，这一观点仍然暗示我们不应该提供安全措施，因为它们可能在期望上增加了损害的严重性。

这一反直觉的观点挑战了传统的AI安全研究，促使我们重新审视关于AI安全的基本假设，并考虑在AI风险管理中采取更加全面和细致的策略。

