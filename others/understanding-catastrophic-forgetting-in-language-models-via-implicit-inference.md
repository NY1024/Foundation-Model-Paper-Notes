# UNDERSTANDING CATASTROPHIC FORGETTING IN LANGUAGE MODELS VIA IMPLICIT INFERENCE

<figure><img src="../.gitbook/assets/image (3) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 研究背景

本研究探讨了在对大型语言模型（LLMs）进行微调（fine-tuning）时，模型在特定任务上的性能提升可能会以牺牲其他任务能力为代价的现象，即“灾难性遗忘”（catastrophic forgetting）。微调是训练语言模型以执行特定任务的关键步骤，但现有研究对微调的影响缺乏系统性理解，尤其是在微调分布之外的任务上。研究者通过简化场景，展示了在微调数据分布内的 task 上提升性能会削弱模型在其他任务上的表现，特别是在与微调分布“最接近”的任务上。

## 过去方案和缺点

以往的研究主要集中在如何通过指令调整（instruction-tuning）或基于人类反馈的强化学习等方法来微调语言模型，以提高其在特定任务上的表现。然而，这些方法通常忽略了微调可能导致模型在其他任务上性能下降的问题。此外，现有研究往往缺乏对微调影响的全面评估，特别是对微调分布外任务的影响。

## 本文方案和步骤

研究者提出了一种称为“共轭提示”（Conjugate Prompting）的方法，旨在通过改变提示的方式恢复预训练模型的能力。具体步骤包括：

1. 通过合成设置理解微调的影响。
2. 提出假设：微调过程中模型隐式推断的任务与微调分布中的任务相符，而微调主要使任务推断偏向于微调分布中的任务。
3. 通过共轭提示测试是否可以恢复预训练能力。
4. 将共轭提示应用于真实世界的大型语言模型（LLMs），并观察微调分布通常偏向英语的情况。
5. 通过翻译提示到不同语言来测试共轭提示的效果。

## 本文创新点与贡献

1. 提出了共轭提示的概念，这是一种新的策略，用于恢复微调后模型在预训练任务上的能力。
2. 通过合成实验验证了微调可能导致模型在预训练任务上的性能下降，而不是完全遗忘这些任务。
3. 展示了通过将提示翻译成不同语言，可以恢复模型在预训练任务上的能力，这一发现对于理解模型的多语言能力和微调的影响具有重要意义。
4. 揭示了微调可能导致的“灾难性遗忘”现象，为未来的研究提供了新的视角和方法。

## 本文实验

实验包括在合成设置中测试共轭提示的效果，以及在真实世界的大型语言模型上应用共轭提示。实验结果显示，共轭提示能够在一定程度上恢复微调模型在预训练任务上的性能。

## 实验结论

实验结果支持了研究者的假设，即微调可能导致模型在预训练任务上的性能下降，但这些能力并没有被完全遗忘，而是被抑制。通过共轭提示，可以部分恢复这些能力。

## 全文结论

研究表明，微调可能导致模型在预训练任务上的性能下降，但这种现象可以通过共轭提示得到缓解。这一发现对于理解微调的影响、改进微调方法以及开发更可靠的模型部署策略具有重要意义。



注1：\


“共轭提示”（Conjugate Prompting）的方法能够恢复预训练模型的能力，是因为它基于对大型语言模型（LLMs）在微调过程中行为变化的理解。具体来说，这种方法的核心思想和步骤如下：

1. **隐式任务推断假设**：研究者们假设，语言模型在处理提示时会隐式地推断出任务的性质，并根据这种推断来决定如何生成响应。在微调过程中，模型通过学习微调数据中的特定任务和模式，调整了它的任务推断机制，使其更偏向于微调任务。
2. **任务推断的偏移**：微调导致模型在处理与微调分布相近但不同的任务时，其隐式任务推断偏向于微调任务，从而抑制了模型使用预训练时学到的能力。这种偏移尤其在与微调分布“最接近”的任务上最为明显。
3. **共轭提示的设计**：为了恢复预训练模型的能力，研究者们设计了共轭提示方法。这种方法通过改变输入提示的外观，使其看起来与微调分布中的任务不同，从而“欺骗”模型，使其认为当前任务与微调任务不同，进而恢复使用预训练时学到的能力。
4. **实验验证**：在合成实验中，研究者们通过改变提示的规模（例如，通过缩放标签）来测试共轭提示的效果，并发现这种方法可以改善微调模型在预训练任务上的表现。在真实世界的大型语言模型上，通过将提示翻译成不同语言，研究者们发现这种方法同样有效，因为它降低了提示与微调分布的相似性。

总的来说，“共轭提示”之所以有效，是因为它直接针对模型的隐式任务推断机制，通过改变输入提示的方式，引导模型回到预训练时的状态，从而恢复其在预训练任务上的能力。这种方法为理解和操纵大型语言模型的行为提供了新的视角，并可能对改进微调过程和提高模型的适应性具有重要意义。



注2：\
在大型语言模型（LLMs）经过微调之后，模型在学习特定任务时会形成一种倾向性，即在处理新的输入（提示）时，模型会根据其在微调过程中学到的特征和模式来推断当前任务的性质，并据此生成响应。这种倾向性可能导致模型在处理与微调任务类似的新任务时，过度依赖微调过程中学到的知识和行为模式，而忽视或抑制了它在预训练阶段学到的更广泛的能力。

“共轭提示”方法的目的是通过改变输入提示的形式或内容，使模型不再将其与微调任务联系起来。具体来说，这种方法通过以下方式实现：

1. **改变提示的外观**：通过某种转换（例如，改变语言、使用不同的表达方式等），使得输入提示在模型看来与微调数据中的任务不同。这种改变不会改变任务的本质，但足以影响模型对任务的隐式推断。
2. **恢复预训练能力**：当模型不再将当前任务视为微调任务时，它就可能回退到预训练状态下的行为模式，即利用预训练过程中学到的更广泛的知识和能力来处理任务。这样，模型就能够展现出在微调过程中可能被抑制的原始能力。

简而言之，这句话表达的是“共轭提示”方法通过改变输入提示的方式，误导模型认为它正在处理的是一个与微调任务不同的任务，从而鼓励模型利用其在预训练阶段学到的更广泛的能力，而不是仅仅依赖于微调过程中学到的特定任务的解决方案。这种方法有助于恢复模型在多样化任务处理上的灵活性和适应性。

## 阅读总结报告

本研究通过深入分析微调对大型语言模型的影响，提出了共轭提示这一创新方法，用于恢复模型在预训练任务上的能力。研究不仅揭示了微调可能导致的灾难性遗忘现象，还为未来的研究和实践提供了新的视角和工具。通过实验验证，共轭提示在合成设置和真实世界的大型语言模型上均显示出恢复预训练能力的潜力，这对于提高模型的多语言能力和适应性具有重要意义。研究的结论强调了在微调过程中需要更加关注模型的全面性能，以及在实际应用中如何平衡模型在不同任务上的表现。
