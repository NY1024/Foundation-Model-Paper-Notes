# The Foundation Model Transparency Index

<figure><img src="../.gitbook/assets/image (10) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

随着基础模型（Foundation Models，FMs）如LLaMA和DALL-E等在人工智能领域的兴起，它们对社会产生了显著影响。这些资源密集型的模型通常需要处理数万亿字节的数据，并耗费数百万美元的建设成本。基础模型推动了消费者技术快速发展，引起了商业投资和公众对人工智能的广泛关注。然而，这些模型的透明度正在下降，这与过去数字技术（如社交媒体）的不透明性相似，最终导致了伤害。为了确保公共问责、科学创新和有效治理，透明度是一个至关重要的前提条件。

### 2. 过去方案和缺点

过去的方案通常缺乏透明度，这导致了对基础模型如何开发和部署的理解不足。透明度的缺失使得利益相关者无法了解这些模型对社会的影响。历史上，数字技术公司在开发和部署技术时往往不透明，这种做法最终导致了伤害，如缅甸的罗兴亚种族灭绝事件和剑桥分析丑闻。尽管学术界、民间社会组织、公司和政府都呼吁基础模型开发者提高透明度，但目前透明度仍然很低。

<figure><img src="../.gitbook/assets/image (11) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一个名为“基础模型透明度指数”（Foundation Model Transparency Index，FMTI）的评估框架。该指数通过100个具体的指标来量化和评估基础模型开发者的透明度。这些指标涵盖了从上游资源（如数据、劳动力、计算资源）到模型本身（如能力、风险评估）再到下游使用（如分发渠道、使用政策、影响地理区域）的各个方面。通过这些指标，研究者们能够对10家主要的基础模型开发者进行评分，并提出改进透明度的建议。

### 4. 本文创新点与贡献

本文的主要创新点在于提出了一个全面、系统的基础模型透明度评估框架。这个框架不仅为评估透明度提供了一个量化工具，而且通过公开和可复现的评分过程，鼓励基础模型开发者提高透明度。此外，FMTI的发布旨在推动行业标准和监管干预，以促进基础模型治理的进步。

### 5. 本文实验

研究者们对10家主要的基础模型开发者进行了评分，包括OpenAI、Google、Meta等。评分过程包括对每个开发者的旗舰基础模型的实践进行评估，并使用标准化的搜索协议来确保评分的一致性和准确性。此外，研究者们还通知了开发者，并在发布前提供了他们对评分提出异议的机会。

### 6. 实验结论

实验结果显示，尽管所有开发者在某些透明度指标上取得了进展，但总体上，基础模型生态系统的透明度仍有显著的提升空间。研究者们发现，开发者在上游资源透明度、模型风险评估和下游影响方面的表现尤其不足。此外，开放开发者在透明度方面普遍优于封闭开发者。

### 7. 全文结论

本文强调了提高基础模型透明度的重要性，并提出了一个实用的评估工具来促进这一目标。通过FMTI，研究者们不仅揭示了当前透明度的不足，还为如何改进提供了明确的指导。这一工作为未来的基础模型治理和透明度提升奠定了基础。

### 阅读总结

本文通过提出基础模型透明度指数（FMTI），为评估和提升基础模型开发者的透明度提供了一个全面的框架。这一工作不仅有助于提高公众对基础模型如何开发和使用的理解，而且对于促进负责任的AI实践和政策制定具有重要意义。尽管透明度本身并不足以解决所有问题，但它是实现公共问责和有效治理的关键第一步。通过FMTI的实施，研究者们鼓励开发者、部署者和政策制定者共同努力，提高透明度，以确保基础模型技术能够更好地服务于社会。
