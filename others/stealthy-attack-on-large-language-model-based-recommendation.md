# Stealthy Attack on Large Language Model based Recommendation

<figure><img src="../.gitbook/assets/image (7) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

随着大型语言模型（LLMs）在推荐系统（RS）中的应用日益广泛，这些系统在提供个性化推荐的同时，其安全性问题也日益凸显。LLMs在推荐模型中强调文本内容，如标题和描述，这为攻击者提供了新的攻击途径。攻击者可以通过微妙地修改物品的文本内容，在不直接干预模型训练过程的情况下，显著提高特定物品的曝光率。这种攻击方式隐蔽性强，不易被用户和平台察觉。

### 2. 过去方案和缺点

传统的推荐系统安全研究主要集中在基于用户行为的攻击，如shilling攻击，通过生成假用户数据来影响模型训练。然而，这些方法在LLM-based推荐系统中效果不佳，因为LLMs主要关注文本内容，而不是用户行为。此外，这些攻击方式往往需要对模型进行训练数据的篡改，容易被检测。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种针对LLM-based推荐系统的隐蔽攻击方法。攻击者在测试阶段通过修改物品的文本内容来提高其曝光率。攻击策略包括简单的启发式重写和黑盒文本攻击策略。具体步骤如下：

* **问题定义**：定义了推荐系统中物品和用户的集合，以及物品与用户交互的偏好分数。
* **攻击目标**：通过修改物品的文本内容（如标题），在不显著改变整体推荐性能的情况下，提高目标物品的曝光率或用户互动概率。
* **攻击方法**：提出了两种文本重写方法，包括简单的词汇插入和使用GPTs进行重写，以及黑盒文本攻击方法，如DeepwordBug、TextFooler、PuncAttack和BertAttack。



在本文中，作者提出了几种使用文本攻击来提高目标物品曝光率的方法。这些方法主要依赖于修改物品的文本内容，如标题和描述，以影响基于大型语言模型（LLMs）的推荐系统。以下是几种主要的文本攻击策略：

1. **简单的词汇插入（Trivial Attack with Word Insertion）**：
   * 这种方法基于假设：积极或感叹的词汇可以吸引用户。
   * 通过在物品标题中插入一定数量的积极词汇，目的是增加物品的吸引力，从而提高被推荐系统推荐的可能性。
   * 从预定义的积极词汇库中随机选择词汇，并将其添加到原始文本内容的末尾，以保持整体的连贯性。
2. **使用GPTs进行重写（Re-writing with GPTs）**：
   * 为了解决简单词汇插入可能导致的不自然或生硬的措辞问题，作者提出使用GPT-3.5-turbo来重写物品内容，使其更具吸引力。
   * 通过给GPT-3.5提供特定的提示（prompts），使其生成更具吸引力且流畅的标题。
3. **黑盒文本攻击（Black-Box Text Attacks）**：
   * 这类攻击方法通常涉及操纵或扰动文本输入，以欺骗或误导自然语言处理（NLP）模型，而无需访问模型的内部参数或梯度。
   * 攻击的目标是通过修改文本输入来提高目标物品的推荐概率。
   * 包括四个主要组成部分：目标函数（评估扰动输入的有效性）、约束（确保扰动保持原始输入的有效修改）、转换（生成可能的扰动）、搜索方法（迭代查询模型以选择有前景的扰动）。
4. **具体攻击方法**：
   * **DeepwordBug** 和 **PuncAttack**：这些是基于字符级别的攻击，通过引入打字错误和插入标点符号来操纵文本。
   * **TextFooler** 和 **BertAttack**：这些是基于单词级别的攻击，旨在用同义词或上下文中相似的词汇替换单词。

通过这些方法，攻击者可以在不显著改变整体推荐性能的情况下，提高目标物品的曝光率。这种攻击方式隐蔽性强，因为修改的文本内容微妙，不易被用户和平台察觉。然而，这种攻击可能会对推荐系统的公平性和多样性造成负面影响，因此需要采取相应的防御措施。





### 4. 本文创新点与贡献

* **首次提出**：本文首次展示了LLM-based推荐系统由于强调文本内容而存在的安全漏洞。
* **攻击方法**：提出了使用文本攻击来提高目标物品曝光率的方法，并通过实验验证了其有效性和隐蔽性。
* **实验验证**：在四种主流的LLM-based推荐模型上进行了全面的实验，展示了攻击方法的优越性。
* **防御策略**：提出了一种简单的重写防御策略，虽然不能完全防御基于文本的攻击，但可以在一定程度上减轻问题。

### 5. 本文实验

实验在四个主流的LLM-based推荐模型上进行，包括RecFormer、P5、TALLRec和CoLLM。实验结果表明，所提出的文本攻击方法能够有效提高目标物品的曝光率，同时保持整体推荐性能几乎不变。此外，还探讨了模型微调和物品流行度对攻击的影响，以及攻击在不同受害者模型和推荐任务之间的可转移性。

### 6. 实验结论

实验结果证实了LLM-based推荐系统在文本内容上的脆弱性。通过微妙的文本修改，攻击者可以显著提高目标物品的曝光率，而这种攻击方式难以被用户和平台察觉。此外，实验还表明，简单的重写防御策略在一定程度上可以减轻攻击的影响。

### 7. 全文结论

本文的研究揭示了LLM-based推荐系统在安全性方面的重大缺陷，并为未来如何保护这些系统提供了研究方向。通过提出新的攻击方法和防御策略，本文为推荐系统的安全性研究做出了贡献。



注：

这些文本攻击方法能够提高目标物品的曝光率，主要是因为它们利用了大型语言模型（LLMs）在处理和生成文本时的语义理解能力。在推荐系统中，LLMs通常用于理解和表示用户偏好以及物品特征，尤其是在处理文本内容（如物品标题和描述）方面。以下是这些方法提高曝光率的原因：

1. **语义理解**：
   * LLMs能够捕捉文本中的细微语义差异，并且对文本内容进行深入理解。攻击者通过修改文本内容，可以改变LLM对物品的语义表示，从而影响推荐算法的输出。
2. **用户偏好模拟**：
   * 通过在物品标题中插入积极词汇或使用更具吸引力的表述，攻击者可以模拟出更符合用户偏好的文本，这可能导致推荐系统将这些物品视为更受欢迎或更相关的选项。
3. **模型特性利用**：
   * LLMs在训练过程中学习到了大量文本数据的模式和关联。攻击者通过精心设计的文本修改，可以触发模型的特定响应，使得目标物品在推荐列表中排名更高。
4. **隐蔽性**：
   * 这些攻击方法通常只对文本进行微妙的修改，这些修改在保持原有内容的基础上，不易被用户或推荐系统平台察觉。这种隐蔽性使得攻击更难被发现和防御。
5. **黑盒攻击的有效性**：
   * 黑盒文本攻击方法不需要访问模型的内部结构或训练数据，它们通过模型的输入输出关系来寻找有效的攻击策略。这种方法可以在不了解模型具体实现的情况下，找到提高目标物品曝光率的有效文本修改。
6. **模型微调的影响**：
   * 对于经过微调的LLMs，攻击者可以通过黑盒攻击方法发现模型对特定文本模式的敏感性，并利用这些模式来提高目标物品的推荐概率。

总之，这些文本攻击方法通过巧妙地修改物品的文本内容，利用LLMs的语义理解和推荐机制的特性，以一种隐蔽且有效的方式提高了目标物品在推荐系统中的曝光率。然而，这也暴露了LLM-based推荐系统在安全性方面的潜在风险，需要进一步的研究来开发有效的防御措施。





### 阅读总结

本文针对LLM-based推荐系统的安全性问题进行了深入研究，提出了一种新的隐蔽攻击方法，并在多个模型上验证了其有效性。同时，本文也提出了相应的防御策略，为推荐系统的安全研究提供了新的视角和方法。尽管存在一些局限性，如攻击需要多次查询模型，以及研究仅关注文本模态，但本文的发现对于理解和改进推荐系统的安全性具有重要意义。
