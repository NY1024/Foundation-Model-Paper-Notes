# LARGE LANGUAGE MODELS AS AUTOMATED ALIGNERS FOR BENCHMARKING VISION-LANGUAGE MODELS

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>



#### 1. 研究背景

随着大型语言模型（LLMs）的进步，视觉-语言模型（VLMs）在执行复杂认知和推理任务方面展现出显著的能力。然而，现有的评估基准主要依赖于手工制作的固定数据集来衡量特定任务的性能，这在评估这些日益拟人化的模型与人类智能的对齐方面存在显著局限性。

#### 2. 过去方案和缺点

以往的评估方法存在以下缺点：

* 有限的策划：传统的策划依赖于手动注释，这在全面性上存在固有的局限性，无法充分验证模型的能力。
* 狭窄的评估：传统的评估通常依赖于基于规则的度量来检查任务导向的偏好，这在开放式和能力导向的判断方面存在挑战。

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

#### 3. 本文方案和步骤

本文提出了Auto-Bench，它利用LLMs作为熟练的对齐器，通过自动数据策划和评估来衡量VLMs与人类智能和价值的对齐。Auto-Bench的两个核心设计是：

* LLM作为自动策划者：使用GPT-4自动生成大量问题-答案-推理三元组。
* LLM作为自动裁判：使用GPT-3.5作为评估裁判，进行定量和定性的自动评估。

#### 4. 本文创新点与贡献

* 提出了一个创新的基准框架Auto-Bench，使用先进的LLMs自动策划数据和评估模型，确保客观地访问VLMs与人类能力和偏好的对齐。
* Auto-Bench生成了28.5K人工验证和3504K原始三元组，涵盖了四个总体能力和16个子技能。
* 通过定量和定性比较，对流行的VLMs进行了广泛的实证评估。

#### 5. 本文实验

* 使用Auto-Bench评估了八种流行的VLMs，包括BLIP2、InstructBLIP、LLaMA-Adapter V2等。
* 通过人工验证和用户研究比较Auto-Bench与其他流行的VQA数据集的质量。
* 进行了大量的实证评估，包括定量和定性比较。

#### 6. 实验结论

* 现有的VLMs在执行复杂任务方面仍有不足，并且揭示了与人类价值观相关的有问题的行为。
* Auto-Bench生成的3504K原始三元组促进了对现有VLMs进行监督微调（SFT）的过程，从而相应地提高了它们的能力。

#### 7. 全文结论

Auto-Bench作为一个灵活、可扩展、全面的基准，为评估和提升VLMs提供了宝贵的资源，促进了对VLMs评估和增强的进一步发展。

#### 阅读总结报告

本研究提出了一个新的基准框架Auto-Bench，它利用大型语言模型作为自动策划者和裁判，来评估视觉-语言模型的性能和对齐度。Auto-Bench通过自动生成和评估大量的问题-答案-推理三元组，克服了传统评估方法的局限性。实验结果表明，Auto-Bench能够有效地评估VLMs，并揭示了现有模型在处理复杂任务和遵守人类价值观方面的不足。此外，Auto-Bench生成的大规模数据集为VLMs的监督微调提供了资源，有助于提高模型的特定能力。这项工作为VLMs的评估和训练提供了新的视角，并为未来的研究提供了宝贵的资源和方向。
