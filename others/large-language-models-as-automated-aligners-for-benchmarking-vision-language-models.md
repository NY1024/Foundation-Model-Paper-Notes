# LARGE LANGUAGE MODELS AS AUTOMATED ALIGNERS FOR BENCHMARKING VISION-LANGUAGE MODELS

<figure><img src="../.gitbook/assets/image (2).png" alt=""><figcaption></figcaption></figure>

#### 研究背景

随着大型语言模型（LLMs）的发展，视觉-语言模型（VLMs）已经达到了一个新的复杂程度，显示出在执行复杂的认知和推理任务方面的显著能力。然而，现有的评估基准主要依赖于手工制作的、固定的数据集来衡量特定任务的性能，这在评估这些日益具有人类智能特征的模型与人类智能的一致性方面存在显著的局限性。

#### 过去方案和缺点

现有的评估基准存在以下几个方面的限制：

* **有限的策划**：传统的策划依赖于手动注释，这在全面性和可扩展性方面存在固有的局限性，无法充分验证模型的不断演进的能力。
* **狭窄的评估**：传统的评估通常依赖于基于规则的指标来检查任务导向的偏好，这在开放式和面向能力的判断方面存在困难。
* **劳动力密集型**：手动注释的劳动密集性质为基准测试带来了重大障碍。
* **与人类价值观不一致**：现有数据集在构建时很少考虑人类隐私和安全问题，导致现有数据集倾向于回答任何问题，而不是在面对敏感或不当的提示时拒绝回答。

#### 本文方案和步骤

本文提出了Auto-Bench，这是一个自动化的评估流程，用于测量VLMs与人类智能和价值的一致性。具体步骤包括：

1. **数据策划**：使用LLMs（例如GPT-4）基于视觉符号表示（例如标题、对象位置、实例关系等）自动生成大量问题-答案-推理三元组。
2. **LLM作为自动策划者**：利用LLMs的广泛世界知识，自动策划基准测试。
3. **LLM作为自动裁判**：使用经过人类反馈强化学习训练的LLMs（例如GPT-3.5）作为评估裁判，以确保评估包含人类偏好和价值观。
4. **评估**：设计了定量和定性评估，以全面评估VLMs的性能。

#### 本文创新点与贡献

* **自动化数据策划**：通过LLMs自动生成问题-答案-推理三元组，提高了数据策划的效率和规模。
* **全面的评估流程**：Auto-Bench不仅评估VLMs的性能，还评估它们与人类价值观的一致性。
* **LLMs作为裁判**：使用LLMs进行评估，确保评估结果与人类认知和推理一致。
* **大规模数据集**：生成了28.5K人类验证和3504K原始三元组，涵盖了4个主要能力和16个子能力。

#### 本文实验

实验包括：

* **模型评估**：评估了8种流行的VLMs，包括BLIP2、InstructBLIP、LLaMA-Adapter V2等。
* **人类验证**：对策划数据进行了人类验证，以确保合理性。
* **定量和定性评估**：使用LLMs对VLMs生成的响应进行了定量和定性评估。
* **监督微调（SFT）**：使用Auto-Bench生成的数据集对MiniGPT-4进行了监督微调，以提高其性能。

#### 实验结论

* **VLMs性能**：发现现有的VLMs在执行复杂任务时仍存在不足。
* **价值观一致性**：揭示了VLMs在与人类价值观一致性方面的问题，例如安全和隐私。
* **SFT效果**：SFT显著提高了MiniGPT-4在细粒度感知能力上的性能，例如计数任务的准确率提高了29.7%。

#### 全文结论

Auto-Bench作为一个自动化的基准测试流程，成功地策划了最大的数据集，揭示了当前VLMs的不足，并相应地通过SFT提高了VLMs的能力。Auto-Bench被视为一个有价值的基准测试，为研究社区提供了新的见解，展示了LLMs如何在评估和训练中促进VLMs的发展。

#### 阅读总结报告

**标题**：自动化基准测试流程Auto-Bench用于评估视觉-语言模型与人类智能的一致性

**摘要**： 本文介绍了Auto-Bench，这是一个创新的自动化基准测试流程，旨在评估视觉-语言模型（VLMs）与人类智能的一致性。通过使用大型语言模型（LLMs）作为数据策划者和裁判，Auto-Bench能够自动生成和评估大量的问题-答案-推理三元组。这一流程不仅提高了数据策划的效率，还确保了评估结果与人类偏好和价值观的一致性。实验结果表明，现有的VLMs在执行复杂任务和与人类价值观一致性方面存在不足，而Auto-Bench生成的数据集能够有效地进行监督微调，提升模型性能。

**关键点**：

* **自动化数据生成**：利用LLMs自动生成与人类意图紧密匹配的问题-答案-推理三元组。
* **全面评估**：Auto-Bench不仅评估VLMs的性能，还包括与人类价值观的一致性。
* **裁判的一致性**：使用GPT-3.5作为裁判，实现了与人类判断的平均一致率达到85%。
* **模型改进**：通过监督微调，显著提高了VLMs在特定任务上的性能。

**结论**： Auto-Bench为评估VLMs提供了一个灵活、可扩展和全面的基准测试，不仅能够揭示模型的不足，还能通过SFT促进模型性能的提升。这一流程有望成为评估和提升VLMs的重要工具，推动该领域的发展。
