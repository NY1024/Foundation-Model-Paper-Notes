# Adversarial Preference Optimization

<figure><img src="../.gitbook/assets/image (112).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

大型语言模型（LLMs）在自然语言处理等领域展现出了卓越的能力。为了提高LLMs与人类交互的质量，需要对模型进行偏好对齐，即让模型生成更符合用户偏好的响应。现有的偏好对齐方法依赖于手动标注的偏好数据来指导模型优化方向。然而，随着模型的不断更新，模型生成的样本与人类偏好的响应之间出现了分布差距，这影响了模型微调的效率。

### 2. 过去方案和缺点

以往的偏好对齐方法主要基于强化学习（RLHF）、对比学习和语言建模。这些方法在实施过程中存在不稳定、复杂度高和计算资源消耗大等问题。此外，由于采样分布偏移问题，这些方法在几次偏好对齐更新后，需要额外的人类反馈标注来适应新的分布，这导致了人力成本的增加。

<figure><img src="../.gitbook/assets/image (113).png" alt=""><figcaption></figcaption></figure>

### 3. 本文方案和步骤

本文提出了一种名为对抗性偏好优化（APO）的框架，通过最小-最大博弈，使得LLM代理和偏好模型交替更新。APO方法不需要额外的标注，可以通过对抗学习过程自我适应生成分布的差距。具体步骤包括：

* 使用对抗性学习过程，LLM生成响应以最大化预期奖励分数，而偏好模型（RM）旨在区分黄金响应和LLM生成响应之间的分数差异。
* 在LLM训练查询集上进行拒绝采样，生成响应样本。
* 更新RM以扩大黄金响应和生成响应之间的奖励差距。
* 使用RM对LLM生成的响应进行评分，并使用如RJS、RRHF和DPO等对齐方法更新LLM。

### 4. 本文创新点与贡献

* 提出了一种新的对抗性学习框架，用于更有效地进行人类偏好优化。
* APO方法可以在不增加额外标注资源的情况下，通过对抗学习过程自我适应生成分布的差距。
* 实验结果表明，APO能够进一步提升基线方法在有用性和无害性方面的对齐性能。

### 5. 本文实验

实验在Helpful\&Harmless（HH）数据集上进行，使用Alpaca和LLaMA-2作为基础LLM。实验结果表明，APO方法在偏好准确性、概率校准和RM平均分数方面均优于基线方法。

### 6. 实验结论

APO方法在LLM对齐任务中表现出色，尤其是在多轮LLM对齐中，APO与RJS方法的性能差距随着训练轮数的增加而扩大。这表明APO方法可以随着对齐轮数的增加而累积性能提升。

### 7. 全文结论

APO框架为LLMs的人类偏好对齐提供了一种有效的对抗性学习方法。通过交替更新RM和LLM，APO能够在不增加额外标注资源的情况下，提高偏好优化的效率。实验结果支持了APO方法的有效性，并表明其在实际LLM训练场景中的应用潜力。

### 阅读总结

本文针对LLMs的偏好对齐问题，提出了一种新的对抗性学习框架APO。APO通过对抗性游戏的方式，使得LLM和RM能够交替更新，从而有效地适应生成分布的变化。实验结果表明，APO方法在提高LLMs的有用性和无害性方面优于现有的基线方法。尽管APO方法在实验中表现出色，但作者也指出了其局限性，例如在在线RLHF方法结合APO的验证、以及对有害数据影响的不明确性。总体而言，APO为LLMs的偏好对齐提供了一种有前景的解决方案。
