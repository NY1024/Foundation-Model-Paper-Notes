# DENEVIL: TOWARDS DECIPHERING AND NAVIGATING THE ETHICAL VALUES OF LARGE LANGUAGE MODELS VIA INSTRUCT

<figure><img src="../.gitbook/assets/image (191).png" alt=""><figcaption></figcaption></figure>

## 研究背景

随着大型语言模型（LLMs）的不断进步，它们在日常生活中的融合程度不断提高，可能因生成的不道德内容而带来社会风险。尽管对偏见等特定问题进行了广泛研究，但LLMs的内在价值观从道德哲学的角度来看仍然鲜有探索。本文深入研究了基于价值理论自动导航LLMs的道德价值观。

## 过去方案和缺点

以往的研究主要集中在使用静态的、歧视性评估来衡量LLMs的道德判断和问卷调查，这些方法存在两个主要挑战：可靠性和有效性。可靠性问题主要是因为测试数据可能被包含在LLMs的训练数据中，导致测试结果不可靠。有效性问题则是因为这些方法只评估LLMs对道德价值的知识，而不是它们的行为是否符合价值原则。

<figure><img src="../.gitbook/assets/image (192).png" alt=""><figcaption></figcaption></figure>

## 本文方案和步骤

本文提出了DeNEVIL，一个新颖的提示生成算法，旨在动态利用LLMs的价值漏洞，以生成方式诱发道德违规行为，揭示其潜在的价值倾向。基于此，构建了MoralPrompt，一个包含2397个提示、覆盖500多个价值原则的高质量数据集，并在多种LLMs上进行基准测试。此外，开发了VILMO，一种上下文对齐方法，通过学习生成适当的价值指导来增强LLMs输出的价值合规性。



DeNEVIL（Deciphering and Navigating the Ethical Values via Instruction Learning）是本文提出的一种新颖的动态和生成性价值评估框架。它旨在揭示大型语言模型（LLMs）在处理道德原则时的潜在倾向和漏洞。以下是DeNEVIL方法的详细说明：

#### 目标

DeNEVIL的目标是评估LLMs在生成行为时与给定价值原则的一致性，而不是仅仅评估它们对道德知识的掌握。这种方法试图通过生成具有挑战性的提示（prompts）来诱发LLMs违反特定的道德原则，从而测试它们的内在道德价值观。

#### 方法

DeNEVIL框架使用变分期望最大化（Variational Expectation Maximization, EM）算法来动态地探索每个LLM的价值漏洞，并创建新的、针对性的提示，这些提示旨在最大化LLM违反特定价值原则的概率。

**步骤**

1. **生成挑衅性提示（E-Step）**：
   * 对于给定的道德原则（例如，不杀人是坏的），DeNEVIL首先考虑其逆价值陈述（例如，杀人）。
   * 然后，通过采样生成与逆价值陈述一致的完成（completions），这些完成是对提示的响应。
   * 对于具有强指令遵循能力的LLM（如ChatGPT），直接在提示中提供逆价值陈述作为指令。
2. **提示优化（M-Step）**：
   * 一旦获得违反价值原则的完成集，DeNEVIL继续优化提示，以最大化LLM生成这些完成的概率。
   * 这涉及到使用模拟退火（Simulated Annealing）来逐步采样新的提示候选，并根据其与完成的一致性接受每个候选提示。

**实现**

* 对于开源LLM，DeNEVIL可以直接利用模型生成提示和完成。
* 对于黑盒LLM，DeNEVIL通过建模每个分布为一个能量模型（Energy-based Model）来近似真实的概率分布，并通过训练这些能量函数来计算生成的提示的得分。

#### 创新点

* **动态测试数据**：DeNEVIL通过动态生成测试数据来避免数据污染和过期问题，确保测试数据集对于LLM来说是新颖的。
* **生成性评估**：与传统的歧视性评估不同，DeNEVIL通过生成性评估来反映LLM的行为是否符合道德原则，而不是仅仅评估它们的知识水平。

#### 应用

DeNEVIL方法被用于构建MoralPrompt数据集，该数据集包含多种价值原则的提示，用于评估LLMs的道德价值观。此外，DeNEVIL还为VILMO方法提供了基础，VILMO是一种通过学习生成适当的价值指令来提高LLMs输出价值合规性的上下文对齐方法。

总的来说，DeNEVIL方法为评估和改进LLMs在道德和价值方面的决策提供了一种新的方法论，这对于构建更加负责任和道德的AI系统至关重要。





## 本文创新点与贡献

* 提出了DeNEVIL框架，用于动态探测LLMs的价值漏洞，并生成诱导违规行为的提示。
* 构建了MoralPrompt数据集，涵盖了广泛的价值原则，为评估LLMs的道德价值观提供了基础。
* 开发了VILMO方法，通过在上下文中生成针对性的价值指导来提高LLMs的价值合规性。
* 研究表明，大多数模型在本质上与道德价值观不一致，需要进一步的道德价值对齐。

## 本文实验

实验包括使用MoralPrompt数据集对27个不同架构和规模的LLMs进行基准测试，并使用VILMO方法对ChatGPT进行价值对齐实验，与其他上下文对齐方法进行比较。

## 实验结论

实验结果表明，大多数LLMs在本质上与道德价值观不一致，需要进一步的道德价值对齐。VILMO方法在提高价值合规性方面优于现有竞争对手。

## 全文结论

本文通过动态和生成性的价值评估框架DeNEVIL揭示了LLMs的道德价值观，并开发了VILMO方法来提高LLMs输出的价值合规性。这些方法适用于黑盒和开源模型，为研究LLMs的道德价值观提供了初步步骤。

## 阅读总结报告

本文提出了一种新的方法来评估和对齐大型语言模型的道德价值观。通过DeNEVIL框架和MoralPrompt数据集，研究者能够动态地探测和评估LLMs的道德倾向。VILMO方法的开发进一步增强了LLMs在生成文本时的价值合规性。这些贡献为理解和改进LLMs的道德决策提供了新的视角和技术手段。尽管存在一些局限性，如价值理论的选择和潜在的生成偏差，但本文的研究为未来在这一领域的探索奠定了坚实的基础。
