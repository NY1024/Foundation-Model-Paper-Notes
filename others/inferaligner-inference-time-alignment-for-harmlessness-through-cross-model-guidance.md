# InferAligner: Inference-Time Alignment for Harmlessness through Cross-Model Guidance

<figure><img src="../.gitbook/assets/image (184).png" alt=""><figcaption></figcaption></figure>

### 研究背景

随着大型语言模型（LLMs）的快速发展，它们不仅被用作通用的AI助手，还通过进一步的微调来满足不同应用的需求。当前LLMs成功的关键在于对齐过程，即确保LLMs与人类价值观和意图保持一致。然而，现有的对齐方法，如监督式微调（SFT）和基于人类反馈的强化学习（RLHF），主要关注训练时的对齐，实施过程复杂且繁琐。因此，本文提出了InferAligner，一种新颖的推理时对齐方法，通过跨模型指导来实现无害性对齐。

<figure><img src="../.gitbook/assets/image (185).png" alt=""><figcaption></figcaption></figure>

### 过去方案和缺点

以往的对齐方法，如SFT和RLHF，虽然在对齐结果上表现出色，但通常涉及复杂的训练过程，需要大量的资源，这显著增加了实施训练时对齐的挑战。此外，现有的推理时对齐方法，如在输入中添加对齐提示或沿着目标模型本身提取的方向移动激活，虽然更易于使用，但在对齐效果上较弱，且显著影响模型在下游任务上的性能和能力。

<figure><img src="../.gitbook/assets/image (186).png" alt=""><figcaption></figcaption></figure>

### 本文方案和步骤

InferAligner通过利用从安全对齐模型中提取的安全引导向量（SSVs）来修改目标模型在响应有害输入时的激活，从而引导目标模型提供无害的响应。具体步骤包括：

1. 利用对话模板和安全相关向量（SRVs）作为安全引导向量（SSVs）。
2. 在目标模型的推理阶段，基于SRVs监督输入指令的意图。
3. 一旦检测到有害意图，就在所有令牌位置添加SSVs，改变激活并引导模型安全地响应恶意输入。

### 本文创新点与贡献

* 提出了InferAligner，这是一种新颖的推理时对齐方法，能有效增强模型的安全性，同时保持下游性能不变。
* InferAligner简单易用，无需训练，即使在没有对齐模型的情况下也能有效地使用。
* 首次探索了多模态大型语言模型（MLLMs）的无害性对齐，并提出了MM-Harmful Bench，这是第一个专门用于安全研究的多模态数据集。

### 本文实验

实验结果表明，InferAligner可以非常有效地应用于金融、医学和数学等领域的特定模型，以及像LLaVA这样的多模态大型语言模型。它显著降低了有害指令和越狱攻击的攻击成功率（ASR），同时在下游任务中保持几乎不变的性能。

### 实验结论

InferAligner在保护模型免受有害指令和越狱攻击方面表现出色，同时保持了模型在下游任务上的性能。这证明了InferAligner在推理时对齐方法中的有效性和实用性。

### 全文结论

本文提出的InferAligner方法，通过在推理时对模型进行无害性对齐，有效地提高了模型的安全性，同时保持了其在各种任务上的性能。这一方法为未来的LLMs安全对齐提供了一个简单而有效的解决方案。



注：

安全引导向量（SSVs）是通过比较有害指令和无害指令在模型中的激活差异来得到的。具体步骤如下：

1. **选择指令样本**：首先，研究者们准备了两类指令样本，一类是展示有害意图的指令（有害指令），另一类是展示无害意图的指令（无害指令）。
2. **构建对话模板**：使用这些指令与对话模板结合，形成具体的有害和无害提示（prompts）。
3. **计算激活差异**：在模型中输入这些提示，记录最后一个令牌在不同提示下的激活情况。然后，通过计算有害提示和无害提示之间最后一个令牌激活的平均差异，得到安全相关向量（SRVs）。
4. **提取SSVs**：从那些已经针对无害性进行了对齐的模型中提取SRVs，这些SRVs被用作安全引导向量（SSVs）。这些对齐模型是通过特定的训练过程（如SFT或RLHF）调整过的，以确保它们能够生成无害的响应。
5. **规范化**：对计算出的SRVs进行规范化处理，得到最终的SSVs。

在InferAligner的上下文中，SSVs被用来在推理时调整目标模型的激活，以便在面对有害输入时，模型能够生成无害的响应。这种方法利用了已经训练好的安全模型的知识，来指导其他模型在推理时的安全性表现，而无需对目标模型本身进行复杂的再训练过程。



### 阅读总结报告

本论文介绍了InferAligner，这是一种新的推理时对齐方法，旨在提高大型语言模型（LLMs）的安全性。随着LLMs在各种应用中的广泛使用，确保它们的输出与人类价值观保持一致变得至关重要。传统的对齐方法在实施上存在复杂性和资源密集型的问题，而InferAligner通过在推理阶段引入跨模型指导，提供了一种简单且资源高效的替代方案。

InferAligner的核心在于使用安全引导向量（SSVs）来调整模型的激活，从而在不牺牲下游任务性能的情况下，引导模型产生无害的响应。这种方法不仅适用于传统的文本模型，还成功应用于多模态模型，显示了其广泛的适用性和有效性。

实验结果表明，InferAligner能显著降低有害指令和越狱攻击的成功率，同时保持模型在金融、医学和数学等领域的专业知识和能力。此外，该方法不需要额外的训练，使其成为一种实用的、即插即用的解决方案，适用于需要快速提高模型安全性的场景。

总的来说，InferAligner为LLMs的安全性对齐提供了一种新颖的方法，它简单、高效，并且能够适应多种模型和应用场景。这项工作不仅推动了AI安全性研究的边界，也为开发更加可靠和安全的AI系统提供了重要的工具。
