# Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI

<figure><img src="../.gitbook/assets/image (15) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 研究背景

艺术家们越来越关注图像生成模型的进步，这些模型能够紧密复制他们独特的艺术风格。作为回应，已经开发了几种保护工具，通过在在线发布的作品中加入小的对抗性扰动来防止风格模仿。这些工具受到了媒体的广泛关注，并已被下载超过100万次。

### 过去方案和缺点

现有的艺术风格保护工具，例如Glaze、Mist和Anti-DreamBooth，通过在艺术家发布的图片中添加对抗性扰动来防止风格模仿。然而，这些工具的有效性尚未经过严格的测试，尤其是在面对积极尝试绕过它们的攻击者时。

### 本文方案和步骤

本文评估了流行的保护工具的有效性，并展示了它们只提供了一种虚假的安全感。研究发现，低技术含量的“现成”技术，如图像上采样，足以创建强大的模仿方法，显著降低现有保护的有效性。通过用户研究，本文证明了所有现有的保护都可被轻易绕过，使艺术家容易受到风格模仿的威胁。

<figure><img src="../.gitbook/assets/image (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 本文创新点与贡献

1. 展示了现有的艺术风格保护工具在面对简单且强大的模仿方法时是无效的。
2. 提供了统一和严格的评估协议，以可靠地评估现有保护措施对各种简单和自然的强模仿方法的有效性。
3. 设计了四种强模仿方法，这些方法简单易用，适用于技术不高的攻击者。

### 本文实验

实验设置包括评估三种保护工具（Mist、Glaze和Anti-DreamBooth）对抗四种强模仿方法（高斯噪声、DiffPure、Noisy Upscaling和IMPRESS++）以及基线模仿方法。实验使用了10位不同艺术家的图像，并为每种风格模仿场景设计了10个不同的提示。

### 实验结论

实验结果表明，所有现有的保护工具都容易被绕过。特别是，Noisy Upscaling是最有效的强模仿方法，每种保护工具的中位成功率都超过了40%。这表明，即使使用了保护工具，攻击者仍然可以生成与未受保护的艺术作品风格相似的图像。

### 全文结论

对抗性机器学习技术无法可靠地保护艺术家免受生成式风格模仿的威胁。本文敦促开发替代的保护措施来保护艺术家，并在发布结果之前将这些发现披露给受影响的保护工具，以便他们为现有用户确定最佳的行动方案。

### 阅读总结报告

这篇论文《Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI》由Robert Hönig等人撰写，评估了用于保护艺术家免受风格模仿攻击的工具的有效性。研究表明，尽管这些工具广受欢迎并被广泛下载，但它们实际上并不可靠。通过简单的技术，如图像上采样，可以轻易绕过这些保护，使艺术家的作品仍然容易受到风格模仿的威胁。

论文的主要贡献在于揭示了现有保护措施的脆弱性，并提出了一种统一的评估方法来测试这些保护措施。此外，论文还提出了四种新的强模仿方法，这些方法简单、易于使用，并且不需要专业知识。

实验部分包括了对三种不同的保护工具的评估，使用了10位艺术家的作品和10个不同的提示。用户研究结果表明，即使在应用了保护措施的情况下，攻击者仍然能够生成与原始艺术风格无法区分的图像。

最终，论文得出结论，对抗性扰动并不能有效地保护艺术家免受生成式AI的攻击，并呼吁开发更可靠的保护措施。论文的发现对于艺术家和开发保护工具的开发者都具有重要的启示作用。

