# More RLHF, More Trust? On The Impact of Human Preference Alignment On Language Model Trustworthiness

<figure><img src="../.gitbook/assets/image (2) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 阅读总结报告

#### 1. 研究背景

随着大型语言模型（LLMs）的迅猛发展，它们在众多认知任务上展现出前所未有的性能。然而，为了安全地利用这些强大模型的力量，需要确保它们的输出与人类价值观紧密对齐。尽管像基于人类反馈的强化学习（RLHF）这样的偏好学习算法在对齐人类偏好方面表现出了有效性，但它们对模型可信度的假设性改进尚未得到充分证实。

#### 2. 过去方案和缺点

现有研究中存在一个明显的差距，即我们对于流行的偏好学习框架如何影响它们微调模型的可信度了解不足。RLHF及其变体旨在增强模型与人类偏好的对齐，但它们的影响范围尚未被充分探索。

#### 3. 本文方案和步骤

本研究旨在填补这一知识空白，通过检查不同RLHF变体，如监督微调（SFT）、近端策略优化（PPO）和直接偏好优化（DPO）对各种语言模型可信度基准的影响。研究涉及多个关键方面，包括模型在毒性、刻板印象偏见、机器伦理、真实性和隐私方面的表现。

#### 4. 本文创新点与贡献

* 本研究提供了实证结果，展示了偏好学习算法对LLMs的影响。
* 分析了这些结果，开始理解这些对齐方法的复杂动态及其对可信AI发展的影响。
* 通过研究，希望引导社区朝着开发既具备能力又可信的LLMs方向发展。

#### 5. 本文实验

* 使用开源Pythia Suite作为目标模型，尺寸从7000万到69亿参数不等。
* 采用Anthropic HH数据集作为人类偏好数据集，用于模型对齐。
* 使用三种流行的RLHF变体进行微调，并在五个可信度维度上评估模型。

#### 6. 实验结论

* RLHF通过偏好数据、对齐算法和特定可信度方面之间的复杂相互作用，其对可信度的改进远非保证，有时甚至可能产生相反的效果。
* 在模型尺寸扩大时，只有DPO带来了轻微的减毒效果，而PPO和SFT导致毒性增加。
* 所有三种对齐方法都显著增加了刻板印象偏见，并且在模型输出中减少了真实性。

#### 7. 全文结论

本研究提供了对人类偏好对齐技术，特别是三种基于人类反馈的强化学习变体对语言模型在五个关键垂直领域的可信度影响的全面分析。研究结果揭示了这些方面和所采用的对齐方法之间的复杂相互作用，强调了在开发和部署语言模型时考虑可信度的多面性的重要性。

#### 阅读总结

这项研究深入探讨了RLHF及其变体对语言模型在多个关键可信度方面的影响，发现对齐过程并不总是能提升模型的可信度，有时甚至可能带来负面影响。研究结果表明，对齐方法的影响取决于所使用的偏好数据集和特定算法，这些关系很难泛化，本质上是复杂的。通过揭示这些对齐方法的内在动态，本研究希望为未来开发更可信的语言模型提供指导。
