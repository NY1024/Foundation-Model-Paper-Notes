# Scalable Extraction of Training Data from (Production) Language Models

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

##

### 1. 研究背景

大型语言模型（LLMs）在训练过程中会记忆训练数据中的示例，这可能导致攻击者通过精心设计的输入来提取或泄露这些信息。这种现象被称为“可提取记忆”（extractable memorization），它揭示了LLMs在隐私保护方面的潜在风险。

<figure><img src="../.gitbook/assets/image (5) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

### 2. 过去方案和缺点

以往的研究主要关注于通过大规模研究来量化LLMs中可提取记忆的总量，以及开发实际攻击来提取小型模型（如GPT-2）的训练数据。然而，这些方法在提取大型模型（如GPT-3）的训练数据时效果有限，且通常需要手动验证提取的数据是否确实来自训练集，这既耗时又可能存在误差。

### 3. 本文方案和步骤

本文提出了一种新的可扩展方法来检测和量化LLMs中的可提取记忆。研究者们开发了一种算法，通过分析模型输出的困惑度（perplexity）来检测记忆化的训练数据。该方法首先在开源模型上进行测试，然后扩展到半开放和封闭模型，如LLaMA和ChatGPT。对于ChatGPT，研究者们特别开发了一种新的“发散攻击”（divergence attack），使其偏离正常的对话生成模式，从而更容易提取训练数据。



新的“发散攻击”（divergence attack）之所以能够使模型偏离正常的对话生成模式，从而更容易提取训练数据，原因在于这种攻击策略专门针对了模型的训练数据记忆特性。在正常情况下，经过对齐处理的模型（如ChatGPT）被设计为遵循特定的对话风格和行为准则，以避免生成不当或敏感的内容。这种对齐通常涉及强化学习（RLHF）等技术，以确保模型的输出符合预期的行为模式。

然而，当模型被设计为在特定条件下（例如，遵循特定的对话格式或回答特定类型的问题）生成响应时，它可能会在训练数据中形成一种模式或倾向，这些模式或倾向在模型的权重中得以体现。发散攻击通过构造特定的输入，诱使模型“偏离”这种对齐状态，即不再遵循其训练时所学习的正常对话模式。这种偏离可能导致模型输出与训练数据中的原始文本更相似，因为模型不再受到对齐策略的限制，从而更容易“回忆”起训练数据中的特定片段。

通过这种方法，攻击者可以诱导模型生成与训练数据更直接相关的文本，而不是模型在正常操作模式下会生成的内容。这样，攻击者就能够从模型的输出中提取出训练数据的片段，这些片段可能包含敏感信息或私人数据，从而揭示了模型的隐私漏洞。这种攻击的成功实施表明，即使模型经过对齐处理，仍然可能存在记忆化问题，需要进一步的安全措施来保护模型免受此类攻击。





### 4. 本文创新点与贡献

* 提出了一种新的检测方法，可以在标记级别上检测LLMs中的可提取记忆。
* 开发了一种新的攻击策略，专门针对经过对齐处理的ChatGPT模型，使其在被攻击时产生更多的训练数据。
* 通过实验表明，即使是经过对齐处理的模型，也能通过精心设计的攻击提取出大量训练数据，这揭示了当前对齐技术并不能完全消除记忆化问题。

### 5. 本文实验

实验部分详细描述了实现细节、数据集构建、实验结果和模型依赖性分析。研究者们在多个模型上进行了实验，包括开源模型和半开放模型，以及经过对齐处理的ChatGPT。实验结果表明，新提出的攻击方法能够有效地从这些模型中提取训练数据。

### 6. 实验结论

实验结果证明了所提出方法的有效性。在开源模型上，研究者们能够提取出大量训练数据。对于ChatGPT，新的攻击方法使其在被攻击时产生比正常模式下多150倍的训练数据。这表明即使是经过对齐处理的模型，也存在可提取记忆的问题。

### 7. 全文结论

本文的研究结果强调了LLMs在隐私保护方面的脆弱性，即使是经过对齐处理的模型。研究者们提出的新攻击方法和检测策略为理解和改进LLMs的隐私保护机制提供了新的视角。同时，这也为未来的研究提供了新的挑战，即如何在不牺牲模型性能的前提下，进一步减少模型的记忆化现象。

### 阅读总结

本文通过提出新的检测方法和攻击策略，深入研究了LLMs中的可提取记忆问题。实验结果表明，即使是经过对齐处理的模型，也存在隐私泄露的风险。这项研究不仅揭示了LLMs的潜在安全问题，也为未来的隐私保护研究提供了重要的参考。
