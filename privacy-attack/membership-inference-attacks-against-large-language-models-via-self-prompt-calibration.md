# Membership Inference Attacks against Large Language Models via Self-prompt Calibration

<figure><img src="../.gitbook/assets/image (15).png" alt=""><figcaption></figcaption></figure>



#### 1. 研究背景

随着大型语言模型（LLMs）的普及，它们在多种复杂应用场景中展现出卓越的能力，例如聊天机器人、代码生成、文章合写等。然而，LLMs在提供便利的同时，也带来了潜在的隐私风险。会员推断攻击（Membership Inference Attacks, MIA）旨在推断目标数据记录是否被用于模型训练，这对于评估LLMs的隐私风险至关重要。

#### 2. 过去方案和缺点

现有的MIA主要分为两类：无需参考模型的攻击（reference-free）和基于参考模型的攻击（reference-based）。这些方法基于一个假设：训练记录被抽样的概率更高。但这一假设严重依赖于目标模型的过拟合，而过拟合可以通过多种正则化方法得到缓解，并且在大规模语言模型（LLMs）的泛化能力下会被削弱。此外，基于参考的攻击虽然在理论上有效，但其性能高度依赖于与训练数据集相似的参考数据集，这在实际情况下通常难以获得。

#### 3. 本文方案和步骤

为了解决现有MIA在实际LLMs中的局限性，本文提出了一种基于自校准概率变化的会员推断攻击（Self-calibrated Probabilistic Variation, SPV-MIA）。具体步骤包括：

* 利用LLMs在训练过程中不可避免的记忆效应，而不是过拟合，作为更可靠的会员信号。
* 通过自提示方法（self-prompt approach），使用目标LLM自身生成的数据集来微调参考模型，从而获得与训练数据集分布相似的数据集。

#### 4. 本文创新点与贡献

* 提出了一种新的会员信号，基于记忆而非过拟合，通过二阶导数测试检测局部最大值点。
* 引入自提示方法，通过简短文本提示目标LLM生成文本，构建与微调数据集分布相似的数据集。
* 在四个代表性的LLMs和三个数据集上进行了广泛的实验，证明了SPV-MIA在攻击性能上相比于现有基线提高了约23.6%。

#### 5. 本文实验

实验在四个代表性的LLMs（GPT-2, GPT-J, Falcon, LLaMA）和三个数据集（Wiki, AG News, Xsum）上进行。实验结果表明，SPV-MIA在AUC（Area Under the Curve）上相比于最好的基线提高了约23.6%。

#### 6. 实验结论

SPV-MIA在多个微调后的LLMs和数据集上揭示了显著更高的隐私风险，与现有的MIA相比有显著的改进。此外，实验还探讨了不同因素对SPV-MIA性能的影响，如参考模型的质量、自提示文本的来源和长度、参考数据集的规模等。

#### 7. 全文结论

本文通过提出SPV-MIA，揭示了现有MIA方法在实际应用中的不足，并展示了SPV-MIA在多种挑战性场景下的有效性。研究表明，即使在严格的使用限制下，SPV-MIA也能有效地揭示LLMs的隐私风险。

#### 阅读总结

本文针对当前LLMs在隐私保护方面的挑战，提出了一种新的MIA方法SPV-MIA。通过自校准概率变化和自提示方法，SPV-MIA能够有效地评估和利用LLMs在微调过程中的隐私风险。实验结果证明了SPV-MIA在多个维度上相比于现有方法的优越性，为理解和防御LLMs的隐私风险提供了新的视角和工具。
