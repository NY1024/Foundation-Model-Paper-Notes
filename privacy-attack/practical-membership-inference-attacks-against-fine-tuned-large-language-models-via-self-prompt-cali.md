# Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Cali

<figure><img src="../.gitbook/assets/image (168).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

随着大型语言模型（LLMs）在自然语言处理（NLP）任务中的表现不断提升，它们在实际应用中的隐私风险也日益受到关注。LLMs在微调过程中可能会泄露用于训练的私有数据集信息。会员推断攻击（Membership Inference Attacks, MIAs）旨在判断特定数据记录是否被用于模型训练。尽管已有研究探讨了LLMs的隐私风险，但现有MIA算法在实际大型语言模型上的有效性尚不明确。

### 2. 过去方案和缺点

现有的MIA算法主要分为两类：无参考（reference-free）和有参考（reference-based）。这些方法基于训练记录在模型中被更高概率采样的假设，但这一假设依赖于模型的过拟合，而现代LLMs通常采用多种正则化方法来避免过拟合。此外，有参考攻击需要一个与训练数据集分布相似的参考数据集，这在实际场景中通常是不可获取的。

### 3. 本文方案和步骤

本文提出了一种基于自校准概率变化（Self-calibrated Probabilistic Variation, SPV）的会员推断攻击（SPV-MIA）。该方法首先识别LLMs在训练过程中不可避免的记忆现象，并基于此提出一种更可靠的会员信号。其次，引入自提示（self-prompt）方法，通过目标LLM自身生成的数据集来微调参考模型，从而在不需要访问训练数据集的情况下收集具有相似分布的数据集。

<figure><img src="../.gitbook/assets/image (169).png" alt=""><figcaption></figcaption></figure>

### 4. 本文创新点与贡献

* 提出了一种新的会员信号，基于记忆而非过拟合，以检测LLMs中成员记录的特征。
* 提出了自提示方法，通过向目标LLM发送短文本块并收集生成的文本来构建参考数据集，从而在实际场景中提高攻击性能。
* 在四个代表性LLM和三个数据集上进行了广泛的实验，证明了SPV-MIA相较于现有MIA算法在攻击性能上约有23.6%的提高。

### 5. 本文实验

实验在三个数据集上对四个LLM进行了攻击性能评估，并与五种最先进的MIA算法进行了比较。实验结果表明，SPV-MIA在所有LLM上的表现均优于基线方法，并且在极端条件下的有效性得到了验证。

### 6. 实验结论

SPV-MIA在实际LLM应用中揭示了显著的隐私风险，并且在不同的微调技术和隐私保护算法下，其攻击性能仍然保持在较高水平。

### 7. 全文结论

本文通过提出SPV-MIA，展示了现有LLMs在面对会员推断攻击时的脆弱性，并提出了一种有效的攻击方法。这一发现强调了在设计和部署LLMs时需要考虑的隐私保护措施。

### 阅读总结

本文针对LLMs在微调过程中可能泄露私有数据集信息的问题，提出了一种新的会员推断攻击方法。通过自提示方法和基于记忆的会员信号，SPV-MIA在实验中显示出比现有方法更高的攻击成功率。这一研究不仅揭示了LLMs在隐私保护方面的潜在风险，也为未来的隐私保护研究提供了新的视角。
