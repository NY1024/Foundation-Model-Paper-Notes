# Prompt Stealing Attacks Against Large Language Models

<figure><img src="../.gitbook/assets/image (164).png" alt=""><figcaption></figcaption></figure>

## 阅读总结报告

### 1. 研究背景

随着大型语言模型（LLMs）如ChatGPT在各个领域的广泛应用，"prompt工程"（即设计高质量的输入提示以改善模型输出）变得越来越重要。公司大量投资于专业的提示工程师，并提供教育资源以满足市场需求。然而，这些精心设计的提示可能会被攻击者通过逆向工程手段窃取，从而引发安全问题。

### 2. 过去方案和缺点

以往的研究主要关注于如何设计高质量的提示以提高LLMs的性能，而没有充分考虑提示本身的安全性。现有的安全措施并未针对提示窃取攻击进行优化，因此存在潜在的安全漏洞。



### 3. 本文方案和步骤

本文提出了一种名为"提示窃取攻击"（prompt stealing attacks）的新方法，旨在通过生成的答案来窃取原始的提示。攻击包含两个主要模块：参数提取器（parameter extractor）和提示重构器（prompt reconstructor）。参数提取器用于确定原始提示的属性，而提示重构器则基于生成的答案和提取的特征来重建原始提示。

### 4. 本文创新点与贡献

* 提出了一种新的针对LLMs的攻击方法，即提示窃取攻击。
* 设计了两个模块化的攻击工具：参数提取器和提示重构器，用于从生成的答案中逆向工程出原始提示。
* 通过实验验证了所提出攻击方法的有效性，并展示了攻击者可以如何利用这些技术来窃取提示。

### 5. 本文实验

实验在两个基准问答数据集（RetrievalQA和AlpacaGPT4）和两种流行的LLMs（ChatGPT和LLaMA）上进行。实验结果表明，所提出的参数提取器和提示重构器能够成功预测原始提示的类型和结构信息，并生成与原始提示相似的逆向提示。

### 6. 实验结论

实验结果显示，所提出的提示窃取攻击在预测提示类型和重构提示方面表现出色。这表明LLMs的提示可以被有效地窃取，从而暴露了LLMs在安全性方面的潜在风险。

### 7. 全文结论

本文提出了一种新的针对LLMs的提示窃取攻击方法，并展示了其有效性。这一发现强调了LLMs在安全性方面的脆弱性，并为未来的研究提供了新的视角。同时，本文也提出了一些可能的防御策略，尽管这些策略可能会影响LLMs的实用性，但它们为如何保护LLMs免受此类攻击提供了初步的思路。

### 阅读总结

本文针对LLMs的安全性问题提出了一种新的攻击方法——提示窃取攻击，并展示了如何通过生成的答案来逆向工程出原始的提示。这一研究不仅揭示了LLMs在安全性方面的潜在风险，也为如何设计更安全的LLMs提供了新的研究方向。同时，本文的实验结果也为防御此类攻击提供了实验基础。
