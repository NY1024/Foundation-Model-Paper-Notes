# PRSA: Prompt Reverse Stealing Attacks against Large Language Models

<figure><img src="../.gitbook/assets/image (109).png" alt=""><figcaption></figcaption></figure>

## 研究背景

大型语言模型（LLMs）在自然语言处理（NLP）领域的应用已经引起了广泛关注，尤其是在自动化代码生成、文案创作和数据分析等下游任务中表现出色。LLMs通常不需要针对特定任务进行微调，而是通过精心设计的提示（prompts）来高效处理任务。然而，提示的有效性直接影响任务的准确性和效率，而设计有效的提示需要专业知识。随着提示服务的兴起，如提示市场和LLM应用平台，提供商通常通过输入输出示例来展示提示的能力，以吸引用户。然而，这种做法可能存在潜在的安全风险，即暴露输入输出对可能导致提示泄露，侵犯开发者的知识产权。

## 过去方案和缺点

以往的研究主要集中在通过恶意输入来利用LLMs的漏洞进行“越狱”攻击，以揭示隐藏的提示。然而，这些攻击方法有限，需要与目标LLM应用进行交互，并且针对特定的漏洞。一旦漏洞被修复，攻击就会失效。此外，这些方法通常需要与模型的内部机制直接交互，这在实际场景中可能受到限制。

<figure><img src="../.gitbook/assets/image (4) (1) (1) (1) (1).png" alt=""><figcaption></figcaption></figure>

## 本文方案和步骤

本文提出了一种名为PRSA（Prompt Reverse Stealing Attacks）的新型攻击框架，用于反向窃取商业LLMs的提示。PRSA的核心思想是通过分析输入输出对的关键特征来模仿并逐步推断（窃取）目标提示。PRSA主要包括两个关键阶段：提示变异（prompt mutation）和提示修剪（prompt pruning）。在变异阶段，提出了一种基于差分反馈的提示注意力算法，以捕捉关键特征并有效推断目标提示。在修剪阶段，识别并屏蔽依赖于特定输入的词汇，使提示能够适应多样化的输入以实现泛化。

## 本文创新点与贡献

* 提出了PRSA框架，这是首次深入探索针对LLMs的提示窃取攻击。
* 提出了一种基于差分反馈的提示注意力算法，该算法通过优化提示注意力，有效地指导提示生成模型捕捉目标提示的关键特征，从而准确模仿目标提示。
* 在两个实际场景——提示市场和LLM应用中进行了广泛的实验，展示了PRSA对提示版权的实际威胁。

## 本文实验

实验在18个类别的提示市场中的提示上进行评估，结果强烈证实了PRSA的有效性和泛化能力。然后，在流行的LLM应用平台如OpenGPT和GPTsdex上进行了评估。结果进一步表明，PRSA可以实现56%和38%的成功率，且成本极低。

## 实验结论

PRSA在实际场景中对提示版权构成了严重威胁。在提示市场中，PRSA在语义、句法和结构相似性方面的表现优于现有基线方法。在LLM应用平台上，PRSA的攻击成功率也显著高于基线方法。

## 全文结论

本文通过PRSA框架展示了LLMs中提示窃取攻击的可行性和有效性，揭示了提示版权保护的重要性。研究结果鼓励提示服务提供商采取保护措施，以防止提示版权受到侵犯。

## 阅读总结报告

本文针对LLMs中提示窃取的安全性问题进行了深入研究，并提出了PRSA框架。通过实验验证了PRSA在不同场景下的有效性，展示了其在提示窃取攻击中的潜力。研究不仅提高了对LLMs知识产权保护的认识，也为未来的安全研究提供了新的方向。
