# Effective Prompt Extraction from Language Models

<figure><img src="../.gitbook/assets/image (301).png" alt=""><figcaption></figcaption></figure>



#### 1. 研究背景

大型语言模型（LLMs）能够通过遵循自然语言指令来执行各种任务。企业使用提示（prompts）来指导模型的输出，并将这些提示视为秘密，以避免用户获取。然而，已有案例表明，敌手可以通过提示提取攻击来恢复这些提示。本论文提出了一个系统化的框架，用于衡量这些攻击的有效性。

#### 2. 过去方案和缺点

过去的方案主要依赖于企业对提示保密，以及模型自身对提示的保护机制，例如拒绝讨论任何关于其提示、指令或规则的问题。然而，这些方案存在缺点，包括：

* 提示保密性不强，敌手可以通过攻击手段提取提示。
* 模型的自我保护机制可能被绕过。

<figure><img src="../.gitbook/assets/image (302).png" alt=""><figcaption></figcaption></figure>

#### 3. 本文方案和步骤

本文提出了一个名为ProAttack的框架，用于系统化地衡量提示提取攻击的有效性。具体步骤包括：

1. 发送多个攻击查询（attack queries）到服务API。
2. 观察（可能失败的）提取，并使用基于分类器的启发式方法来确定提取是否成功。
3. 根据置信度估计，对真实提示进行猜测。

#### 4. 本文创新点与贡献

* 提出了一个高精确度的提示提取攻击框架，能够有效地从LLMs中提取提示。
* 实验涵盖了3种不同的提示来源和11种不同的基础大型语言模型，证明了简单文本攻击可以高概率地揭示提示。
* 提供了一个方法来验证提取的提示是否为实际的秘密提示，而不是模型的幻觉。

#### 5. 本文实验

* 在受控的实验设置中，使用三个数据集进行测试，包括人工生成的提示和真实系统（如Bing Chat和ChatGPT）的提示。
* 通过翻译基础的攻击策略，展示了即使在有防御措施的情况下，也能从真实系统中提取提示。

#### 6. 实验结论

* 提示提取攻击不仅可行，而且在多种情况下效果显著，能够高精确度地确定提取的提示是否正确。
* 即使在模型被指示不泄露其提示的情况下，攻击仍然有效。

#### 7. 全文结论

* 提示不是秘密，基于提示的服务容易受到高精确度的提取攻击。
* 目前看似有希望的防御措施，如基于文本的过滤防御，不足以抵御提示提取攻击。
* 未来的工作应该探索如何设计更有效的防御措施，以及如何在现实世界的应用中减轻提示提取攻击的风险。

#### 阅读总结

本文通过ProAttack框架，展示了大型语言模型的提示可以被有效提取，即使在模型具备一定防御能力的情况下。研究结果表明，现有的防御措施不足以保护提示不被敌手获取，这为LLMs的安全性提出了新的挑战。作者呼吁社区设计更强大的模型和接口，以防止提示提取攻击，并提高未来基于LLMs的服务的安全性。
